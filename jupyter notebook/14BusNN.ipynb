{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "torch.set_printoptions(precision=5, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(dataset, percentage):\n",
    "    data_size = len(dataset)\n",
    "    return dataset[:int(data_size*percentage/100)]\n",
    "\n",
    "def make_dataset(dataset, n_bus):\n",
    "    x_raw_1, y_raw_1 = [], []\n",
    "    x_raw, y_raw = [], []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        for n in range(n_bus):\n",
    "            x_raw_1.extend(dataset[i, 4*n+1:4*n+3])\n",
    "            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])\n",
    "        x_raw.append(x_raw_1)\n",
    "        y_raw.append(y_raw_1)\n",
    "        x_raw_1, y_raw_1 = [], []\n",
    "        \n",
    "    x_raw = torch.tensor(x_raw, dtype=torch.float)\n",
    "    y_raw = torch.tensor(y_raw, dtype=torch.float)\n",
    "    return x_raw, y_raw\n",
    "\n",
    "def normalize_dataset(x, y):\n",
    "    x_mean = torch.mean(x,0)\n",
    "    y_mean = torch.mean(y,0)\n",
    "    x_std = torch.std(x,0)\n",
    "    y_std = torch.std(y,0)\n",
    "    x_norm = (x-x_mean)/x_std\n",
    "    y_norm = (y-y_mean)/y_std\n",
    "    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    return x_norm, y_norm, x_mean, y_mean, x_std, y_std\n",
    "\n",
    "def denormalize_output(y_norm, y_mean, y_std):\n",
    "    y = y_norm*y_std+y_mean\n",
    "    return y\n",
    "\n",
    "def NRMSE(yhat,y):\n",
    "    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))\n",
    "\n",
    "def MSE(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_excel('dataset\\Grid_14 bus_1.xlsx').values\n",
    "dataset2 = pd.read_excel('dataset\\Grid_14 bus_2.xlsx').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 100\n",
    "val_percentage = 100\n",
    "\n",
    "train_dataset = slice_dataset(dataset1, train_percentage)\n",
    "val_dataset = slice_dataset(dataset2, val_percentage)\n",
    "\n",
    "n_bus = 14\n",
    "\n",
    "#actual data\n",
    "x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)\n",
    "x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)\n",
    "\n",
    "#normalized data\n",
    "x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)\n",
    "x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_NN(torch.nn.Module):\n",
    "    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None):\n",
    "        super(My_NN, self).__init__()\n",
    "        self.input_size = input_size if input_size is not None else 18 \n",
    "        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38\n",
    "        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38\n",
    "        self.output_size = output_size if output_size is not None else 18\n",
    "        \n",
    "        self.lin1 = Linear(self.input_size, self.hidden_size1)\n",
    "        self.lin2 = Linear(self.hidden_size1, self.hidden_size2)\n",
    "        self.lin3 = Linear(self.hidden_size2, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self, model, name):\n",
    "        torch.save(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0    train loss: 0.3544139    val loss: 0.3500614\n",
      "epoch: 10    train loss: 0.3186767    val loss: 0.3159057\n",
      "epoch: 20    train loss: 0.2727171    val loss: 0.2701104\n",
      "epoch: 30    train loss: 0.2071412    val loss: 0.2039799\n",
      "epoch: 40    train loss: 0.1276366    val loss: 0.1245110\n",
      "epoch: 50    train loss: 0.0614628    val loss: 0.0599505\n",
      "epoch: 60    train loss: 0.0307530    val loss: 0.0302672\n",
      "epoch: 70    train loss: 0.0208093    val loss: 0.0198023\n",
      "epoch: 80    train loss: 0.0167030    val loss: 0.0158139\n",
      "epoch: 90    train loss: 0.0151878    val loss: 0.0145732\n",
      "epoch: 100    train loss: 0.0140068    val loss: 0.0134536\n",
      "epoch: 110    train loss: 0.0129689    val loss: 0.0124188\n",
      "epoch: 120    train loss: 0.0120168    val loss: 0.0115251\n",
      "epoch: 130    train loss: 0.0111431    val loss: 0.0107224\n",
      "epoch: 140    train loss: 0.0103312    val loss: 0.0099432\n",
      "epoch: 150    train loss: 0.0095719    val loss: 0.0092016\n",
      "epoch: 160    train loss: 0.0088665    val loss: 0.0085237\n",
      "epoch: 170    train loss: 0.0082161    val loss: 0.0079049\n",
      "epoch: 180    train loss: 0.0076234    val loss: 0.0073412\n",
      "epoch: 190    train loss: 0.0070904    val loss: 0.0068391\n",
      "epoch: 200    train loss: 0.0066180    val loss: 0.0064014\n",
      "epoch: 210    train loss: 0.0062042    val loss: 0.0060237\n",
      "epoch: 220    train loss: 0.0058442    val loss: 0.0057009\n",
      "epoch: 230    train loss: 0.0055307    val loss: 0.0054265\n",
      "epoch: 240    train loss: 0.0052560    val loss: 0.0051914\n",
      "epoch: 250    train loss: 0.0050126    val loss: 0.0049875\n",
      "epoch: 260    train loss: 0.0047942    val loss: 0.0048084\n",
      "epoch: 270    train loss: 0.0045960    val loss: 0.0046485\n",
      "epoch: 280    train loss: 0.0044143    val loss: 0.0045038\n",
      "epoch: 290    train loss: 0.0042465    val loss: 0.0043715\n",
      "epoch: 300    train loss: 0.0040906    val loss: 0.0042494\n",
      "epoch: 310    train loss: 0.0039451    val loss: 0.0041360\n",
      "epoch: 320    train loss: 0.0038089    val loss: 0.0040301\n",
      "epoch: 330    train loss: 0.0036808    val loss: 0.0039306\n",
      "epoch: 340    train loss: 0.0035603    val loss: 0.0038370\n",
      "epoch: 350    train loss: 0.0034465    val loss: 0.0037485\n",
      "epoch: 360    train loss: 0.0033390    val loss: 0.0036648\n",
      "epoch: 370    train loss: 0.0032371    val loss: 0.0035853\n",
      "epoch: 380    train loss: 0.0031406    val loss: 0.0035098\n",
      "epoch: 390    train loss: 0.0030490    val loss: 0.0034379\n",
      "epoch: 400    train loss: 0.0029619    val loss: 0.0033693\n",
      "epoch: 410    train loss: 0.0028790    val loss: 0.0033038\n",
      "epoch: 420    train loss: 0.0028000    val loss: 0.0032411\n",
      "epoch: 430    train loss: 0.0027247    val loss: 0.0031812\n",
      "epoch: 440    train loss: 0.0026528    val loss: 0.0031237\n",
      "epoch: 450    train loss: 0.0025842    val loss: 0.0030685\n",
      "epoch: 460    train loss: 0.0025185    val loss: 0.0030156\n",
      "epoch: 470    train loss: 0.0024556    val loss: 0.0029646\n",
      "epoch: 480    train loss: 0.0023954    val loss: 0.0029157\n",
      "epoch: 490    train loss: 0.0023377    val loss: 0.0028685\n",
      "epoch: 500    train loss: 0.0022823    val loss: 0.0028230\n",
      "epoch: 510    train loss: 0.0022291    val loss: 0.0027792\n",
      "epoch: 520    train loss: 0.0021780    val loss: 0.0027369\n",
      "epoch: 530    train loss: 0.0021289    val loss: 0.0026960\n",
      "epoch: 540    train loss: 0.0020816    val loss: 0.0026565\n",
      "epoch: 550    train loss: 0.0020362    val loss: 0.0026183\n",
      "epoch: 560    train loss: 0.0019924    val loss: 0.0025814\n",
      "epoch: 570    train loss: 0.0019502    val loss: 0.0025456\n",
      "epoch: 580    train loss: 0.0019095    val loss: 0.0025109\n",
      "epoch: 590    train loss: 0.0018702    val loss: 0.0024773\n",
      "epoch: 600    train loss: 0.0018323    val loss: 0.0024447\n",
      "epoch: 610    train loss: 0.0017957    val loss: 0.0024130\n",
      "epoch: 620    train loss: 0.0017603    val loss: 0.0023822\n",
      "epoch: 630    train loss: 0.0017261    val loss: 0.0023523\n",
      "epoch: 640    train loss: 0.0016930    val loss: 0.0023232\n",
      "epoch: 650    train loss: 0.0016610    val loss: 0.0022949\n",
      "epoch: 660    train loss: 0.0016299    val loss: 0.0022673\n",
      "epoch: 670    train loss: 0.0015999    val loss: 0.0022404\n",
      "epoch: 680    train loss: 0.0015708    val loss: 0.0022142\n",
      "epoch: 690    train loss: 0.0015425    val loss: 0.0021887\n",
      "epoch: 700    train loss: 0.0015151    val loss: 0.0021638\n",
      "epoch: 710    train loss: 0.0014885    val loss: 0.0021395\n",
      "epoch: 720    train loss: 0.0014626    val loss: 0.0021157\n",
      "epoch: 730    train loss: 0.0014375    val loss: 0.0020926\n",
      "epoch: 740    train loss: 0.0014131    val loss: 0.0020699\n",
      "epoch: 750    train loss: 0.0013893    val loss: 0.0020478\n",
      "epoch: 760    train loss: 0.0013662    val loss: 0.0020261\n",
      "epoch: 770    train loss: 0.0013437    val loss: 0.0020049\n",
      "epoch: 780    train loss: 0.0013218    val loss: 0.0019842\n",
      "epoch: 790    train loss: 0.0013005    val loss: 0.0019640\n",
      "epoch: 800    train loss: 0.0012797    val loss: 0.0019441\n",
      "epoch: 810    train loss: 0.0012595    val loss: 0.0019247\n",
      "epoch: 820    train loss: 0.0012397    val loss: 0.0019057\n",
      "epoch: 830    train loss: 0.0012204    val loss: 0.0018871\n",
      "epoch: 840    train loss: 0.0012016    val loss: 0.0018689\n",
      "epoch: 850    train loss: 0.0011833    val loss: 0.0018511\n",
      "epoch: 860    train loss: 0.0011654    val loss: 0.0018336\n",
      "epoch: 870    train loss: 0.0011479    val loss: 0.0018165\n",
      "epoch: 880    train loss: 0.0011308    val loss: 0.0017997\n",
      "epoch: 890    train loss: 0.0011142    val loss: 0.0017832\n",
      "epoch: 900    train loss: 0.0010979    val loss: 0.0017671\n",
      "epoch: 910    train loss: 0.0010819    val loss: 0.0017513\n",
      "epoch: 920    train loss: 0.0010664    val loss: 0.0017357\n",
      "epoch: 930    train loss: 0.0010512    val loss: 0.0017205\n",
      "epoch: 940    train loss: 0.0010363    val loss: 0.0017056\n",
      "epoch: 950    train loss: 0.0010217    val loss: 0.0016910\n",
      "epoch: 960    train loss: 0.0010075    val loss: 0.0016766\n",
      "epoch: 970    train loss: 0.0009936    val loss: 0.0016625\n",
      "epoch: 980    train loss: 0.0009799    val loss: 0.0016486\n",
      "epoch: 990    train loss: 0.0009666    val loss: 0.0016350\n",
      "epoch: 1000    train loss: 0.0009535    val loss: 0.0016217\n",
      "epoch: 1010    train loss: 0.0009407    val loss: 0.0016085\n",
      "epoch: 1020    train loss: 0.0009282    val loss: 0.0015957\n",
      "epoch: 1030    train loss: 0.0009160    val loss: 0.0015830\n",
      "epoch: 1040    train loss: 0.0009040    val loss: 0.0015706\n",
      "epoch: 1050    train loss: 0.0008922    val loss: 0.0015583\n",
      "epoch: 1060    train loss: 0.0008807    val loss: 0.0015463\n",
      "epoch: 1070    train loss: 0.0008694    val loss: 0.0015344\n",
      "epoch: 1080    train loss: 0.0008583    val loss: 0.0015228\n",
      "epoch: 1090    train loss: 0.0008475    val loss: 0.0015113\n",
      "epoch: 1100    train loss: 0.0008369    val loss: 0.0015001\n",
      "epoch: 1110    train loss: 0.0008265    val loss: 0.0014890\n",
      "epoch: 1120    train loss: 0.0008162    val loss: 0.0014780\n",
      "epoch: 1130    train loss: 0.0008062    val loss: 0.0014673\n",
      "epoch: 1140    train loss: 0.0007964    val loss: 0.0014567\n",
      "epoch: 1150    train loss: 0.0007868    val loss: 0.0014462\n",
      "epoch: 1160    train loss: 0.0007773    val loss: 0.0014360\n",
      "epoch: 1170    train loss: 0.0007681    val loss: 0.0014258\n",
      "epoch: 1180    train loss: 0.0007590    val loss: 0.0014158\n",
      "epoch: 1190    train loss: 0.0007500    val loss: 0.0014060\n",
      "epoch: 1200    train loss: 0.0007413    val loss: 0.0013962\n",
      "epoch: 1210    train loss: 0.0007327    val loss: 0.0013866\n",
      "epoch: 1220    train loss: 0.0007242    val loss: 0.0013772\n",
      "epoch: 1230    train loss: 0.0007159    val loss: 0.0013678\n",
      "epoch: 1240    train loss: 0.0007079    val loss: 0.0013583\n",
      "epoch: 1250    train loss: 0.0006998    val loss: 0.0013504\n",
      "epoch: 1260    train loss: 0.0006919    val loss: 0.0013405\n",
      "epoch: 1270    train loss: 0.0006842    val loss: 0.0013321\n",
      "epoch: 1280    train loss: 0.0006766    val loss: 0.0013228\n",
      "epoch: 1290    train loss: 0.0006692    val loss: 0.0013142\n",
      "epoch: 1300    train loss: 0.0006618    val loss: 0.0013055\n",
      "epoch: 1310    train loss: 0.0006548    val loss: 0.0012962\n",
      "epoch: 1320    train loss: 0.0006476    val loss: 0.0012897\n",
      "epoch: 1330    train loss: 0.0006406    val loss: 0.0012803\n",
      "epoch: 1340    train loss: 0.0006338    val loss: 0.0012730\n",
      "epoch: 1350    train loss: 0.0006272    val loss: 0.0012655\n",
      "epoch: 1360    train loss: 0.0006205    val loss: 0.0012568\n",
      "epoch: 1370    train loss: 0.0006139    val loss: 0.0012493\n",
      "epoch: 1380    train loss: 0.0006076    val loss: 0.0012421\n",
      "epoch: 1390    train loss: 0.0006012    val loss: 0.0012337\n",
      "epoch: 1400    train loss: 0.0005950    val loss: 0.0012247\n",
      "epoch: 1410    train loss: 0.0005890    val loss: 0.0012173\n",
      "epoch: 1420    train loss: 0.0005829    val loss: 0.0012111\n",
      "epoch: 1430    train loss: 0.0005770    val loss: 0.0012041\n",
      "epoch: 1440    train loss: 0.0005712    val loss: 0.0011962\n",
      "epoch: 1450    train loss: 0.0005656    val loss: 0.0011892\n",
      "epoch: 1460    train loss: 0.0005599    val loss: 0.0011819\n",
      "epoch: 1470    train loss: 0.0005547    val loss: 0.0011778\n",
      "epoch: 1480    train loss: 0.0005490    val loss: 0.0011663\n",
      "epoch: 1490    train loss: 0.0005435    val loss: 0.0011614\n",
      "epoch: 1500    train loss: 0.0005381    val loss: 0.0011535\n",
      "epoch: 1510    train loss: 0.0005329    val loss: 0.0011474\n",
      "epoch: 1520    train loss: 0.0005280    val loss: 0.0011405\n",
      "epoch: 1530    train loss: 0.0005227    val loss: 0.0011336\n",
      "epoch: 1540    train loss: 0.0005176    val loss: 0.0011271\n",
      "epoch: 1550    train loss: 0.0005127    val loss: 0.0011208\n",
      "epoch: 1560    train loss: 0.0005084    val loss: 0.0011174\n",
      "epoch: 1570    train loss: 0.0005034    val loss: 0.0011063\n",
      "epoch: 1580    train loss: 0.0004985    val loss: 0.0011023\n",
      "epoch: 1590    train loss: 0.0004937    val loss: 0.0010950\n",
      "epoch: 1600    train loss: 0.0004891    val loss: 0.0010886\n",
      "epoch: 1610    train loss: 0.0004848    val loss: 0.0010824\n",
      "epoch: 1620    train loss: 0.0004802    val loss: 0.0010766\n",
      "epoch: 1630    train loss: 0.0004759    val loss: 0.0010702\n",
      "epoch: 1640    train loss: 0.0004714    val loss: 0.0010646\n",
      "epoch: 1650    train loss: 0.0004671    val loss: 0.0010586\n",
      "epoch: 1660    train loss: 0.0004630    val loss: 0.0010543\n",
      "epoch: 1670    train loss: 0.0004590    val loss: 0.0010470\n",
      "epoch: 1680    train loss: 0.0004549    val loss: 0.0010422\n",
      "epoch: 1690    train loss: 0.0004506    val loss: 0.0010354\n",
      "epoch: 1700    train loss: 0.0004466    val loss: 0.0010299\n",
      "epoch: 1710    train loss: 0.0004428    val loss: 0.0010239\n",
      "epoch: 1720    train loss: 0.0004391    val loss: 0.0010184\n",
      "epoch: 1730    train loss: 0.0004350    val loss: 0.0010127\n",
      "epoch: 1740    train loss: 0.0004312    val loss: 0.0010081\n",
      "epoch: 1750    train loss: 0.0004274    val loss: 0.0010021\n",
      "epoch: 1760    train loss: 0.0004237    val loss: 0.0009971\n",
      "epoch: 1770    train loss: 0.0004200    val loss: 0.0009912\n",
      "epoch: 1780    train loss: 0.0004179    val loss: 0.0009841\n",
      "epoch: 1790    train loss: 0.0004137    val loss: 0.0009834\n",
      "epoch: 1800    train loss: 0.0004097    val loss: 0.0009774\n",
      "epoch: 1810    train loss: 0.0004060    val loss: 0.0009707\n",
      "epoch: 1820    train loss: 0.0004026    val loss: 0.0009660\n",
      "epoch: 1830    train loss: 0.0003995    val loss: 0.0009621\n",
      "epoch: 1840    train loss: 0.0003960    val loss: 0.0009566\n",
      "epoch: 1850    train loss: 0.0003928    val loss: 0.0009512\n",
      "epoch: 1860    train loss: 0.0003894    val loss: 0.0009463\n",
      "epoch: 1870    train loss: 0.0003880    val loss: 0.0009423\n",
      "epoch: 1880    train loss: 0.0003835    val loss: 0.0009383\n",
      "epoch: 1890    train loss: 0.0003799    val loss: 0.0009318\n",
      "epoch: 1900    train loss: 0.0003767    val loss: 0.0009276\n",
      "epoch: 1910    train loss: 0.0003743    val loss: 0.0009266\n",
      "epoch: 1920    train loss: 0.0003713    val loss: 0.0009152\n",
      "epoch: 1930    train loss: 0.0003679    val loss: 0.0009135\n",
      "epoch: 1940    train loss: 0.0003647    val loss: 0.0009102\n",
      "epoch: 1950    train loss: 0.0003618    val loss: 0.0009043\n",
      "epoch: 1960    train loss: 0.0003607    val loss: 0.0009032\n",
      "epoch: 1970    train loss: 0.0003585    val loss: 0.0008981\n",
      "epoch: 1980    train loss: 0.0003536    val loss: 0.0008901\n",
      "epoch: 1990    train loss: 0.0003506    val loss: 0.0008870\n",
      "epoch: 2000    train loss: 0.0003476    val loss: 0.0008825\n",
      "epoch: 2010    train loss: 0.0003449    val loss: 0.0008787\n",
      "epoch: 2020    train loss: 0.0003422    val loss: 0.0008742\n",
      "epoch: 2030    train loss: 0.0003402    val loss: 0.0008689\n",
      "epoch: 2040    train loss: 0.0003383    val loss: 0.0008665\n",
      "epoch: 2050    train loss: 0.0003347    val loss: 0.0008603\n",
      "epoch: 2060    train loss: 0.0003325    val loss: 0.0008592\n",
      "epoch: 2070    train loss: 0.0003294    val loss: 0.0008542\n",
      "epoch: 2080    train loss: 0.0003269    val loss: 0.0008504\n",
      "epoch: 2090    train loss: 0.0003243    val loss: 0.0008462\n",
      "epoch: 2100    train loss: 0.0003222    val loss: 0.0008431\n",
      "epoch: 2110    train loss: 0.0003225    val loss: 0.0008398\n",
      "epoch: 2120    train loss: 0.0003177    val loss: 0.0008352\n",
      "epoch: 2130    train loss: 0.0003149    val loss: 0.0008304\n",
      "epoch: 2140    train loss: 0.0003124    val loss: 0.0008271\n",
      "epoch: 2150    train loss: 0.0003100    val loss: 0.0008237\n",
      "epoch: 2160    train loss: 0.0003079    val loss: 0.0008192\n",
      "epoch: 2170    train loss: 0.0003081    val loss: 0.0008170\n",
      "epoch: 2180    train loss: 0.0003038    val loss: 0.0008135\n",
      "epoch: 2190    train loss: 0.0003012    val loss: 0.0008101\n",
      "epoch: 2200    train loss: 0.0002993    val loss: 0.0008054\n",
      "epoch: 2210    train loss: 0.0002984    val loss: 0.0008042\n",
      "epoch: 2220    train loss: 0.0002949    val loss: 0.0007988\n",
      "epoch: 2230    train loss: 0.0002926    val loss: 0.0007943\n",
      "epoch: 2240    train loss: 0.0002904    val loss: 0.0007911\n",
      "epoch: 2250    train loss: 0.0002883    val loss: 0.0007884\n",
      "epoch: 2260    train loss: 0.0002901    val loss: 0.0007911\n",
      "epoch: 2270    train loss: 0.0002861    val loss: 0.0007815\n",
      "epoch: 2280    train loss: 0.0002832    val loss: 0.0007775\n",
      "epoch: 2290    train loss: 0.0002802    val loss: 0.0007754\n",
      "epoch: 2300    train loss: 0.0002782    val loss: 0.0007719\n",
      "epoch: 2310    train loss: 0.0002762    val loss: 0.0007682\n",
      "epoch: 2320    train loss: 0.0002746    val loss: 0.0007652\n",
      "epoch: 2330    train loss: 0.0002734    val loss: 0.0007618\n",
      "epoch: 2340    train loss: 0.0002708    val loss: 0.0007585\n",
      "epoch: 2350    train loss: 0.0002711    val loss: 0.0007547\n",
      "epoch: 2360    train loss: 0.0002675    val loss: 0.0007550\n",
      "epoch: 2370    train loss: 0.0002655    val loss: 0.0007512\n",
      "epoch: 2380    train loss: 0.0002641    val loss: 0.0007476\n",
      "epoch: 2390    train loss: 0.0002617    val loss: 0.0007439\n",
      "epoch: 2400    train loss: 0.0002599    val loss: 0.0007407\n",
      "epoch: 2410    train loss: 0.0002581    val loss: 0.0007381\n",
      "epoch: 2420    train loss: 0.0002568    val loss: 0.0007351\n",
      "epoch: 2430    train loss: 0.0002568    val loss: 0.0007332\n",
      "epoch: 2440    train loss: 0.0002536    val loss: 0.0007300\n",
      "epoch: 2450    train loss: 0.0002513    val loss: 0.0007258\n",
      "epoch: 2460    train loss: 0.0002497    val loss: 0.0007218\n",
      "epoch: 2470    train loss: 0.0002492    val loss: 0.0007190\n",
      "epoch: 2480    train loss: 0.0002476    val loss: 0.0007194\n",
      "epoch: 2490    train loss: 0.0002453    val loss: 0.0007139\n",
      "epoch: 2500    train loss: 0.0002431    val loss: 0.0007136\n",
      "epoch: 2510    train loss: 0.0002415    val loss: 0.0007094\n",
      "epoch: 2520    train loss: 0.0002413    val loss: 0.0007095\n",
      "epoch: 2530    train loss: 0.0002387    val loss: 0.0007049\n",
      "epoch: 2540    train loss: 0.0002369    val loss: 0.0007013\n",
      "epoch: 2550    train loss: 0.0002351    val loss: 0.0006988\n",
      "epoch: 2560    train loss: 0.0002340    val loss: 0.0006959\n",
      "epoch: 2570    train loss: 0.0002356    val loss: 0.0006933\n",
      "epoch: 2580    train loss: 0.0002319    val loss: 0.0006939\n",
      "epoch: 2590    train loss: 0.0002294    val loss: 0.0006885\n",
      "epoch: 2600    train loss: 0.0002277    val loss: 0.0006859\n",
      "epoch: 2610    train loss: 0.0002265    val loss: 0.0006846\n",
      "epoch: 2620    train loss: 0.0002321    val loss: 0.0006897\n",
      "epoch: 2630    train loss: 0.0002248    val loss: 0.0006781\n",
      "epoch: 2640    train loss: 0.0002226    val loss: 0.0006753\n",
      "epoch: 2650    train loss: 0.0002209    val loss: 0.0006741\n",
      "epoch: 2660    train loss: 0.0002193    val loss: 0.0006717\n",
      "epoch: 2670    train loss: 0.0002178    val loss: 0.0006690\n",
      "epoch: 2680    train loss: 0.0002164    val loss: 0.0006666\n",
      "epoch: 2690    train loss: 0.0002160    val loss: 0.0006639\n",
      "epoch: 2700    train loss: 0.0002150    val loss: 0.0006641\n",
      "epoch: 2710    train loss: 0.0002137    val loss: 0.0006588\n",
      "epoch: 2720    train loss: 0.0002120    val loss: 0.0006581\n",
      "epoch: 2730    train loss: 0.0002101    val loss: 0.0006559\n",
      "epoch: 2740    train loss: 0.0002088    val loss: 0.0006531\n",
      "epoch: 2750    train loss: 0.0002080    val loss: 0.0006518\n",
      "epoch: 2760    train loss: 0.0002085    val loss: 0.0006518\n",
      "epoch: 2770    train loss: 0.0002056    val loss: 0.0006459\n",
      "epoch: 2780    train loss: 0.0002040    val loss: 0.0006451\n",
      "epoch: 2790    train loss: 0.0002029    val loss: 0.0006422\n",
      "epoch: 2800    train loss: 0.0002030    val loss: 0.0006406\n",
      "epoch: 2810    train loss: 0.0002017    val loss: 0.0006412\n",
      "epoch: 2820    train loss: 0.0001997    val loss: 0.0006375\n",
      "epoch: 2830    train loss: 0.0001979    val loss: 0.0006328\n",
      "epoch: 2840    train loss: 0.0001971    val loss: 0.0006323\n",
      "epoch: 2850    train loss: 0.0001991    val loss: 0.0006315\n",
      "epoch: 2860    train loss: 0.0001947    val loss: 0.0006292\n",
      "epoch: 2870    train loss: 0.0001932    val loss: 0.0006259\n",
      "epoch: 2880    train loss: 0.0001919    val loss: 0.0006231\n",
      "epoch: 2890    train loss: 0.0001911    val loss: 0.0006210\n",
      "epoch: 2900    train loss: 0.0001920    val loss: 0.0006196\n",
      "epoch: 2910    train loss: 0.0001895    val loss: 0.0006197\n",
      "epoch: 2920    train loss: 0.0001888    val loss: 0.0006170\n",
      "epoch: 2930    train loss: 0.0001874    val loss: 0.0006152\n",
      "epoch: 2940    train loss: 0.0001855    val loss: 0.0006114\n",
      "epoch: 2950    train loss: 0.0001843    val loss: 0.0006095\n",
      "epoch: 2960    train loss: 0.0001832    val loss: 0.0006078\n",
      "epoch: 2970    train loss: 0.0001838    val loss: 0.0006090\n",
      "epoch: 2980    train loss: 0.0001820    val loss: 0.0006067\n",
      "epoch: 2990    train loss: 0.0001817    val loss: 0.0006039\n",
      "epoch: 3000    train loss: 0.0001790    val loss: 0.0006004\n",
      "epoch: 3010    train loss: 0.0001782    val loss: 0.0005989\n",
      "epoch: 3020    train loss: 0.0001770    val loss: 0.0005968\n",
      "epoch: 3030    train loss: 0.0001758    val loss: 0.0005949\n",
      "epoch: 3040    train loss: 0.0001748    val loss: 0.0005932\n",
      "epoch: 3050    train loss: 0.0001769    val loss: 0.0005957\n",
      "epoch: 3060    train loss: 0.0001752    val loss: 0.0005938\n",
      "epoch: 3070    train loss: 0.0001723    val loss: 0.0005873\n",
      "epoch: 3080    train loss: 0.0001713    val loss: 0.0005863\n",
      "epoch: 3090    train loss: 0.0001702    val loss: 0.0005849\n",
      "epoch: 3100    train loss: 0.0001694    val loss: 0.0005834\n",
      "epoch: 3110    train loss: 0.0001697    val loss: 0.0005837\n",
      "epoch: 3120    train loss: 0.0001676    val loss: 0.0005790\n",
      "epoch: 3130    train loss: 0.0001663    val loss: 0.0005779\n",
      "epoch: 3140    train loss: 0.0001654    val loss: 0.0005760\n",
      "epoch: 3150    train loss: 0.0001644    val loss: 0.0005746\n",
      "epoch: 3160    train loss: 0.0001637    val loss: 0.0005730\n",
      "epoch: 3170    train loss: 0.0001670    val loss: 0.0005758\n",
      "epoch: 3180    train loss: 0.0001642    val loss: 0.0005722\n",
      "epoch: 3190    train loss: 0.0001620    val loss: 0.0005679\n",
      "epoch: 3200    train loss: 0.0001603    val loss: 0.0005674\n",
      "epoch: 3210    train loss: 0.0001593    val loss: 0.0005643\n",
      "epoch: 3220    train loss: 0.0001589    val loss: 0.0005636\n",
      "epoch: 3230    train loss: 0.0001607    val loss: 0.0005636\n",
      "epoch: 3240    train loss: 0.0001580    val loss: 0.0005600\n",
      "epoch: 3250    train loss: 0.0001560    val loss: 0.0005589\n",
      "epoch: 3260    train loss: 0.0001551    val loss: 0.0005577\n",
      "epoch: 3270    train loss: 0.0001551    val loss: 0.0005576\n",
      "epoch: 3280    train loss: 0.0001551    val loss: 0.0005572\n",
      "epoch: 3290    train loss: 0.0001536    val loss: 0.0005543\n",
      "epoch: 3300    train loss: 0.0001520    val loss: 0.0005502\n",
      "epoch: 3310    train loss: 0.0001509    val loss: 0.0005500\n",
      "epoch: 3320    train loss: 0.0001501    val loss: 0.0005483\n",
      "epoch: 3330    train loss: 0.0001552    val loss: 0.0005530\n",
      "epoch: 3340    train loss: 0.0001499    val loss: 0.0005463\n",
      "epoch: 3350    train loss: 0.0001479    val loss: 0.0005438\n",
      "epoch: 3360    train loss: 0.0001470    val loss: 0.0005433\n",
      "epoch: 3370    train loss: 0.0001474    val loss: 0.0005440\n",
      "epoch: 3380    train loss: 0.0001461    val loss: 0.0005394\n",
      "epoch: 3390    train loss: 0.0001456    val loss: 0.0005394\n",
      "epoch: 3400    train loss: 0.0001440    val loss: 0.0005370\n",
      "epoch: 3410    train loss: 0.0001429    val loss: 0.0005351\n",
      "epoch: 3420    train loss: 0.0001427    val loss: 0.0005354\n",
      "epoch: 3430    train loss: 0.0001531    val loss: 0.0005466\n",
      "epoch: 3440    train loss: 0.0001446    val loss: 0.0005339\n",
      "epoch: 3450    train loss: 0.0001412    val loss: 0.0005284\n",
      "epoch: 3460    train loss: 0.0001396    val loss: 0.0005295\n",
      "epoch: 3470    train loss: 0.0001386    val loss: 0.0005279\n",
      "epoch: 3480    train loss: 0.0001382    val loss: 0.0005265\n",
      "epoch: 3490    train loss: 0.0001422    val loss: 0.0005265\n",
      "epoch: 3500    train loss: 0.0001380    val loss: 0.0005246\n",
      "epoch: 3510    train loss: 0.0001361    val loss: 0.0005219\n",
      "epoch: 3520    train loss: 0.0001350    val loss: 0.0005204\n",
      "epoch: 3530    train loss: 0.0001343    val loss: 0.0005199\n",
      "epoch: 3540    train loss: 0.0001347    val loss: 0.0005201\n",
      "epoch: 3550    train loss: 0.0001399    val loss: 0.0005219\n",
      "epoch: 3560    train loss: 0.0001396    val loss: 0.0005256\n",
      "epoch: 3570    train loss: 0.0001326    val loss: 0.0005160\n",
      "epoch: 3580    train loss: 0.0001318    val loss: 0.0005139\n",
      "epoch: 3590    train loss: 0.0001303    val loss: 0.0005120\n",
      "epoch: 3600    train loss: 0.0001296    val loss: 0.0005100\n",
      "epoch: 3610    train loss: 0.0001288    val loss: 0.0005093\n",
      "epoch: 3620    train loss: 0.0001282    val loss: 0.0005079\n",
      "epoch: 3630    train loss: 0.0001281    val loss: 0.0005082\n",
      "epoch: 3640    train loss: 0.0001408    val loss: 0.0005152\n",
      "epoch: 3650    train loss: 0.0001303    val loss: 0.0005110\n",
      "epoch: 3660    train loss: 0.0001264    val loss: 0.0005048\n",
      "epoch: 3670    train loss: 0.0001254    val loss: 0.0005014\n",
      "epoch: 3680    train loss: 0.0001245    val loss: 0.0005009\n",
      "epoch: 3690    train loss: 0.0001237    val loss: 0.0005001\n",
      "epoch: 3700    train loss: 0.0001233    val loss: 0.0004989\n",
      "epoch: 3710    train loss: 0.0001298    val loss: 0.0005112\n",
      "epoch: 3720    train loss: 0.0001245    val loss: 0.0004958\n",
      "epoch: 3730    train loss: 0.0001226    val loss: 0.0004968\n",
      "epoch: 3740    train loss: 0.0001213    val loss: 0.0004950\n",
      "epoch: 3750    train loss: 0.0001202    val loss: 0.0004921\n",
      "epoch: 3760    train loss: 0.0001195    val loss: 0.0004911\n",
      "epoch: 3770    train loss: 0.0001194    val loss: 0.0004916\n",
      "epoch: 3780    train loss: 0.0001337    val loss: 0.0004957\n",
      "epoch: 3790    train loss: 0.0001202    val loss: 0.0004949\n",
      "epoch: 3800    train loss: 0.0001191    val loss: 0.0004914\n",
      "epoch: 3810    train loss: 0.0001171    val loss: 0.0004875\n",
      "epoch: 3820    train loss: 0.0001164    val loss: 0.0004836\n",
      "epoch: 3830    train loss: 0.0001168    val loss: 0.0004842\n",
      "epoch: 3840    train loss: 0.0001261    val loss: 0.0004943\n",
      "epoch: 3850    train loss: 0.0001191    val loss: 0.0004840\n",
      "epoch: 3860    train loss: 0.0001144    val loss: 0.0004807\n",
      "epoch: 3870    train loss: 0.0001135    val loss: 0.0004800\n",
      "epoch: 3880    train loss: 0.0001127    val loss: 0.0004783\n",
      "epoch: 3890    train loss: 0.0001126    val loss: 0.0004788\n",
      "epoch: 3900    train loss: 0.0001191    val loss: 0.0004819\n",
      "epoch: 3910    train loss: 0.0001138    val loss: 0.0004784\n",
      "epoch: 3920    train loss: 0.0001118    val loss: 0.0004774\n",
      "epoch: 3930    train loss: 0.0001102    val loss: 0.0004741\n",
      "epoch: 3940    train loss: 0.0001095    val loss: 0.0004714\n",
      "epoch: 3950    train loss: 0.0001105    val loss: 0.0004710\n",
      "epoch: 3960    train loss: 0.0001214    val loss: 0.0004790\n",
      "epoch: 3970    train loss: 0.0001107    val loss: 0.0004766\n",
      "epoch: 3980    train loss: 0.0001088    val loss: 0.0004676\n",
      "epoch: 3990    train loss: 0.0001075    val loss: 0.0004668\n",
      "epoch: 4000    train loss: 0.0001064    val loss: 0.0004665\n",
      "epoch: 4010    train loss: 0.0001063    val loss: 0.0004656\n",
      "epoch: 4020    train loss: 0.0001151    val loss: 0.0004737\n",
      "epoch: 4030    train loss: 0.0001079    val loss: 0.0004699\n",
      "epoch: 4040    train loss: 0.0001057    val loss: 0.0004657\n",
      "epoch: 4050    train loss: 0.0001046    val loss: 0.0004611\n",
      "epoch: 4060    train loss: 0.0001036    val loss: 0.0004619\n",
      "epoch: 4070    train loss: 0.0001033    val loss: 0.0004605\n",
      "epoch: 4080    train loss: 0.0001096    val loss: 0.0004706\n",
      "epoch: 4090    train loss: 0.0001055    val loss: 0.0004607\n",
      "epoch: 4100    train loss: 0.0001028    val loss: 0.0004592\n",
      "epoch: 4110    train loss: 0.0001020    val loss: 0.0004546\n",
      "epoch: 4120    train loss: 0.0001021    val loss: 0.0004556\n",
      "epoch: 4130    train loss: 0.0001055    val loss: 0.0004548\n",
      "epoch: 4140    train loss: 0.0001038    val loss: 0.0004551\n",
      "epoch: 4150    train loss: 0.0000999    val loss: 0.0004498\n",
      "epoch: 4160    train loss: 0.0000992    val loss: 0.0004530\n",
      "epoch: 4170    train loss: 0.0000990    val loss: 0.0004526\n",
      "epoch: 4180    train loss: 0.0001037    val loss: 0.0004608\n",
      "epoch: 4190    train loss: 0.0000982    val loss: 0.0004471\n",
      "epoch: 4200    train loss: 0.0000987    val loss: 0.0004457\n",
      "epoch: 4210    train loss: 0.0001084    val loss: 0.0004476\n",
      "epoch: 4220    train loss: 0.0000985    val loss: 0.0004542\n",
      "epoch: 4230    train loss: 0.0000959    val loss: 0.0004459\n",
      "epoch: 4240    train loss: 0.0000953    val loss: 0.0004450\n",
      "epoch: 4250    train loss: 0.0000946    val loss: 0.0004440\n",
      "epoch: 4260    train loss: 0.0000942    val loss: 0.0004438\n",
      "epoch: 4270    train loss: 0.0000951    val loss: 0.0004453\n",
      "epoch: 4280    train loss: 0.0001138    val loss: 0.0004690\n",
      "epoch: 4290    train loss: 0.0000963    val loss: 0.0004394\n",
      "epoch: 4300    train loss: 0.0000941    val loss: 0.0004369\n",
      "epoch: 4310    train loss: 0.0000924    val loss: 0.0004392\n",
      "epoch: 4320    train loss: 0.0000917    val loss: 0.0004380\n",
      "epoch: 4330    train loss: 0.0000911    val loss: 0.0004366\n",
      "epoch: 4340    train loss: 0.0000913    val loss: 0.0004395\n",
      "epoch: 4350    train loss: 0.0001102    val loss: 0.0004669\n",
      "epoch: 4360    train loss: 0.0000966    val loss: 0.0004354\n",
      "epoch: 4370    train loss: 0.0000907    val loss: 0.0004349\n",
      "epoch: 4380    train loss: 0.0000892    val loss: 0.0004327\n",
      "epoch: 4390    train loss: 0.0000889    val loss: 0.0004321\n",
      "epoch: 4400    train loss: 0.0000883    val loss: 0.0004318\n",
      "epoch: 4410    train loss: 0.0000878    val loss: 0.0004298\n",
      "epoch: 4420    train loss: 0.0000876    val loss: 0.0004294\n",
      "epoch: 4430    train loss: 0.0000948    val loss: 0.0004346\n",
      "epoch: 4440    train loss: 0.0000917    val loss: 0.0004316\n",
      "epoch: 4450    train loss: 0.0000898    val loss: 0.0004297\n",
      "epoch: 4460    train loss: 0.0000876    val loss: 0.0004308\n",
      "epoch: 4470    train loss: 0.0000858    val loss: 0.0004254\n",
      "epoch: 4480    train loss: 0.0000857    val loss: 0.0004238\n",
      "epoch: 4490    train loss: 0.0000887    val loss: 0.0004260\n",
      "epoch: 4500    train loss: 0.0000874    val loss: 0.0004198\n",
      "epoch: 4510    train loss: 0.0000917    val loss: 0.0004238\n",
      "epoch: 4520    train loss: 0.0000853    val loss: 0.0004267\n",
      "epoch: 4530    train loss: 0.0000838    val loss: 0.0004221\n",
      "epoch: 4540    train loss: 0.0000836    val loss: 0.0004236\n",
      "epoch: 4550    train loss: 0.0000864    val loss: 0.0004289\n",
      "epoch: 4560    train loss: 0.0000850    val loss: 0.0004221\n",
      "epoch: 4570    train loss: 0.0000852    val loss: 0.0004169\n",
      "epoch: 4580    train loss: 0.0000974    val loss: 0.0004269\n",
      "epoch: 4590    train loss: 0.0000854    val loss: 0.0004239\n",
      "epoch: 4600    train loss: 0.0000824    val loss: 0.0004156\n",
      "epoch: 4610    train loss: 0.0000808    val loss: 0.0004144\n",
      "epoch: 4620    train loss: 0.0000800    val loss: 0.0004144\n",
      "epoch: 4630    train loss: 0.0000800    val loss: 0.0004145\n",
      "epoch: 4640    train loss: 0.0000905    val loss: 0.0004230\n",
      "epoch: 4650    train loss: 0.0000812    val loss: 0.0004196\n",
      "epoch: 4660    train loss: 0.0000794    val loss: 0.0004111\n",
      "epoch: 4670    train loss: 0.0000785    val loss: 0.0004108\n",
      "epoch: 4680    train loss: 0.0000781    val loss: 0.0004116\n",
      "epoch: 4690    train loss: 0.0000842    val loss: 0.0004242\n",
      "epoch: 4700    train loss: 0.0000831    val loss: 0.0004151\n",
      "epoch: 4710    train loss: 0.0000777    val loss: 0.0004080\n",
      "epoch: 4720    train loss: 0.0000772    val loss: 0.0004089\n",
      "epoch: 4730    train loss: 0.0000766    val loss: 0.0004087\n",
      "epoch: 4740    train loss: 0.0000758    val loss: 0.0004060\n",
      "epoch: 4750    train loss: 0.0000753    val loss: 0.0004062\n",
      "epoch: 4760    train loss: 0.0000759    val loss: 0.0004072\n",
      "epoch: 4770    train loss: 0.0000904    val loss: 0.0004239\n",
      "epoch: 4780    train loss: 0.0000865    val loss: 0.0004178\n",
      "epoch: 4790    train loss: 0.0000775    val loss: 0.0004009\n",
      "epoch: 4800    train loss: 0.0000752    val loss: 0.0004039\n",
      "epoch: 4810    train loss: 0.0000737    val loss: 0.0004031\n",
      "epoch: 4820    train loss: 0.0000733    val loss: 0.0003986\n",
      "epoch: 4830    train loss: 0.0000753    val loss: 0.0004011\n",
      "epoch: 4840    train loss: 0.0000766    val loss: 0.0003985\n",
      "epoch: 4850    train loss: 0.0000802    val loss: 0.0004063\n",
      "epoch: 4860    train loss: 0.0000768    val loss: 0.0003955\n",
      "epoch: 4870    train loss: 0.0000733    val loss: 0.0004012\n",
      "epoch: 4880    train loss: 0.0000721    val loss: 0.0003954\n",
      "epoch: 4890    train loss: 0.0000709    val loss: 0.0003959\n",
      "epoch: 4900    train loss: 0.0000706    val loss: 0.0003943\n",
      "epoch: 4910    train loss: 0.0000725    val loss: 0.0003931\n",
      "epoch: 4920    train loss: 0.0000900    val loss: 0.0003976\n",
      "epoch: 4930    train loss: 0.0000741    val loss: 0.0003970\n",
      "epoch: 4940    train loss: 0.0000694    val loss: 0.0003940\n",
      "epoch: 4950    train loss: 0.0000690    val loss: 0.0003902\n",
      "epoch: 4960    train loss: 0.0000686    val loss: 0.0003920\n",
      "epoch: 4970    train loss: 0.0000682    val loss: 0.0003897\n",
      "epoch: 4980    train loss: 0.0000682    val loss: 0.0003901\n",
      "epoch: 4990    train loss: 0.0000914    val loss: 0.0004106\n",
      "epoch: 5000    train loss: 0.0000766    val loss: 0.0004005\n",
      "epoch: 5010    train loss: 0.0000703    val loss: 0.0003972\n",
      "epoch: 5020    train loss: 0.0000675    val loss: 0.0003974\n",
      "epoch: 5030    train loss: 0.0000668    val loss: 0.0003867\n",
      "epoch: 5040    train loss: 0.0000663    val loss: 0.0003851\n",
      "epoch: 5050    train loss: 0.0000661    val loss: 0.0003858\n",
      "epoch: 5060    train loss: 0.0000749    val loss: 0.0003981\n",
      "epoch: 5070    train loss: 0.0000699    val loss: 0.0003826\n",
      "epoch: 5080    train loss: 0.0000706    val loss: 0.0003845\n",
      "epoch: 5090    train loss: 0.0000657    val loss: 0.0003849\n",
      "epoch: 5100    train loss: 0.0000655    val loss: 0.0003871\n",
      "epoch: 5110    train loss: 0.0000654    val loss: 0.0003864\n",
      "epoch: 5120    train loss: 0.0000677    val loss: 0.0003925\n",
      "epoch: 5130    train loss: 0.0000705    val loss: 0.0003944\n",
      "epoch: 5140    train loss: 0.0000645    val loss: 0.0003784\n",
      "epoch: 5150    train loss: 0.0000641    val loss: 0.0003770\n",
      "epoch: 5160    train loss: 0.0000630    val loss: 0.0003793\n",
      "epoch: 5170    train loss: 0.0000640    val loss: 0.0003806\n",
      "epoch: 5180    train loss: 0.0000755    val loss: 0.0003852\n",
      "epoch: 5190    train loss: 0.0000693    val loss: 0.0003790\n",
      "epoch: 5200    train loss: 0.0000648    val loss: 0.0003809\n",
      "epoch: 5210    train loss: 0.0000628    val loss: 0.0003773\n",
      "epoch: 5220    train loss: 0.0000617    val loss: 0.0003776\n",
      "epoch: 5230    train loss: 0.0000639    val loss: 0.0003800\n",
      "epoch: 5240    train loss: 0.0000669    val loss: 0.0003825\n",
      "epoch: 5250    train loss: 0.0000640    val loss: 0.0003824\n",
      "epoch: 5260    train loss: 0.0000637    val loss: 0.0003766\n",
      "epoch: 5270    train loss: 0.0000606    val loss: 0.0003741\n",
      "epoch: 5280    train loss: 0.0000598    val loss: 0.0003729\n",
      "epoch: 5290    train loss: 0.0000595    val loss: 0.0003697\n",
      "epoch: 5300    train loss: 0.0000624    val loss: 0.0003697\n",
      "epoch: 5310    train loss: 0.0000759    val loss: 0.0003704\n",
      "epoch: 5320    train loss: 0.0000641    val loss: 0.0003727\n",
      "epoch: 5330    train loss: 0.0000588    val loss: 0.0003715\n",
      "epoch: 5340    train loss: 0.0000581    val loss: 0.0003652\n",
      "epoch: 5350    train loss: 0.0000577    val loss: 0.0003694\n",
      "epoch: 5360    train loss: 0.0000573    val loss: 0.0003662\n",
      "epoch: 5370    train loss: 0.0000574    val loss: 0.0003675\n",
      "epoch: 5380    train loss: 0.0000698    val loss: 0.0003845\n",
      "epoch: 5390    train loss: 0.0000639    val loss: 0.0003666\n",
      "epoch: 5400    train loss: 0.0000602    val loss: 0.0003698\n",
      "epoch: 5410    train loss: 0.0000564    val loss: 0.0003651\n",
      "epoch: 5420    train loss: 0.0000563    val loss: 0.0003629\n",
      "epoch: 5430    train loss: 0.0000558    val loss: 0.0003642\n",
      "epoch: 5440    train loss: 0.0000569    val loss: 0.0003676\n",
      "epoch: 5450    train loss: 0.0000850    val loss: 0.0004022\n",
      "epoch: 5460    train loss: 0.0000650    val loss: 0.0003615\n",
      "epoch: 5470    train loss: 0.0000557    val loss: 0.0003619\n",
      "epoch: 5480    train loss: 0.0000549    val loss: 0.0003628\n",
      "epoch: 5490    train loss: 0.0000543    val loss: 0.0003595\n",
      "epoch: 5500    train loss: 0.0000539    val loss: 0.0003595\n",
      "epoch: 5510    train loss: 0.0000536    val loss: 0.0003584\n",
      "epoch: 5520    train loss: 0.0000533    val loss: 0.0003579\n",
      "epoch: 5530    train loss: 0.0000531    val loss: 0.0003582\n",
      "epoch: 5540    train loss: 0.0000562    val loss: 0.0003684\n",
      "epoch: 5550    train loss: 0.0000713    val loss: 0.0003770\n",
      "epoch: 5560    train loss: 0.0000597    val loss: 0.0003660\n",
      "epoch: 5570    train loss: 0.0000535    val loss: 0.0003596\n",
      "epoch: 5580    train loss: 0.0000531    val loss: 0.0003553\n",
      "epoch: 5590    train loss: 0.0000518    val loss: 0.0003542\n",
      "epoch: 5600    train loss: 0.0000516    val loss: 0.0003525\n",
      "epoch: 5610    train loss: 0.0000512    val loss: 0.0003528\n",
      "epoch: 5620    train loss: 0.0000510    val loss: 0.0003528\n",
      "epoch: 5630    train loss: 0.0000521    val loss: 0.0003526\n",
      "epoch: 5640    train loss: 0.0000691    val loss: 0.0003694\n",
      "epoch: 5650    train loss: 0.0000548    val loss: 0.0003572\n",
      "epoch: 5660    train loss: 0.0000525    val loss: 0.0003569\n",
      "epoch: 5670    train loss: 0.0000544    val loss: 0.0003570\n",
      "epoch: 5680    train loss: 0.0000500    val loss: 0.0003494\n",
      "epoch: 5690    train loss: 0.0000503    val loss: 0.0003464\n",
      "epoch: 5700    train loss: 0.0000512    val loss: 0.0003445\n",
      "epoch: 5710    train loss: 0.0000544    val loss: 0.0003457\n",
      "epoch: 5720    train loss: 0.0000507    val loss: 0.0003448\n",
      "epoch: 5730    train loss: 0.0000492    val loss: 0.0003508\n",
      "epoch: 5740    train loss: 0.0000545    val loss: 0.0003585\n",
      "epoch: 5750    train loss: 0.0000519    val loss: 0.0003452\n",
      "epoch: 5760    train loss: 0.0000496    val loss: 0.0003449\n",
      "epoch: 5770    train loss: 0.0000477    val loss: 0.0003450\n",
      "epoch: 5780    train loss: 0.0000475    val loss: 0.0003445\n",
      "epoch: 5790    train loss: 0.0000484    val loss: 0.0003460\n",
      "epoch: 5800    train loss: 0.0000598    val loss: 0.0003613\n",
      "epoch: 5810    train loss: 0.0000561    val loss: 0.0003579\n",
      "epoch: 5820    train loss: 0.0000510    val loss: 0.0003482\n",
      "epoch: 5830    train loss: 0.0000486    val loss: 0.0003392\n",
      "epoch: 5840    train loss: 0.0000464    val loss: 0.0003417\n",
      "epoch: 5850    train loss: 0.0000464    val loss: 0.0003432\n",
      "epoch: 5860    train loss: 0.0000479    val loss: 0.0003479\n",
      "epoch: 5870    train loss: 0.0000624    val loss: 0.0003623\n",
      "epoch: 5880    train loss: 0.0000507    val loss: 0.0003421\n",
      "epoch: 5890    train loss: 0.0000475    val loss: 0.0003440\n",
      "epoch: 5900    train loss: 0.0000462    val loss: 0.0003427\n",
      "epoch: 5910    train loss: 0.0000487    val loss: 0.0003509\n",
      "epoch: 5920    train loss: 0.0000528    val loss: 0.0003504\n",
      "epoch: 5930    train loss: 0.0000462    val loss: 0.0003337\n",
      "epoch: 5940    train loss: 0.0000443    val loss: 0.0003357\n",
      "epoch: 5950    train loss: 0.0000445    val loss: 0.0003389\n",
      "epoch: 5960    train loss: 0.0000473    val loss: 0.0003405\n",
      "epoch: 5970    train loss: 0.0000563    val loss: 0.0003423\n",
      "epoch: 5980    train loss: 0.0000477    val loss: 0.0003369\n",
      "epoch: 5990    train loss: 0.0000447    val loss: 0.0003338\n",
      "epoch: 6000    train loss: 0.0000436    val loss: 0.0003320\n",
      "epoch: 6010    train loss: 0.0000481    val loss: 0.0003293\n",
      "epoch: 6020    train loss: 0.0000569    val loss: 0.0003229\n",
      "epoch: 6030    train loss: 0.0000438    val loss: 0.0003335\n",
      "epoch: 6040    train loss: 0.0000428    val loss: 0.0003351\n",
      "epoch: 6050    train loss: 0.0000425    val loss: 0.0003273\n",
      "epoch: 6060    train loss: 0.0000419    val loss: 0.0003319\n",
      "epoch: 6070    train loss: 0.0000414    val loss: 0.0003278\n",
      "epoch: 6080    train loss: 0.0000411    val loss: 0.0003290\n",
      "epoch: 6090    train loss: 0.0000409    val loss: 0.0003274\n",
      "epoch: 6100    train loss: 0.0000427    val loss: 0.0003256\n",
      "epoch: 6110    train loss: 0.0000560    val loss: 0.0003189\n",
      "epoch: 6120    train loss: 0.0000453    val loss: 0.0003295\n",
      "epoch: 6130    train loss: 0.0000463    val loss: 0.0003398\n",
      "epoch: 6140    train loss: 0.0000427    val loss: 0.0003275\n",
      "epoch: 6150    train loss: 0.0000403    val loss: 0.0003266\n",
      "epoch: 6160    train loss: 0.0000398    val loss: 0.0003256\n",
      "epoch: 6170    train loss: 0.0000401    val loss: 0.0003215\n",
      "epoch: 6180    train loss: 0.0000566    val loss: 0.0003281\n",
      "epoch: 6190    train loss: 0.0000427    val loss: 0.0003369\n",
      "epoch: 6200    train loss: 0.0000420    val loss: 0.0003213\n",
      "epoch: 6210    train loss: 0.0000399    val loss: 0.0003267\n",
      "epoch: 6220    train loss: 0.0000388    val loss: 0.0003208\n",
      "epoch: 6230    train loss: 0.0000387    val loss: 0.0003202\n",
      "epoch: 6240    train loss: 0.0000479    val loss: 0.0003230\n",
      "epoch: 6250    train loss: 0.0000419    val loss: 0.0003263\n",
      "epoch: 6260    train loss: 0.0000393    val loss: 0.0003228\n",
      "epoch: 6270    train loss: 0.0000390    val loss: 0.0003208\n",
      "epoch: 6280    train loss: 0.0000379    val loss: 0.0003189\n",
      "epoch: 6290    train loss: 0.0000374    val loss: 0.0003180\n",
      "epoch: 6300    train loss: 0.0000378    val loss: 0.0003171\n",
      "epoch: 6310    train loss: 0.0000521    val loss: 0.0003244\n",
      "epoch: 6320    train loss: 0.0000410    val loss: 0.0003221\n",
      "epoch: 6330    train loss: 0.0000396    val loss: 0.0003133\n",
      "epoch: 6340    train loss: 0.0000376    val loss: 0.0003149\n",
      "epoch: 6350    train loss: 0.0000365    val loss: 0.0003148\n",
      "epoch: 6360    train loss: 0.0000360    val loss: 0.0003164\n",
      "epoch: 6370    train loss: 0.0000362    val loss: 0.0003176\n",
      "epoch: 6380    train loss: 0.0000627    val loss: 0.0003719\n",
      "epoch: 6390    train loss: 0.0000518    val loss: 0.0003094\n",
      "epoch: 6400    train loss: 0.0000364    val loss: 0.0003101\n",
      "epoch: 6410    train loss: 0.0000371    val loss: 0.0003191\n",
      "epoch: 6420    train loss: 0.0000349    val loss: 0.0003138\n",
      "epoch: 6430    train loss: 0.0000348    val loss: 0.0003097\n",
      "epoch: 6440    train loss: 0.0000345    val loss: 0.0003108\n",
      "epoch: 6450    train loss: 0.0000343    val loss: 0.0003112\n",
      "epoch: 6460    train loss: 0.0000342    val loss: 0.0003093\n",
      "epoch: 6470    train loss: 0.0000473    val loss: 0.0003216\n",
      "epoch: 6480    train loss: 0.0000471    val loss: 0.0003204\n",
      "epoch: 6490    train loss: 0.0000392    val loss: 0.0003160\n",
      "epoch: 6500    train loss: 0.0000344    val loss: 0.0003059\n",
      "epoch: 6510    train loss: 0.0000338    val loss: 0.0003078\n",
      "epoch: 6520    train loss: 0.0000333    val loss: 0.0003099\n",
      "epoch: 6530    train loss: 0.0000329    val loss: 0.0003077\n",
      "epoch: 6540    train loss: 0.0000327    val loss: 0.0003056\n",
      "epoch: 6550    train loss: 0.0000326    val loss: 0.0003059\n",
      "epoch: 6560    train loss: 0.0000335    val loss: 0.0003057\n",
      "epoch: 6570    train loss: 0.0000514    val loss: 0.0003048\n",
      "epoch: 6580    train loss: 0.0000357    val loss: 0.0003036\n",
      "epoch: 6590    train loss: 0.0000335    val loss: 0.0003099\n",
      "epoch: 6600    train loss: 0.0000387    val loss: 0.0003050\n",
      "epoch: 6610    train loss: 0.0000330    val loss: 0.0003008\n",
      "epoch: 6620    train loss: 0.0000322    val loss: 0.0003036\n",
      "epoch: 6630    train loss: 0.0000318    val loss: 0.0003034\n",
      "epoch: 6640    train loss: 0.0000328    val loss: 0.0003096\n",
      "epoch: 6650    train loss: 0.0000416    val loss: 0.0003247\n",
      "epoch: 6660    train loss: 0.0000316    val loss: 0.0003024\n",
      "epoch: 6670    train loss: 0.0000321    val loss: 0.0002964\n",
      "epoch: 6680    train loss: 0.0000317    val loss: 0.0003055\n",
      "epoch: 6690    train loss: 0.0000319    val loss: 0.0003028\n",
      "epoch: 6700    train loss: 0.0000402    val loss: 0.0003117\n",
      "epoch: 6710    train loss: 0.0000324    val loss: 0.0003067\n",
      "epoch: 6720    train loss: 0.0000299    val loss: 0.0002991\n",
      "epoch: 6730    train loss: 0.0000301    val loss: 0.0002975\n",
      "epoch: 6740    train loss: 0.0000301    val loss: 0.0002947\n",
      "epoch: 6750    train loss: 0.0000374    val loss: 0.0002919\n",
      "epoch: 6760    train loss: 0.0000377    val loss: 0.0002874\n",
      "epoch: 6770    train loss: 0.0000298    val loss: 0.0002987\n",
      "epoch: 6780    train loss: 0.0000296    val loss: 0.0003038\n",
      "epoch: 6790    train loss: 0.0000295    val loss: 0.0002891\n",
      "epoch: 6800    train loss: 0.0000289    val loss: 0.0002982\n",
      "epoch: 6810    train loss: 0.0000287    val loss: 0.0002930\n",
      "epoch: 6820    train loss: 0.0000318    val loss: 0.0003001\n",
      "epoch: 6830    train loss: 0.0000393    val loss: 0.0002998\n",
      "epoch: 6840    train loss: 0.0000312    val loss: 0.0002884\n",
      "epoch: 6850    train loss: 0.0000298    val loss: 0.0002963\n",
      "epoch: 6860    train loss: 0.0000319    val loss: 0.0002898\n",
      "epoch: 6870    train loss: 0.0000287    val loss: 0.0002892\n",
      "epoch: 6880    train loss: 0.0000279    val loss: 0.0002878\n",
      "epoch: 6890    train loss: 0.0000274    val loss: 0.0002934\n",
      "epoch: 6900    train loss: 0.0000279    val loss: 0.0002967\n",
      "epoch: 6910    train loss: 0.0000430    val loss: 0.0003319\n",
      "epoch: 6920    train loss: 0.0000318    val loss: 0.0002895\n",
      "epoch: 6930    train loss: 0.0000296    val loss: 0.0002973\n",
      "epoch: 6940    train loss: 0.0000281    val loss: 0.0002802\n",
      "epoch: 6950    train loss: 0.0000271    val loss: 0.0002952\n",
      "epoch: 6960    train loss: 0.0000265    val loss: 0.0002849\n",
      "epoch: 6970    train loss: 0.0000263    val loss: 0.0002898\n",
      "epoch: 6980    train loss: 0.0000268    val loss: 0.0002881\n",
      "epoch: 6990    train loss: 0.0000408    val loss: 0.0003153\n",
      "epoch: 7000    train loss: 0.0000313    val loss: 0.0002852\n",
      "epoch: 7010    train loss: 0.0000303    val loss: 0.0002916\n",
      "epoch: 7020    train loss: 0.0000401    val loss: 0.0003014\n",
      "epoch: 7030    train loss: 0.0000296    val loss: 0.0002895\n",
      "epoch: 7040    train loss: 0.0000266    val loss: 0.0002848\n",
      "epoch: 7050    train loss: 0.0000255    val loss: 0.0002822\n",
      "epoch: 7060    train loss: 0.0000251    val loss: 0.0002871\n",
      "epoch: 7070    train loss: 0.0000251    val loss: 0.0002853\n",
      "epoch: 7080    train loss: 0.0000279    val loss: 0.0002999\n",
      "epoch: 7090    train loss: 0.0000494    val loss: 0.0003256\n",
      "epoch: 7100    train loss: 0.0000313    val loss: 0.0002737\n",
      "epoch: 7110    train loss: 0.0000259    val loss: 0.0002944\n",
      "epoch: 7120    train loss: 0.0000246    val loss: 0.0002789\n",
      "epoch: 7130    train loss: 0.0000242    val loss: 0.0002799\n",
      "epoch: 7140    train loss: 0.0000240    val loss: 0.0002805\n",
      "epoch: 7150    train loss: 0.0000239    val loss: 0.0002810\n",
      "epoch: 7160    train loss: 0.0000239    val loss: 0.0002775\n",
      "epoch: 7170    train loss: 0.0000282    val loss: 0.0002812\n",
      "epoch: 7180    train loss: 0.0000348    val loss: 0.0002781\n",
      "epoch: 7190    train loss: 0.0000255    val loss: 0.0002729\n",
      "epoch: 7200    train loss: 0.0000244    val loss: 0.0002922\n",
      "epoch: 7210    train loss: 0.0000239    val loss: 0.0002713\n",
      "epoch: 7220    train loss: 0.0000234    val loss: 0.0002789\n",
      "epoch: 7230    train loss: 0.0000238    val loss: 0.0002737\n",
      "epoch: 7240    train loss: 0.0000312    val loss: 0.0002743\n",
      "epoch: 7250    train loss: 0.0000237    val loss: 0.0002764\n",
      "epoch: 7260    train loss: 0.0000237    val loss: 0.0002787\n",
      "epoch: 7270    train loss: 0.0000234    val loss: 0.0002784\n",
      "epoch: 7280    train loss: 0.0000240    val loss: 0.0002850\n",
      "epoch: 7290    train loss: 0.0000387    val loss: 0.0003120\n",
      "epoch: 7300    train loss: 0.0000235    val loss: 0.0002744\n",
      "epoch: 7310    train loss: 0.0000223    val loss: 0.0002737\n",
      "epoch: 7320    train loss: 0.0000221    val loss: 0.0002756\n",
      "epoch: 7330    train loss: 0.0000220    val loss: 0.0002708\n",
      "epoch: 7340    train loss: 0.0000218    val loss: 0.0002757\n",
      "epoch: 7350    train loss: 0.0000216    val loss: 0.0002739\n",
      "epoch: 7360    train loss: 0.0000237    val loss: 0.0002802\n",
      "epoch: 7370    train loss: 0.0000444    val loss: 0.0003025\n",
      "epoch: 7380    train loss: 0.0000256    val loss: 0.0002653\n",
      "epoch: 7390    train loss: 0.0000212    val loss: 0.0002742\n",
      "epoch: 7400    train loss: 0.0000216    val loss: 0.0002713\n",
      "epoch: 7410    train loss: 0.0000213    val loss: 0.0002712\n",
      "epoch: 7420    train loss: 0.0000213    val loss: 0.0002672\n",
      "epoch: 7430    train loss: 0.0000372    val loss: 0.0002684\n",
      "epoch: 7440    train loss: 0.0000265    val loss: 0.0002692\n",
      "epoch: 7450    train loss: 0.0000239    val loss: 0.0002663\n",
      "epoch: 7460    train loss: 0.0000214    val loss: 0.0002805\n",
      "epoch: 7470    train loss: 0.0000205    val loss: 0.0002688\n",
      "epoch: 7480    train loss: 0.0000203    val loss: 0.0002647\n",
      "epoch: 7490    train loss: 0.0000202    val loss: 0.0002712\n",
      "epoch: 7500    train loss: 0.0000200    val loss: 0.0002655\n",
      "epoch: 7510    train loss: 0.0000199    val loss: 0.0002679\n",
      "epoch: 7520    train loss: 0.0000198    val loss: 0.0002670\n",
      "epoch: 7530    train loss: 0.0000198    val loss: 0.0002683\n",
      "epoch: 7540    train loss: 0.0000267    val loss: 0.0002817\n",
      "epoch: 7550    train loss: 0.0000277    val loss: 0.0002989\n",
      "epoch: 7560    train loss: 0.0000248    val loss: 0.0002591\n",
      "epoch: 7570    train loss: 0.0000199    val loss: 0.0002581\n",
      "epoch: 7580    train loss: 0.0000199    val loss: 0.0002674\n",
      "epoch: 7590    train loss: 0.0000193    val loss: 0.0002636\n",
      "epoch: 7600    train loss: 0.0000191    val loss: 0.0002657\n",
      "epoch: 7610    train loss: 0.0000190    val loss: 0.0002645\n",
      "epoch: 7620    train loss: 0.0000189    val loss: 0.0002626\n",
      "epoch: 7630    train loss: 0.0000200    val loss: 0.0002618\n",
      "epoch: 7640    train loss: 0.0000533    val loss: 0.0002702\n",
      "epoch: 7650    train loss: 0.0000308    val loss: 0.0002979\n",
      "epoch: 7660    train loss: 0.0000208    val loss: 0.0002364\n",
      "epoch: 7670    train loss: 0.0000197    val loss: 0.0002731\n",
      "epoch: 7680    train loss: 0.0000186    val loss: 0.0002665\n",
      "epoch: 7690    train loss: 0.0000184    val loss: 0.0002576\n",
      "epoch: 7700    train loss: 0.0000183    val loss: 0.0002641\n",
      "epoch: 7710    train loss: 0.0000189    val loss: 0.0002597\n",
      "epoch: 7720    train loss: 0.0000303    val loss: 0.0002715\n",
      "epoch: 7730    train loss: 0.0000202    val loss: 0.0002610\n",
      "epoch: 7740    train loss: 0.0000185    val loss: 0.0002655\n",
      "epoch: 7750    train loss: 0.0000179    val loss: 0.0002610\n",
      "epoch: 7760    train loss: 0.0000178    val loss: 0.0002594\n",
      "epoch: 7770    train loss: 0.0000178    val loss: 0.0002616\n",
      "epoch: 7780    train loss: 0.0000176    val loss: 0.0002545\n",
      "epoch: 7790    train loss: 0.0000213    val loss: 0.0002409\n",
      "epoch: 7800    train loss: 0.0000310    val loss: 0.0002614\n",
      "epoch: 7810    train loss: 0.0000189    val loss: 0.0002376\n",
      "epoch: 7820    train loss: 0.0000191    val loss: 0.0002864\n",
      "epoch: 7830    train loss: 0.0000180    val loss: 0.0002451\n",
      "epoch: 7840    train loss: 0.0000173    val loss: 0.0002642\n",
      "epoch: 7850    train loss: 0.0000179    val loss: 0.0002635\n",
      "epoch: 7860    train loss: 0.0000263    val loss: 0.0002762\n",
      "epoch: 7870    train loss: 0.0000195    val loss: 0.0002561\n",
      "epoch: 7880    train loss: 0.0000215    val loss: 0.0002707\n",
      "epoch: 7890    train loss: 0.0000214    val loss: 0.0002559\n",
      "epoch: 7900    train loss: 0.0000180    val loss: 0.0002606\n",
      "epoch: 7910    train loss: 0.0000168    val loss: 0.0002518\n",
      "epoch: 7920    train loss: 0.0000168    val loss: 0.0002597\n",
      "epoch: 7930    train loss: 0.0000169    val loss: 0.0002619\n",
      "epoch: 7940    train loss: 0.0000185    val loss: 0.0002716\n",
      "epoch: 7950    train loss: 0.0000312    val loss: 0.0003054\n",
      "epoch: 7960    train loss: 0.0000210    val loss: 0.0002421\n",
      "epoch: 7970    train loss: 0.0000186    val loss: 0.0002694\n",
      "epoch: 7980    train loss: 0.0000172    val loss: 0.0002477\n",
      "epoch: 7990    train loss: 0.0000166    val loss: 0.0002602\n",
      "epoch: 8000    train loss: 0.0000162    val loss: 0.0002504\n",
      "epoch: 8010    train loss: 0.0000168    val loss: 0.0002573\n",
      "epoch: 8020    train loss: 0.0000248    val loss: 0.0002746\n",
      "epoch: 8030    train loss: 0.0000225    val loss: 0.0002694\n",
      "epoch: 8040    train loss: 0.0000183    val loss: 0.0002454\n",
      "epoch: 8050    train loss: 0.0000167    val loss: 0.0002459\n",
      "epoch: 8060    train loss: 0.0000159    val loss: 0.0002508\n",
      "epoch: 8070    train loss: 0.0000157    val loss: 0.0002515\n",
      "epoch: 8080    train loss: 0.0000158    val loss: 0.0002500\n",
      "epoch: 8090    train loss: 0.0000211    val loss: 0.0002448\n",
      "epoch: 8100    train loss: 0.0000264    val loss: 0.0002383\n",
      "epoch: 8110    train loss: 0.0000182    val loss: 0.0002632\n",
      "epoch: 8120    train loss: 0.0000157    val loss: 0.0002490\n",
      "epoch: 8130    train loss: 0.0000153    val loss: 0.0002496\n",
      "epoch: 8140    train loss: 0.0000151    val loss: 0.0002512\n",
      "epoch: 8150    train loss: 0.0000152    val loss: 0.0002502\n",
      "epoch: 8160    train loss: 0.0000218    val loss: 0.0002336\n",
      "epoch: 8170    train loss: 0.0000170    val loss: 0.0002798\n",
      "epoch: 8180    train loss: 0.0000199    val loss: 0.0002399\n",
      "epoch: 8190    train loss: 0.0000156    val loss: 0.0002457\n",
      "epoch: 8200    train loss: 0.0000152    val loss: 0.0002515\n",
      "epoch: 8210    train loss: 0.0000148    val loss: 0.0002476\n",
      "epoch: 8220    train loss: 0.0000151    val loss: 0.0002440\n",
      "epoch: 8230    train loss: 0.0000257    val loss: 0.0002233\n",
      "epoch: 8240    train loss: 0.0000190    val loss: 0.0002423\n",
      "epoch: 8250    train loss: 0.0000166    val loss: 0.0002484\n",
      "epoch: 8260    train loss: 0.0000158    val loss: 0.0002533\n",
      "epoch: 8270    train loss: 0.0000150    val loss: 0.0002489\n",
      "epoch: 8280    train loss: 0.0000144    val loss: 0.0002488\n",
      "epoch: 8290    train loss: 0.0000143    val loss: 0.0002477\n",
      "epoch: 8300    train loss: 0.0000144    val loss: 0.0002479\n",
      "epoch: 8310    train loss: 0.0000184    val loss: 0.0002412\n",
      "epoch: 8320    train loss: 0.0000222    val loss: 0.0002451\n",
      "epoch: 8330    train loss: 0.0000171    val loss: 0.0002551\n",
      "epoch: 8340    train loss: 0.0000154    val loss: 0.0002305\n",
      "epoch: 8350    train loss: 0.0000147    val loss: 0.0002432\n",
      "epoch: 8360    train loss: 0.0000177    val loss: 0.0002195\n",
      "epoch: 8370    train loss: 0.0000241    val loss: 0.0002223\n",
      "epoch: 8380    train loss: 0.0000156    val loss: 0.0002664\n",
      "epoch: 8390    train loss: 0.0000142    val loss: 0.0002389\n",
      "epoch: 8400    train loss: 0.0000143    val loss: 0.0002487\n",
      "epoch: 8410    train loss: 0.0000136    val loss: 0.0002472\n",
      "epoch: 8420    train loss: 0.0000137    val loss: 0.0002464\n",
      "epoch: 8430    train loss: 0.0000159    val loss: 0.0002587\n",
      "epoch: 8440    train loss: 0.0000379    val loss: 0.0002975\n",
      "epoch: 8450    train loss: 0.0000209    val loss: 0.0002372\n",
      "epoch: 8460    train loss: 0.0000144    val loss: 0.0002521\n",
      "epoch: 8470    train loss: 0.0000137    val loss: 0.0002455\n",
      "epoch: 8480    train loss: 0.0000136    val loss: 0.0002356\n",
      "epoch: 8490    train loss: 0.0000135    val loss: 0.0002493\n",
      "epoch: 8500    train loss: 0.0000150    val loss: 0.0002436\n",
      "epoch: 8510    train loss: 0.0000346    val loss: 0.0002477\n",
      "epoch: 8520    train loss: 0.0000190    val loss: 0.0002416\n",
      "epoch: 8530    train loss: 0.0000159    val loss: 0.0002567\n",
      "epoch: 8540    train loss: 0.0000140    val loss: 0.0002351\n",
      "epoch: 8550    train loss: 0.0000133    val loss: 0.0002453\n",
      "epoch: 8560    train loss: 0.0000130    val loss: 0.0002435\n",
      "epoch: 8570    train loss: 0.0000129    val loss: 0.0002489\n",
      "epoch: 8580    train loss: 0.0000133    val loss: 0.0002609\n",
      "epoch: 8590    train loss: 0.0000261    val loss: 0.0003396\n",
      "epoch: 8600    train loss: 0.0000148    val loss: 0.0002170\n",
      "epoch: 8610    train loss: 0.0000148    val loss: 0.0002737\n",
      "epoch: 8620    train loss: 0.0000140    val loss: 0.0002248\n",
      "epoch: 8630    train loss: 0.0000132    val loss: 0.0002551\n",
      "epoch: 8640    train loss: 0.0000127    val loss: 0.0002366\n",
      "epoch: 8650    train loss: 0.0000127    val loss: 0.0002479\n",
      "epoch: 8660    train loss: 0.0000161    val loss: 0.0002544\n",
      "epoch: 8670    train loss: 0.0000210    val loss: 0.0002571\n",
      "epoch: 8680    train loss: 0.0000156    val loss: 0.0002410\n",
      "epoch: 8690    train loss: 0.0000127    val loss: 0.0002306\n",
      "epoch: 8700    train loss: 0.0000126    val loss: 0.0002505\n",
      "epoch: 8710    train loss: 0.0000128    val loss: 0.0002404\n",
      "epoch: 8720    train loss: 0.0000151    val loss: 0.0002486\n",
      "epoch: 8730    train loss: 0.0000242    val loss: 0.0002387\n",
      "epoch: 8740    train loss: 0.0000151    val loss: 0.0002236\n",
      "epoch: 8750    train loss: 0.0000130    val loss: 0.0002572\n",
      "epoch: 8760    train loss: 0.0000124    val loss: 0.0002447\n",
      "epoch: 8770    train loss: 0.0000121    val loss: 0.0002403\n",
      "epoch: 8780    train loss: 0.0000124    val loss: 0.0002443\n",
      "epoch: 8790    train loss: 0.0000164    val loss: 0.0002610\n",
      "epoch: 8800    train loss: 0.0000163    val loss: 0.0002484\n",
      "epoch: 8810    train loss: 0.0000184    val loss: 0.0002776\n",
      "epoch: 8820    train loss: 0.0000149    val loss: 0.0002598\n",
      "epoch: 8830    train loss: 0.0000141    val loss: 0.0002332\n",
      "epoch: 8840    train loss: 0.0000125    val loss: 0.0002513\n",
      "epoch: 8850    train loss: 0.0000131    val loss: 0.0002592\n",
      "epoch: 8860    train loss: 0.0000146    val loss: 0.0002601\n",
      "epoch: 8870    train loss: 0.0000155    val loss: 0.0002635\n",
      "epoch: 8880    train loss: 0.0000131    val loss: 0.0002544\n",
      "epoch: 8890    train loss: 0.0000127    val loss: 0.0002456\n",
      "epoch: 8900    train loss: 0.0000135    val loss: 0.0002563\n",
      "epoch: 8910    train loss: 0.0000172    val loss: 0.0002895\n",
      "epoch: 8920    train loss: 0.0000268    val loss: 0.0002942\n",
      "epoch: 8930    train loss: 0.0000174    val loss: 0.0002023\n",
      "epoch: 8940    train loss: 0.0000132    val loss: 0.0002630\n",
      "epoch: 8950    train loss: 0.0000121    val loss: 0.0002265\n",
      "epoch: 8960    train loss: 0.0000115    val loss: 0.0002452\n",
      "epoch: 8970    train loss: 0.0000114    val loss: 0.0002347\n",
      "epoch: 8980    train loss: 0.0000116    val loss: 0.0002336\n",
      "epoch: 8990    train loss: 0.0000159    val loss: 0.0002377\n",
      "epoch: 9000    train loss: 0.0000217    val loss: 0.0002563\n",
      "epoch: 9010    train loss: 0.0000138    val loss: 0.0002520\n",
      "epoch: 9020    train loss: 0.0000121    val loss: 0.0002243\n",
      "epoch: 9030    train loss: 0.0000113    val loss: 0.0002383\n",
      "epoch: 9040    train loss: 0.0000111    val loss: 0.0002401\n",
      "epoch: 9050    train loss: 0.0000111    val loss: 0.0002420\n",
      "epoch: 9060    train loss: 0.0000117    val loss: 0.0002559\n",
      "epoch: 9070    train loss: 0.0000258    val loss: 0.0003348\n",
      "epoch: 9080    train loss: 0.0000141    val loss: 0.0002220\n",
      "epoch: 9090    train loss: 0.0000136    val loss: 0.0002700\n",
      "epoch: 9100    train loss: 0.0000133    val loss: 0.0002194\n",
      "epoch: 9110    train loss: 0.0000123    val loss: 0.0002482\n",
      "epoch: 9120    train loss: 0.0000121    val loss: 0.0002366\n",
      "epoch: 9130    train loss: 0.0000146    val loss: 0.0002534\n",
      "epoch: 9140    train loss: 0.0000153    val loss: 0.0002507\n",
      "epoch: 9150    train loss: 0.0000109    val loss: 0.0002332\n",
      "epoch: 9160    train loss: 0.0000116    val loss: 0.0002259\n",
      "epoch: 9170    train loss: 0.0000112    val loss: 0.0002225\n",
      "epoch: 9180    train loss: 0.0000239    val loss: 0.0002034\n",
      "epoch: 9190    train loss: 0.0000144    val loss: 0.0002918\n",
      "epoch: 9200    train loss: 0.0000159    val loss: 0.0002210\n",
      "epoch: 9210    train loss: 0.0000107    val loss: 0.0002283\n",
      "epoch: 9220    train loss: 0.0000112    val loss: 0.0002380\n",
      "epoch: 9230    train loss: 0.0000111    val loss: 0.0002206\n",
      "epoch: 9240    train loss: 0.0000150    val loss: 0.0002040\n",
      "epoch: 9250    train loss: 0.0000148    val loss: 0.0002132\n",
      "epoch: 9260    train loss: 0.0000121    val loss: 0.0002507\n",
      "epoch: 9270    train loss: 0.0000108    val loss: 0.0002325\n",
      "epoch: 9280    train loss: 0.0000107    val loss: 0.0002368\n",
      "epoch: 9290    train loss: 0.0000109    val loss: 0.0002423\n",
      "epoch: 9300    train loss: 0.0000148    val loss: 0.0002616\n",
      "epoch: 9310    train loss: 0.0000146    val loss: 0.0002672\n",
      "epoch: 9320    train loss: 0.0000166    val loss: 0.0002507\n",
      "epoch: 9330    train loss: 0.0000123    val loss: 0.0002450\n",
      "epoch: 9340    train loss: 0.0000111    val loss: 0.0002304\n",
      "epoch: 9350    train loss: 0.0000107    val loss: 0.0002292\n",
      "epoch: 9360    train loss: 0.0000109    val loss: 0.0002146\n",
      "epoch: 9370    train loss: 0.0000177    val loss: 0.0001873\n",
      "epoch: 9380    train loss: 0.0000123    val loss: 0.0002252\n",
      "epoch: 9390    train loss: 0.0000126    val loss: 0.0002548\n",
      "epoch: 9400    train loss: 0.0000144    val loss: 0.0002422\n",
      "epoch: 9410    train loss: 0.0000113    val loss: 0.0002357\n",
      "epoch: 9420    train loss: 0.0000104    val loss: 0.0002215\n",
      "epoch: 9430    train loss: 0.0000103    val loss: 0.0002332\n",
      "epoch: 9440    train loss: 0.0000123    val loss: 0.0002459\n",
      "epoch: 9450    train loss: 0.0000173    val loss: 0.0002630\n",
      "epoch: 9460    train loss: 0.0000117    val loss: 0.0002479\n",
      "epoch: 9470    train loss: 0.0000214    val loss: 0.0002757\n",
      "epoch: 9480    train loss: 0.0000151    val loss: 0.0002034\n",
      "epoch: 9490    train loss: 0.0000139    val loss: 0.0002155\n",
      "epoch: 9500    train loss: 0.0000106    val loss: 0.0002310\n",
      "epoch: 9510    train loss: 0.0000099    val loss: 0.0002396\n",
      "epoch: 9520    train loss: 0.0000103    val loss: 0.0002407\n",
      "epoch: 9530    train loss: 0.0000164    val loss: 0.0002821\n",
      "epoch: 9540    train loss: 0.0000150    val loss: 0.0002532\n",
      "epoch: 9550    train loss: 0.0000120    val loss: 0.0002175\n",
      "epoch: 9560    train loss: 0.0000107    val loss: 0.0002470\n",
      "epoch: 9570    train loss: 0.0000100    val loss: 0.0002237\n",
      "epoch: 9580    train loss: 0.0000101    val loss: 0.0002488\n",
      "epoch: 9590    train loss: 0.0000191    val loss: 0.0002857\n",
      "epoch: 9600    train loss: 0.0000132    val loss: 0.0002368\n",
      "epoch: 9610    train loss: 0.0000121    val loss: 0.0002357\n",
      "epoch: 9620    train loss: 0.0000105    val loss: 0.0002335\n",
      "epoch: 9630    train loss: 0.0000098    val loss: 0.0002285\n",
      "epoch: 9640    train loss: 0.0000096    val loss: 0.0002309\n",
      "epoch: 9650    train loss: 0.0000113    val loss: 0.0002251\n",
      "epoch: 9660    train loss: 0.0000209    val loss: 0.0002210\n",
      "epoch: 9670    train loss: 0.0000125    val loss: 0.0002495\n",
      "epoch: 9680    train loss: 0.0000106    val loss: 0.0002369\n",
      "epoch: 9690    train loss: 0.0000154    val loss: 0.0002819\n",
      "epoch: 9700    train loss: 0.0000157    val loss: 0.0002537\n",
      "epoch: 9710    train loss: 0.0000120    val loss: 0.0002114\n",
      "epoch: 9720    train loss: 0.0000103    val loss: 0.0002463\n",
      "epoch: 9730    train loss: 0.0000097    val loss: 0.0002237\n",
      "epoch: 9740    train loss: 0.0000096    val loss: 0.0002456\n",
      "epoch: 9750    train loss: 0.0000168    val loss: 0.0002761\n",
      "epoch: 9760    train loss: 0.0000113    val loss: 0.0002379\n",
      "epoch: 9770    train loss: 0.0000101    val loss: 0.0002299\n",
      "epoch: 9780    train loss: 0.0000096    val loss: 0.0002288\n",
      "epoch: 9790    train loss: 0.0000093    val loss: 0.0002247\n",
      "epoch: 9800    train loss: 0.0000092    val loss: 0.0002225\n",
      "epoch: 9810    train loss: 0.0000135    val loss: 0.0001937\n",
      "epoch: 9820    train loss: 0.0000218    val loss: 0.0001874\n",
      "epoch: 9830    train loss: 0.0000107    val loss: 0.0002418\n",
      "epoch: 9840    train loss: 0.0000092    val loss: 0.0002371\n",
      "epoch: 9850    train loss: 0.0000092    val loss: 0.0002185\n",
      "epoch: 9860    train loss: 0.0000089    val loss: 0.0002339\n",
      "epoch: 9870    train loss: 0.0000088    val loss: 0.0002274\n",
      "epoch: 9880    train loss: 0.0000089    val loss: 0.0002348\n",
      "epoch: 9890    train loss: 0.0000103    val loss: 0.0002480\n",
      "epoch: 9900    train loss: 0.0000294    val loss: 0.0003056\n",
      "epoch: 9910    train loss: 0.0000140    val loss: 0.0001968\n",
      "epoch: 9920    train loss: 0.0000121    val loss: 0.0002340\n",
      "epoch: 9930    train loss: 0.0000203    val loss: 0.0001956\n",
      "epoch: 9940    train loss: 0.0000097    val loss: 0.0002420\n",
      "epoch: 9950    train loss: 0.0000088    val loss: 0.0002323\n",
      "epoch: 9960    train loss: 0.0000088    val loss: 0.0002199\n",
      "epoch: 9970    train loss: 0.0000087    val loss: 0.0002328\n",
      "epoch: 9980    train loss: 0.0000086    val loss: 0.0002300\n",
      "epoch: 9990    train loss: 0.0000086    val loss: 0.0002341\n",
      "epoch: 10000    train loss: 0.0000096    val loss: 0.0002509\n",
      "CPU times: total: 1min 57s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_size = n_bus*2\n",
    "hidden_size1 = 30\n",
    "hidden_size2 = 30\n",
    "output_size = n_bus*2\n",
    "lr = 0.001\n",
    "\n",
    "model = My_NN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "count=0\n",
    "patience=10000\n",
    "lossMin = 1e10\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_train_prediction = model(x_norm_train)\n",
    "    train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss_list.append(train_loss.detach())\n",
    "\n",
    "    model.eval()\n",
    "    y_val_prediction = model(x_norm_val)\n",
    "    val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))\n",
    "    val_loss_list.append(val_loss.detach())\n",
    "\n",
    "    #early stopping\n",
    "    if (val_loss < lossMin):\n",
    "        lossMin = val_loss\n",
    "        count = 0\n",
    "        best_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        model.save_weights(model, \"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "    else:\n",
    "        count+=1\n",
    "        if(count>patience):\n",
    "            print(\"early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "            print(\"best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(best_epoch, best_train_loss, best_val_loss))\n",
    "            break\n",
    "    \n",
    "    #if (train_loss <= 0):\n",
    "    #    print(\"min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}\".format(epoch, train_loss, val_loss))\n",
    "    #    break\n",
    "\n",
    "    if (epoch % 10) == 0:\n",
    "        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last epoch: 10000, train loss: 0.0000096, val loss: 0.0002509\n",
      "best epoch: 9073, train loss: 0.0000324, val loss: 0.0001645\n"
     ]
    }
   ],
   "source": [
    "plt.title('NN on power flow dataset')\n",
    "plt.plot(train_loss_list, label=\"train loss\")\n",
    "plt.plot(val_loss_list, label=\"val loss\")\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"# Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))\n",
    "print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lin1.weight\n",
      "torch.Size([30, 28])\n",
      "lin1.bias\n",
      "torch.Size([30])\n",
      "lin2.weight\n",
      "torch.Size([30, 30])\n",
      "lin2.bias\n",
      "torch.Size([30])\n",
      "lin3.weight\n",
      "torch.Size([28, 30])\n",
      "lin3.bias\n",
      "torch.Size([28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2668"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  print(name)\n",
    "  print(param.size())\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "Train output prediction: \n",
      "[  10.32369 -154.21858    9.88141 -156.11742    9.75225 -156.71446\n",
      "    9.72426 -156.83875    9.65715 -157.16441    9.60591 -157.39868\n",
      "    9.61655 -157.37032    9.62467 -157.31735    9.58032 -157.5144\n",
      "    9.55804 -157.63274    9.77152 -156.61739    9.68221 -157.04497\n",
      "    9.65724 -157.16107    9.66321 -157.13039]\n",
      "Train loss (MSE): 0.0000100\n",
      "===========================================================================\n",
      "Train output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "Train output prediction: \n",
      "[  10.30192 -154.78087    9.84362 -156.73466    9.6879  -157.44803\n",
      "    9.68471 -157.47455    9.57688 -157.97011    9.56568 -158.04819\n",
      "    9.54452 -158.12344    9.53524 -158.16727    9.51629 -158.29138\n",
      "    9.53275 -158.20586    9.7272  -157.26523    9.63061 -157.71957\n",
      "    9.60009 -157.86021    9.59631 -157.88187]\n",
      "Train loss (MSE): 0.0002509\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "y_train_prediction = model(x_norm_train)\n",
    "train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_train.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"===========================================================================\")\n",
    "\n",
    "y_val_prediction = model(x_norm_val)\n",
    "val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_val.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train output ground-truth: \n",
      "[  10.324 -154.216    9.882 -156.117    9.753 -156.712    9.724 -156.839\n",
      "    9.659 -157.161    9.607 -157.399    9.617 -157.37     9.627 -157.311\n",
      "    9.581 -157.512    9.56  -157.633    9.773 -156.613    9.682 -157.039\n",
      "    9.658 -157.152    9.664 -157.123]\n",
      "Train output prediction: \n",
      "[  10.32379 -154.21645    9.88181 -156.11435    9.75239 -156.71124\n",
      "    9.72461 -156.83609    9.65717 -157.16106    9.60601 -157.39635\n",
      "    9.61664 -157.36684    9.62476 -157.31421    9.58045 -157.51187\n",
      "    9.55803 -157.63058    9.77198 -156.61409    9.68322 -157.04146\n",
      "    9.65793 -157.15793    9.66377 -157.12694]\n",
      "Train loss (MSE): 0.0000310\n",
      "===========================================================================\n",
      "Train output ground-truth: \n",
      "[  10.302 -154.788    9.844 -156.745    9.686 -157.465    9.685 -157.484\n",
      "    9.574 -157.989    9.568 -158.053    9.543 -158.141    9.532 -158.187\n",
      "    9.518 -158.294    9.535 -158.209    9.727 -157.275    9.629 -157.731\n",
      "    9.599 -157.869    9.594 -157.896]\n",
      "Train output prediction: \n",
      "[  10.30151 -154.78867    9.84268 -156.74466    9.68707 -157.45886\n",
      "    9.68379 -157.48462    9.57578 -157.98135    9.56457 -158.05894\n",
      "    9.54327 -158.13559    9.53409 -158.17899    9.51513 -158.3026\n",
      "    9.53148 -158.21695    9.72602 -157.27606    9.62974 -157.73055\n",
      "    9.59898 -157.87157    9.59511 -157.89265]\n",
      "Train loss (MSE): 0.0001645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_3160\\4246943532.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "y_train_prediction = best_model(x_norm_train)\n",
    "train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_train.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(train_loss))\n",
    "\n",
    "print(\"===========================================================================\")\n",
    "\n",
    "y_val_prediction = best_model(x_norm_val)\n",
    "val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))\n",
    "print(\"Train output ground-truth: \\n\" + str(y_raw_val.detach().numpy()[0]))\n",
    "print(\"Train output prediction: \\n\" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))\n",
    "print('Train loss (MSE): {:.7f}'.format(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (MSE): 0.0000310\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (MSE): 0.0001645\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (MSE): 0.0000624\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (MSE): 0.0001228\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (MSE): 0.0002107\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (MSE): 0.0000645\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (MSE): 0.0001044\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (MSE): 0.0002375\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (MSE): 0.0001166\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (MSE): 0.0001733\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (MSE): 0.0001061\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (MSE): 0.0003793\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (MSE): 0.0000728\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (MSE): 0.0000769\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (MSE): 0.0001008\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (MSE): 0.0000672\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (MSE): 0.0000689\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (MSE): 0.0001171\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (MSE): 0.0001006\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (MSE): 0.0002888\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (MSE): 0.0001931\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (MSE): 0.0001410\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (MSE): 0.0000894\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (MSE): 0.0000901\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (MSE): 0.0001103\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (MSE): 0.0001004\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (MSE): 0.0002107\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (MSE): 0.0002832\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (MSE): 0.0000892\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (MSE): 0.0001398\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (MSE): 0.0001047\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (MSE): 0.0000622\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (MSE): 0.0001068\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (MSE): 0.0000739\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (MSE): 0.0000831\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (MSE): 0.0001090\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (MSE): 0.0000727\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (MSE): 0.0000961\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (MSE): 0.0001558\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (MSE): 0.0002222\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (MSE): 0.0001118\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (MSE): 0.0002434\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (MSE): 0.0000696\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (MSE): 0.0001051\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (MSE): 0.0000919\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (MSE): 0.0000822\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (MSE): 0.0001253\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (MSE): 0.0000965\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (MSE): 0.0001247\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (MSE): 0.0001434\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (MSE): 0.0001054\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (MSE): 0.0001605\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (MSE): 0.0000799\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (MSE): 0.0000904\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (MSE): 0.0002288\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (MSE): 0.0002188\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (MSE): 0.0001094\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (MSE): 0.0000817\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (MSE): 0.0001263\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (MSE): 0.0004345\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (MSE): 0.0001199\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (MSE): 0.0001205\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (MSE): 0.0000611\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (MSE): 0.0001287\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (MSE): 0.0001256\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (MSE): 0.0001766\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (MSE): 0.0000751\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (MSE): 0.0001275\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (MSE): 0.0000934\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (MSE): 0.0001293\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (MSE): 0.0001158\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (MSE): 0.0001221\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (MSE): 0.0003840\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (MSE): 0.0001550\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (MSE): 0.0002065\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (MSE): 0.0001544\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (MSE): 0.0000838\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (MSE): 0.0002631\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (MSE): 0.0001265\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (MSE): 0.0000752\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (MSE): 0.0001448\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (MSE): 0.0002321\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (MSE): 0.0001016\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (MSE): 0.0000807\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (MSE): 0.0001043\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (MSE): 0.0000906\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (MSE): 0.0001101\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (MSE): 0.0001024\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (MSE): 0.0002896\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (MSE): 0.0001287\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (MSE): 0.0004209\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (MSE): 0.0001404\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (MSE): 0.0001560\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (MSE): 0.0000766\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (MSE): 0.0001400\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (MSE): 0.0000878\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (MSE): 0.0002125\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (MSE): 0.0000739\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (MSE): 0.0001413\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (MSE): 0.0001033\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (MSE): 0.0002265\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (MSE): 0.0001411\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "CPU times: total: 2min 16s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    y_test_prediction = best_model(x_norm_test)\n",
    "    test_loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_test, y_val_mean, y_val_std))\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))\n",
    "    elif i == 1:\n",
    "        print('Val loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))\n",
    "    else:\n",
    "        print('Test loss (MSE): {:.7f}'.format(test_loss))\n",
    "        test_loss_list.append(test_loss.detach().numpy())\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [MSE] NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset 1\n",
      "Train loss (NRMSE): 0.0216876\n",
      "===========================\n",
      "dataset 2\n",
      "Val loss (NRMSE): 0.0239578\n",
      "===========================\n",
      "dataset 3\n",
      "Test loss (NRMSE): 0.0235791\n",
      "===========================\n",
      "dataset 4\n",
      "Test loss (NRMSE): 0.0270352\n",
      "===========================\n",
      "dataset 5\n",
      "Test loss (NRMSE): 0.0288369\n",
      "===========================\n",
      "dataset 6\n",
      "Test loss (NRMSE): 0.0229688\n",
      "===========================\n",
      "dataset 7\n",
      "Test loss (NRMSE): 0.0500858\n",
      "===========================\n",
      "dataset 8\n",
      "Test loss (NRMSE): 0.0683891\n",
      "===========================\n",
      "dataset 9\n",
      "Test loss (NRMSE): 0.0281425\n",
      "===========================\n",
      "dataset 10\n",
      "Test loss (NRMSE): 0.0561699\n",
      "===========================\n",
      "dataset 11\n",
      "Test loss (NRMSE): 0.0431392\n",
      "===========================\n",
      "dataset 12\n",
      "Test loss (NRMSE): 0.0362823\n",
      "===========================\n",
      "dataset 13\n",
      "Test loss (NRMSE): 0.0237799\n",
      "===========================\n",
      "dataset 14\n",
      "Test loss (NRMSE): 0.0254754\n",
      "===========================\n",
      "dataset 15\n",
      "Test loss (NRMSE): 0.0264731\n",
      "===========================\n",
      "dataset 16\n",
      "Test loss (NRMSE): 0.0306039\n",
      "===========================\n",
      "dataset 17\n",
      "Test loss (NRMSE): 0.0310007\n",
      "===========================\n",
      "dataset 18\n",
      "Test loss (NRMSE): 0.0379744\n",
      "===========================\n",
      "dataset 19\n",
      "Test loss (NRMSE): 0.0282973\n",
      "===========================\n",
      "dataset 20\n",
      "Test loss (NRMSE): 0.0293100\n",
      "===========================\n",
      "dataset 21\n",
      "Test loss (NRMSE): 0.0386898\n",
      "===========================\n",
      "dataset 22\n",
      "Test loss (NRMSE): 0.0341554\n",
      "===========================\n",
      "dataset 23\n",
      "Test loss (NRMSE): 0.0330937\n",
      "===========================\n",
      "dataset 24\n",
      "Test loss (NRMSE): 0.0572399\n",
      "===========================\n",
      "dataset 25\n",
      "Test loss (NRMSE): 0.0405866\n",
      "===========================\n",
      "dataset 26\n",
      "Test loss (NRMSE): 0.0274163\n",
      "===========================\n",
      "dataset 27\n",
      "Test loss (NRMSE): 0.0382566\n",
      "===========================\n",
      "dataset 28\n",
      "Test loss (NRMSE): 0.0287347\n",
      "===========================\n",
      "dataset 29\n",
      "Test loss (NRMSE): 0.0252108\n",
      "===========================\n",
      "dataset 30\n",
      "Test loss (NRMSE): 0.0542242\n",
      "===========================\n",
      "dataset 31\n",
      "Test loss (NRMSE): 0.0471765\n",
      "===========================\n",
      "dataset 32\n",
      "Test loss (NRMSE): 0.0226422\n",
      "===========================\n",
      "dataset 33\n",
      "Test loss (NRMSE): 0.0379924\n",
      "===========================\n",
      "dataset 34\n",
      "Test loss (NRMSE): 0.0265112\n",
      "===========================\n",
      "dataset 35\n",
      "Test loss (NRMSE): 0.0608603\n",
      "===========================\n",
      "dataset 36\n",
      "Test loss (NRMSE): 0.0317969\n",
      "===========================\n",
      "dataset 37\n",
      "Test loss (NRMSE): 0.0233567\n",
      "===========================\n",
      "dataset 38\n",
      "Test loss (NRMSE): 0.0443527\n",
      "===========================\n",
      "dataset 39\n",
      "Test loss (NRMSE): 0.0395204\n",
      "===========================\n",
      "dataset 40\n",
      "Test loss (NRMSE): 0.0514976\n",
      "===========================\n",
      "dataset 41\n",
      "Test loss (NRMSE): 0.0281019\n",
      "===========================\n",
      "dataset 42\n",
      "Test loss (NRMSE): 0.0333704\n",
      "===========================\n",
      "dataset 43\n",
      "Test loss (NRMSE): 0.0285481\n",
      "===========================\n",
      "dataset 44\n",
      "Test loss (NRMSE): 0.0301825\n",
      "===========================\n",
      "dataset 45\n",
      "Test loss (NRMSE): 0.0278198\n",
      "===========================\n",
      "dataset 46\n",
      "Test loss (NRMSE): 0.0291739\n",
      "===========================\n",
      "dataset 47\n",
      "Test loss (NRMSE): 0.0288223\n",
      "===========================\n",
      "dataset 48\n",
      "Test loss (NRMSE): 0.0319345\n",
      "===========================\n",
      "dataset 49\n",
      "Test loss (NRMSE): 0.0418832\n",
      "===========================\n",
      "dataset 50\n",
      "Test loss (NRMSE): 0.0303069\n",
      "===========================\n",
      "dataset 51\n",
      "Test loss (NRMSE): 0.0288985\n",
      "===========================\n",
      "dataset 52\n",
      "Test loss (NRMSE): 0.0322145\n",
      "===========================\n",
      "dataset 53\n",
      "Test loss (NRMSE): 0.0354349\n",
      "===========================\n",
      "dataset 54\n",
      "Test loss (NRMSE): 0.0269761\n",
      "===========================\n",
      "dataset 55\n",
      "Test loss (NRMSE): 0.0282592\n",
      "===========================\n",
      "dataset 56\n",
      "Test loss (NRMSE): 0.0373813\n",
      "===========================\n",
      "dataset 57\n",
      "Test loss (NRMSE): 0.0291036\n",
      "===========================\n",
      "dataset 58\n",
      "Test loss (NRMSE): 0.0344681\n",
      "===========================\n",
      "dataset 59\n",
      "Test loss (NRMSE): 0.0292318\n",
      "===========================\n",
      "dataset 60\n",
      "Test loss (NRMSE): 0.0274021\n",
      "===========================\n",
      "dataset 61\n",
      "Test loss (NRMSE): 0.0307225\n",
      "===========================\n",
      "dataset 62\n",
      "Test loss (NRMSE): 0.0310677\n",
      "===========================\n",
      "dataset 63\n",
      "Test loss (NRMSE): 0.0405330\n",
      "===========================\n",
      "dataset 64\n",
      "Test loss (NRMSE): 0.0281382\n",
      "===========================\n",
      "dataset 65\n",
      "Test loss (NRMSE): 0.0255491\n",
      "===========================\n",
      "dataset 66\n",
      "Test loss (NRMSE): 0.0291498\n",
      "===========================\n",
      "dataset 67\n",
      "Test loss (NRMSE): 0.0271800\n",
      "===========================\n",
      "dataset 68\n",
      "Test loss (NRMSE): 0.0297031\n",
      "===========================\n",
      "dataset 69\n",
      "Test loss (NRMSE): 0.0324260\n",
      "===========================\n",
      "dataset 70\n",
      "Test loss (NRMSE): 0.0544680\n",
      "===========================\n",
      "dataset 71\n",
      "Test loss (NRMSE): 0.0297652\n",
      "===========================\n",
      "dataset 72\n",
      "Test loss (NRMSE): 0.0326099\n",
      "===========================\n",
      "dataset 73\n",
      "Test loss (NRMSE): 0.0616327\n",
      "===========================\n",
      "dataset 74\n",
      "Test loss (NRMSE): 0.0288802\n",
      "===========================\n",
      "dataset 75\n",
      "Test loss (NRMSE): 0.0384062\n",
      "===========================\n",
      "dataset 76\n",
      "Test loss (NRMSE): 0.0244282\n",
      "===========================\n",
      "dataset 77\n",
      "Test loss (NRMSE): 0.0296025\n",
      "===========================\n",
      "dataset 78\n",
      "Test loss (NRMSE): 0.0403448\n",
      "===========================\n",
      "dataset 79\n",
      "Test loss (NRMSE): 0.0251859\n",
      "===========================\n",
      "dataset 80\n",
      "Test loss (NRMSE): 0.0396403\n",
      "===========================\n",
      "dataset 81\n",
      "Test loss (NRMSE): 0.0281520\n",
      "===========================\n",
      "dataset 82\n",
      "Test loss (NRMSE): 0.0475665\n",
      "===========================\n",
      "dataset 83\n",
      "Test loss (NRMSE): 0.0273606\n",
      "===========================\n",
      "dataset 84\n",
      "Test loss (NRMSE): 0.0431991\n",
      "===========================\n",
      "dataset 85\n",
      "Test loss (NRMSE): 0.0263593\n",
      "===========================\n",
      "dataset 86\n",
      "Test loss (NRMSE): 0.0551357\n",
      "===========================\n",
      "dataset 87\n",
      "Test loss (NRMSE): 0.0356155\n",
      "===========================\n",
      "dataset 88\n",
      "Test loss (NRMSE): 0.0283959\n",
      "===========================\n",
      "dataset 89\n",
      "Test loss (NRMSE): 0.0266622\n",
      "===========================\n",
      "dataset 90\n",
      "Test loss (NRMSE): 0.0360288\n",
      "===========================\n",
      "dataset 91\n",
      "Test loss (NRMSE): 0.0271870\n",
      "===========================\n",
      "dataset 92\n",
      "Test loss (NRMSE): 0.0438048\n",
      "===========================\n",
      "dataset 93\n",
      "Test loss (NRMSE): 0.0802252\n",
      "===========================\n",
      "dataset 94\n",
      "Test loss (NRMSE): 0.0333416\n",
      "===========================\n",
      "dataset 95\n",
      "Test loss (NRMSE): 0.0254405\n",
      "===========================\n",
      "dataset 96\n",
      "Test loss (NRMSE): 0.0414558\n",
      "===========================\n",
      "dataset 97\n",
      "Test loss (NRMSE): 0.0211233\n",
      "===========================\n",
      "dataset 98\n",
      "Test loss (NRMSE): 0.0514059\n",
      "===========================\n",
      "dataset 99\n",
      "Test loss (NRMSE): 0.0298132\n",
      "===========================\n",
      "dataset 100\n",
      "Test loss (NRMSE): 0.0262377\n",
      "===========================\n",
      "dataset 101\n",
      "Test loss (NRMSE): 0.0225656\n",
      "===========================\n",
      "dataset 102\n",
      "Test loss (NRMSE): 0.0443111\n",
      "===========================\n",
      "\n",
      "test loss file saved!\n",
      "\n",
      "CPU times: total: 2min 18s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_model = torch.load(\"[PyG] [14 bus] Best_NN_model.pt\")\n",
    "best_model.eval()\n",
    "\n",
    "test_loss_list = []\n",
    "\n",
    "for i in range(102):\n",
    "    \n",
    "    dataset = pd.read_excel('dataset\\Grid_14 bus_%d.xlsx' % (i+1)).values\n",
    "    test_percentage = 100\n",
    "    test_dataset = slice_dataset(dataset, test_percentage)\n",
    "    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)\n",
    "    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)\n",
    "    \n",
    "    print('dataset {:d}'.format(i+1))\n",
    "    \n",
    "    yhat = denormalize_output(best_model(x_norm_test), y_val_mean, y_val_std)\n",
    "    y = y_raw_test\n",
    "    test_loss_NRMSE = NRMSE(yhat, y)\n",
    "    \n",
    "    if i == 0:\n",
    "        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    elif i == 1:\n",
    "        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "    else:\n",
    "        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))\n",
    "        test_loss_list.append(test_loss_NRMSE.detach().numpy())\n",
    "    \n",
    "    print(\"===========================\")\n",
    "\n",
    "column = []\n",
    "for i in range(100):\n",
    "    column.append('test loss %d' % (i+1))\n",
    "    \n",
    "test_loss_file = pd.DataFrame([test_loss_list], columns=column)\n",
    "test_loss_file.to_excel(\"[PyG] [14 bus] [NRMSE] NN test loss.xlsx\")\n",
    "print(\"\\ntest loss file saved!\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
