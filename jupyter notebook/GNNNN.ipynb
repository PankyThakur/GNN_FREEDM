{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "torch.set_printoptions(precision=5, sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv('dataset/measured_active_power.csv')\n",
    "dataset2 = pd.read_csv('dataset/measured_reactive_power.csv')\n",
    "dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv')\n",
    "dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(dataset1)\n",
    "df2 = pd.DataFrame(dataset2)\n",
    "df3 = pd.DataFrame(dataset3)\n",
    "df4 = pd.DataFrame(dataset4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(columns=['Timestep'])\n",
    "df2 = df2.drop(columns=['Timestep'])\n",
    "df3 = df3.drop(columns=['Timestep'])\n",
    "df4 = df4.drop(columns=['Timestep'])\n",
    "\n",
    "phase1_df_p = df1.filter(regex='.1$')\n",
    "phase1_df_q = df2.filter(regex='.1$')\n",
    "phase1_df_v = df3.filter(regex='.1$')\n",
    "phase1_df_d = df4.filter(regex='.1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\FREEDM\\gnn-powerflow\\jupyter notebook\\dataset\\edge_matrix.csv'\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to sort the column issues\n",
    "\n",
    "def update_dataframe_columns(df, column_list):\n",
    "    # Step 1: Extract the part of the column names before `.1`\n",
    "    cleaned_columns = df.columns.str.replace(r'\\.\\d+', '', regex=True).str.lower()\n",
    "    \n",
    "    # Step 2: Compare with the provided list of columns (ignore case)\n",
    "    column_list_lower = [col.lower() for col in column_list]\n",
    "    \n",
    "    # Step 3: Find missing columns from the provided list\n",
    "    missing_columns = [col for col in column_list_lower if col not in cleaned_columns]\n",
    "    \n",
    "    # Step 4: Add missing columns with the `.1` suffix and initialize them with 0\n",
    "    for missing_col in missing_columns:\n",
    "        df[f'{missing_col}.1'] = 0\n",
    "    \n",
    "    # Step 5: Return the DataFrame with all column names in lowercase\n",
    "    return df.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(df, column_list):\n",
    "    # Step 1: Strip the `.1` suffix from the DataFrame columns\n",
    "    ordered_cols = [f\"{col}.1\" for col in column_list]\n",
    "\n",
    "    return df[ordered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n",
      "C:\\Users\\panka\\AppData\\Local\\Temp\\ipykernel_39868\\701338628.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f'{missing_col}.1'] = 0\n"
     ]
    }
   ],
   "source": [
    "phase1_df_p_cleaned = update_dataframe_columns(phase1_df_p, desired_cols)\n",
    "p1_p = reorder_columns(phase1_df_p_cleaned, desired_cols)\n",
    "\n",
    "phase1_df_q_cleaned = update_dataframe_columns(phase1_df_q, desired_cols)\n",
    "p1_q = reorder_columns(phase1_df_q_cleaned, desired_cols)\n",
    "\n",
    "phase1_df_v_cleaned = update_dataframe_columns(phase1_df_v, desired_cols)\n",
    "p1_v = reorder_columns(phase1_df_v_cleaned, df.columns)\n",
    "\n",
    "phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)\n",
    "p1_d = reorder_columns(phase1_df_d_cleaned, desired_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_p.columns = ['P_' + str(col) for col in p1_p.columns]\n",
    "p1_q.columns = ['Q_' + str(col) for col in p1_q.columns]\n",
    "p1_v.columns = ['V_' + str(col) for col in p1_v.columns]\n",
    "p1_d.columns = ['d_' + str(col) for col in p1_d.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([p1_p, p1_q, p1_v, p1_d], axis=1)\n",
    "\n",
    "# Sorting columns by the suffix part to ensure same suffix columns are next to each other\n",
    "combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataset(dataset, percentage):\n",
    "    data_size = len(dataset)\n",
    "    return dataset[:int(data_size*percentage/100)]\n",
    "\n",
    "def make_dataset(dataset, n_bus):\n",
    "    x_raw_1, y_raw_1 = [], []\n",
    "    x_raw, y_raw = [], []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        for n in range(n_bus):\n",
    "            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))\n",
    "            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])\n",
    "\n",
    "        x_raw.append(list(x_raw_1))\n",
    "        y_raw.append(y_raw_1)\n",
    "        x_raw_1, y_raw_1 = [], []\n",
    "\n",
    "    x_raw = torch.tensor(x_raw, dtype=torch.float)\n",
    "    y_raw = torch.tensor(y_raw, dtype=torch.float)\n",
    "    return x_raw, y_raw\n",
    "\n",
    "def normalize_dataset(x, y):\n",
    "    x_mean = torch.mean(x,0)\n",
    "    y_mean = torch.mean(y,0)\n",
    "    x_std = torch.std(x,0)\n",
    "    y_std = torch.std(y,0)\n",
    "    x_norm = (x-x_mean)/x_std\n",
    "    y_norm = (y-y_mean)/y_std\n",
    "    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)\n",
    "    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)\n",
    "    return x_norm, y_norm, x_mean, y_mean, x_std, y_std\n",
    "\n",
    "def denormalize_output(y_norm, y_mean, y_std):\n",
    "    y = y_norm*y_std+y_mean\n",
    "    return y\n",
    "\n",
    "def NRMSE(yhat,y):\n",
    "    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))\n",
    "\n",
    "def MSE(yhat,y):\n",
    "    return torch.mean((yhat-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_GNN_NN(torch.nn.Module):\n",
    "    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):\n",
    "        super(My_GNN_NN, self).__init__()\n",
    "        self.feat_in = feat_in if feat_in is not None else 2\n",
    "        self.feat_size1 = feat_in if feat_in is not None else 4\n",
    "        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20\n",
    "        self.output_size = output_size if output_size is not None else 12\n",
    "        \n",
    "        self.conv1 = GCNConv(feat_in, feat_size1)\n",
    "        self.lin1 = Linear(node_size*feat_size1, hidden_size1)\n",
    "        self.lin2 = Linear(hidden_size1, output_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = x.flatten(start_dim = 0)\n",
    "        x = self.lin1(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def save_weights(self, model, name):\n",
    "        torch.save(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 100\n",
    "n_bus_p1 = 84\n",
    "n_bus_p2 = 84\n",
    "n_bus_p3 = 84\n",
    "\n",
    "#Phase1\n",
    "phase1_dataset = slice_dataset(combined_df, percentage).to_numpy()\n",
    "x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your features and y is your labels\n",
    "X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now split X_temp and y_temp into validation and test sets\n",
    "X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)\n",
    "\n",
    "#Phase1\n",
    "\n",
    "x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)\n",
    "x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)\n",
    "row_names = df.index.tolist()    # Row names (from the first column)\n",
    "col_names = df.columns.tolist()  # Column names (from the first row)\n",
    "\n",
    "# Ensure that the row and column names match (adjacency matrix should be square)\n",
    "assert row_names == col_names, \"Row names and column names must match in an adjacency matrix.\"\n",
    "\n",
    "# Step 3: Initialize two lists to store source and target nodes\n",
    "source_nodes = []\n",
    "target_nodes = []\n",
    "\n",
    "# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)\n",
    "for i in range(df.shape[0]):  # iterate over rows (source nodes)\n",
    "    for j in range(df.shape[1]):  # iterate over columns (target nodes)\n",
    "        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j\n",
    "            source_nodes.append(row_names[i])  # append the source node name\n",
    "            target_nodes.append(col_names[j])  # append the target node name\n",
    "\n",
    "# Step 5: Convert node names to numerical indices\n",
    "node_to_index = {name: idx for idx, name in enumerate(row_names)}\n",
    "\n",
    "source_indices = [node_to_index[node] for node in source_nodes]\n",
    "target_indices = [node_to_index[node] for node in target_nodes]\n",
    "\n",
    "# Step 6: Create the edge_index tensor\n",
    "edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Assuming x_train, y_train, x_val, y_val are already defined\n",
    "x_train, y_train = x_norm_phase1_train, y_norm_phase1_train\n",
    "x_val, y_val = x_norm_phase1_val, y_norm_phase1_val\n",
    "\n",
    "data_train_list, data_val_list = [], []\n",
    "\n",
    "# Generate Data for training\n",
    "for i in range(len(x_train)):\n",
    "    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically\n",
    "   \n",
    "    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))\n",
    "\n",
    "# Generate Data for validation\n",
    "for i in range(len(x_val)):\n",
    "    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically\n",
    "    \n",
    "    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(data_train_list, batch_size=1)\n",
    "val_loader = DataLoader(data_val_list, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias\n",
      "torch.Size([4])\n",
      "conv1.lin.weight\n",
      "torch.Size([4, 2])\n",
      "lin1.weight\n",
      "torch.Size([168, 336])\n",
      "lin1.bias\n",
      "torch.Size([168])\n",
      "lin2.weight\n",
      "torch.Size([168, 168])\n",
      "lin2.bias\n",
      "torch.Size([168])\n"
     ]
    }
   ],
   "source": [
    "n_bus_p1 = 84\n",
    "feat_in = 2\n",
    "feat_size1 = 4\n",
    "hidden_size1 = 168\n",
    "output_size = n_bus_p1*2\n",
    "lr = 0.0001\n",
    "\n",
    "model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)\n",
    "for name, param in model.named_parameters():\n",
    "  print(name)\n",
    "  print(param.size())\n",
    "\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    train loss: 14243.8764695    val loss: 2434.6410006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m y_train_prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Ensure target is on the same device\u001b[39;00m\n\u001b[0;32m     44\u001b[0m target \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 17\u001b[0m, in \u001b[0;36mMy_GNN_NN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m     15\u001b[0m     x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(x)\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(start_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\panka\\.virtualenvs\\gnn-powerflow-0Gf6W7zT\\Lib\\site-packages\\torch_geometric\\utils\\loop.py:634\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    628\u001b[0m     loop_index: Tensor \u001b[38;5;241m=\u001b[39m EdgeIndex(\n\u001b[0;32m    629\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, N, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    630\u001b[0m         sparse_size\u001b[38;5;241m=\u001b[39m(N, N),\n\u001b[0;32m    631\u001b[0m         is_undirected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 634\u001b[0m     loop_index \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    638\u001b[0m     loop_attr \u001b[38;5;241m=\u001b[39m compute_loop_attr(  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    639\u001b[0m         edge_index, edge_attr, N, \u001b[38;5;28;01mFalse\u001b[39;00m, fill_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define your parameters\n",
    "feat_in = 2\n",
    "feat_size1 = 4\n",
    "hidden_size1 = 84\n",
    "output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code\n",
    "lr = 0.0001\n",
    "\n",
    "# Initialize the model and move it to GPU if available\n",
    "model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size).to(device)\n",
    "\n",
    "# Optimizer remains the same\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Lists for storing training and validation losses\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "# Early stopping parameters\n",
    "count = 0\n",
    "patience = 2000\n",
    "lossMin = 1e10\n",
    "\n",
    "# Set the number of steps to accumulate gradients over\n",
    "accumulation_steps = 4  # This means accumulate gradients over 4 batches before updating weights\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(2001):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move batch and all required tensors to GPU\n",
    "        batch = batch.to(device)  # Make sure batch data is on the GPU\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        y_train_prediction = model(batch)\n",
    "\n",
    "        # Ensure target is on the same device\n",
    "        target = batch.y.to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = MSE(\n",
    "            denormalize_output(y_train_prediction, y_phase1_val_mean.to(device), y_phase1_val_std.to(device)),\n",
    "            denormalize_output(target, y_phase1_val_mean.to(device), y_phase1_val_std.to(device))\n",
    "        )\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the loss\n",
    "        train_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation for validation\n",
    "        for batch in val_loader:\n",
    "            # Move the batch data to the same device as the model (GPU or CPU)\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            y_val_prediction = model(batch)\n",
    "\n",
    "            # Compute validation loss\n",
    "            loss = MSE(\n",
    "                denormalize_output(y_val_prediction, y_phase1_val_mean.to(device), y_phase1_val_std.to(device)),\n",
    "                denormalize_output(batch.y, y_phase1_val_mean.to(device), y_phase1_val_std.to(device))\n",
    "            )\n",
    "\n",
    "            # Accumulate the loss, weighted by the number of graphs\n",
    "            val_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "    # Average validation loss over the dataset\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_loss_list.append(val_loss)\n",
    "\n",
    "    # Early stopping mechanism\n",
    "    if val_loss < lossMin:\n",
    "        lossMin = val_loss\n",
    "        count = 0\n",
    "        best_epoch = epoch\n",
    "        best_train_loss = train_loss\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "        # Save the best model\n",
    "        model.save_weights(model, \"New_GNN_NN.pt\")\n",
    "    else:\n",
    "        count += 1\n",
    "        if count > patience:\n",
    "            print(f\"Early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}\")\n",
    "            print(f\"Best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}\")\n",
    "            break\n",
    "\n",
    "    # Stop if training loss is too low\n",
    "    if train_loss <= 0:\n",
    "        print(f\"Min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}\")\n",
    "        break\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-powerflow-0Gf6W7zT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
