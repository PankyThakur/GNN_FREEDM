 1/1:
from sklearn.linear_model import LassoCV
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedKFold
 1/2: pip install skleran
 1/3: pip install scikit-learn
 1/4:
from sklearn.linear_model import LassoCV
from sklearn.model_selection import StratifiedKFold
 1/5:
import pandas as pd
import numpy as np
 1/6: pip install pandas
 1/7: pip install numpy
 1/8: data = pd.read_csv('C:\ALDA\Project\oil_top_10.csv')
 1/9:
import pandas as pd
import numpy as np
1/10: data = pd.read_csv('C:\ALDA\Project\oil_top_10.csv')
1/11: from sklearn.preprocessing import MinMaxScaler
1/12:
scaler_transform = MinMaxScaler()
min_max_normalized_data = scaler_transform.fit_transform(data)
1/13: data.head()
1/14: data.drop('Date')
1/15: data.drop('Date', axis = 'columns')
1/16:
min_max_scaler = MinMaxScaler()
min_max_normalized_data = min_max_scaler.fit_transform(data)
1/17: data.head()
1/18: data = data.drop('Date', axis = 'columns')
1/19:
min_max_scaler = MinMaxScaler()
min_max_normalized_data = min_max_scaler.fit_transform(data)
1/20:
X = min_max_normalized_data.iloc[:, 0:-1]
y = min_max_normalized_data.iloc[:, -1]
skf = StratifiedKFold(n_splits=10
lasso = LassoCV(cv=skf, random_state=42).fit(X, y)print('Selected Features:', list(min_max_normalized_data.feature_names[np.where(lasso.coef_!=0)[0]]))
1/21:
X = min_max_normalized_data.iloc[:, 0:-1]
y = min_max_normalized_data.iloc[:, -1]
skf = StratifiedKFold(n_splits=10)
lasso = LassoCV(cv=skf, random_state=42).fit(X, y)print('Selected Features:', list(min_max_normalized_data.feature_names[np.where(lasso.coef_!=0)[0]]))
1/22:
X = min_max_normalized_data.iloc[:, 0:-1]
y = min_max_normalized_data.iloc[:, -1]
skf = StratifiedKFold(n_splits=10)
lasso = LassoCV(cv=skf, random_state=42).fit(X, y)
print('Selected Features:', list(min_max_normalized_data.feature_names[np.where(lasso.coef_!=0)[0]]))
1/23: type(min_max_normalized_data)
1/24: min_max_normalized_df = pd.Dataframe(min_max_normalized_data)
1/25: min_max_normalized_df = pd.DataFrame(min_max_normalized_data)
1/26:
X = min_max_normalized_df.iloc[:, 0:-1]
y = min_max_normalized_df.iloc[:, -1]
skf = StratifiedKFold(n_splits=10)
lasso = LassoCV(cv=skf, random_state=42).fit(X, y)
print('Selected Features:', list(min_max_normalized_data.feature_names[np.where(lasso.coef_!=0)[0]]))
1/27:
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 10, random_state = 0)
rfe = RFE(model, 10)
rfe = rfe.fit(X, y)
print(rfe.support_)
print(rfe.ranking_)
1/28:
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 10, random_state = 0)
rfe = RFE(model)
rfe = rfe.fit(X, y)
print(rfe.support_)
print(rfe.ranking_)
1/29: data = pd.read_csv('C:\ALDA\Project\oil_merge_13.csv')
1/30: from sklearn.preprocessing import MinMaxScaler
1/31: data.head()
1/32: data = data.drop('Date', axis = 'columns')
1/33:
min_max_scaler = MinMaxScaler()
min_max_normalized_data = min_max_scaler.fit_transform(data)
1/34: min_max_normalized_df = pd.DataFrame(min_max_normalized_data)
1/35:
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 10, random_state = 0)
rfe = RFE(model)
rfe = rfe.fit(X, y)
print(rfe.support_)
print(rfe.ranking_)
1/36:
X = min_max_normalized_df.iloc[:, 0:-1]
y = min_max_normalized_df.iloc[:, -1]
skf = StratifiedKFold(n_splits=10)
lasso = LassoCV(cv=skf, random_state=42).fit(X, y)
print('Selected Features:', list(min_max_normalized_data.feature_names[np.where(lasso.coef_!=0)[0]]))
1/37:
X = min_max_normalized_df.iloc[:, 0:-1]
y = min_max_normalized_df.iloc[:, -1]
1/38:
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 10, random_state = 0)
rfe = RFE(model)
rfe = rfe.fit(X, y)
print(rfe.support_)
print(rfe.ranking_)
1/39:
feature_ranking = rfe.ranking_

# Get the names of the features
feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/40:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

# Get the names of the features
feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/41: pip install matplotlib
1/42:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

# Get the names of the features
feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/43:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = list(data[:,1:13].columns.values)
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/44:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = X.columns.values.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/45:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = X.columns.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/46:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = X.columns.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/47:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = X.columns.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]
print(feature_names)
# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/48:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = X.columns.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]
print(X.head())
# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/49:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]
print(data.head())
# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/50:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()
# Get the names of the features
#feature_names = [f"Feature {i}" for i in range(1, len(feature_ranking) + 1)]
print(feature_names)
# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/51:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('heat_close')
# Get the names of the features
print(feature_names)
# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/52:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
1/53:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('heat_close')
# Get the names of the features

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/54:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]
type(selected_features)
# Print selected features
print('Selected Features:', selected_features)
1/55:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]
type(selected_features)
# Print selected features
print('Selected Features:', selected_features)
1/56:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]
print(type(selected_features))
# Print selected features
print('Selected Features:', selected_features)
1/57:
feature_names = data.columns.tolist()

feature_names.remove('heat_close')

print(feature_names[selected_features])
1/58:
feature_names = data.columns.tolist()

feature_names.remove('heat_close')

print(feature_names[i] i for i in selected_features)
1/59:
feature_names = data.columns.tolist()

feature_names.remove('heat_close')

print([feature_names[i] i for i in selected_features])
1/60:
feature_names = data.columns.tolist()

feature_names.remove('heat_close')

print([feature_names[i] for i in selected_features])
1/61:
X = min_max_normalized_df.iloc[:, 1:-]
y = min_max_normalized_df.iloc[:, 0]
1/62:
X = min_max_normalized_df.iloc[:, 1:]
y = min_max_normalized_df.iloc[:, 0]
1/63: print(X.columns)
1/64: print(X.columns.tolist())
1/65: print(X.columns.values)
1/66: print(X.columns.names)
1/67: print(X.columns.tolist())
1/68:
X = pd.Dataframe(X)
y = pd.Dataframe(y)

print(X.column.values)
print(y.column.values)
1/69:
X = pd.DataFrame(X)
y = pd.DataFrame(y)

print(X.column.values)
print(y.column.values)
1/70:
X = pd.DataFrame(X)
y = pd.DataFrame(y)

print(X.columns.values)
print(y.columns.values)
1/71:
X = pd.DataFrame(X)
y = pd.DataFrame(y)

print(X.columns.tolist())
print(y.columns.tolist())
1/72:
Xx = pd.DataFrame(X)
yy = pd.DataFrame(y)

Xx.head()
1/73:
Xx = pd.DataFrame(min_max_normalized_df.iloc[:, 1:])
yy = pd.DataFrame(min_max_normalized_df.iloc[:, 0])

Xx.head()
1/74:
X = min_max_normalized_df.iloc[:, 1:]
y = min_max_normalized_df.iloc[:, 0]
1/75:
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 10, random_state = 0)
rfe = RFE(model)
rfe = rfe.fit(X, y)
print(rfe.support_)
print(rfe.ranking_)
1/76:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('heat_close')
# Get the names of the features

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/77:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')
# Get the names of the features

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
1/78:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]
print(type(selected_features))
# Print selected features
print('Selected Features:', selected_features)
1/79:
feature_names = data.columns.tolist()

feature_names.remove('heat_close')

print([feature_names[i] for i in selected_features])
1/80:
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

print([feature_names[i] for i in selected_features])
1/81:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
1/82:
X = min_max_normalized_df.iloc[:, 1:]
y = min_max_normalized_df.iloc[:, 0]
1/83:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
1/84:
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

print([feature_names[i] for i in selected_features])
1/85:
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

print(len(feature_names))
1/86:
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

print([feature_names[i-1] for i in selected_features])
 2/1:
import pandas as pd
import numpy as np
 2/2: data = read_csv('adj_real_estate.csv')
 2/3: data = pd.read_csv('adj_real_estate.csv')
 2/4: data.head()
 2/5:
x_vals = data.values[:,0:3]
y_vals = data.values[:,3]
 2/6: x_vals = np.hstack((np.ones((len(x_data),1)),x_data))
 2/7: x_vals = np.hstack((np.ones((len(x_vals),1)),x_data))
 2/8: x_vals = np.hstack((np.ones((len(x_vals),1)),x_vals))
 2/9: x_t = np.matrix.transpose(x_vals)
2/10: x_transpose = np.matrix.transpose(x_vals)
2/11:
temp1 = np.dot(x_transpose, x_vals)
temp1.head()
2/12:
temp1 = np.dot(x_transpose, x_vals)
temp2 = x_transpose@x_vals
2/13: temp1 == temp2
2/14: x_transpose_x = np.dot(x_transpose, x_vals)
2/15: x_tranpose_x_inverse = np.linalg.inv(x_transpose_x)
2/16: x_transpose_y = np.dot(x_transpose, y_vals)
2/17:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    x_tranpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
2/18:
from sklearn.model_selection import LeaveOneOut

cv = LeaveOneOut()

model1 = regression_model(x_vals,y_vals,1)
2/19:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    x_transpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
2/20:
from sklearn.model_selection import LeaveOneOut

cv = LeaveOneOut()

model1 = regression_model(x_vals,y_vals,1)
2/21:
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error',
                         cv=cv, n_jobs=-1)

mean(absolute(scores))
2/22:
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model1, x_vals, y_vals, scoring='neg_mean_absolute_error',
                         cv=cv, n_jobs=-1)

mean(absolute(scores))
2/23:
from sklearn.model_selection import LeaveOneOut

cv = LeaveOneOut()

model1 = regression_model(x_vals,y_vals,1)
2/24:
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model1, x_vals, y_vals, scoring='neg_mean_absolute_error',
                         cv=cv, n_jobs=-1)

mean(absolute(scores))
2/25:
def x_mul_coeff(x,coeff,n):
    x = x**n
    return x@coeff
2/26:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
2/27:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
    return MSE
2/28: print("MSE1: ",leaveOneOut(x_vals,y_vals,1))
 3/1: print("MSE1: ",leaveOneOut(x_vals,y_vals,2))
 3/2:
import pandas as pd
import numpy as np
 3/3: data = pd.read_csv('adj_real_estate.csv')
 3/4: data.head()
 3/5:
x_vals = data.values[:,0:3]
y_vals = data.values[:,3]
 3/6: x_vals = np.hstack((np.ones((len(x_vals),1)),x_vals))
 3/7: x_transpose = np.matrix.transpose(x_vals)
 3/8: x_transpose_x = np.dot(x_transpose, x_vals)
 3/9: x_tranpose_x_inverse = np.linalg.inv(x_transpose_x)
3/10: x_transpose_y = np.dot(x_transpose, y_vals)
3/11:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    x_transpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
3/12:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
    return MSE
3/13:
def x_mul_coeff(x,coeff,n):
    x = x**n
    return x@coeff
3/14: print("MSE1: ",leaveOneOut(x_vals,y_vals,1))
3/15: print("MSE1: ",leaveOneOut(x_vals,y_vals,2))
3/16: print("MSE1: ",leaveOneOut(x_vals,y_vals,3))
3/17:
coeff_alpha = regression_model(x_vals,y_vals,1)
Print("Alpha vals: ", coeff_alpha)
3/18:
coeff_alpha = regression_model(x_vals,y_vals,1)
print("Alpha vals: ", coeff_alpha)
3/19:
coeff_beta = regression_model(x_vals,y_vals,2)
print("Beta values: ", coeff_beta)
3/20:
coeff_gama = regression_model(x_vals,y_vals,3)
print("Gamma vals: ", coeff_gamma)
3/21:
coeff_gamma = regression_model(x_vals,y_vals,3)
print("Gamma vals: ", coeff_gamma)
3/22:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    x_transpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
3/23:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
    return MSE
3/24:
def x_mul_coeff(x,coeff,n):
    x = x**n
    return x@coeff
3/25: print("MSE1: ",leaveOneOut(x_vals,y_vals,1))
3/26:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    #x_transpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    #coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    coeffs = x_transpose_x @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
3/27:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
    return MSE
3/28:
def x_mul_coeff(x,coeff,n):
    x = x**n
    return x@coeff
3/29: print("MSE1: ",leaveOneOut(x_vals,y_vals,1))
3/30: print("MSE1: ",leaveOneOut(x_vals,y_vals,2))
3/31: print("MSE1: ",leaveOneOut(x_vals,y_vals,3))
3/32:
coeff_alpha = regression_model(x_vals,y_vals,1)
print("Alpha vals: ", coeff_alpha)
3/33:
coeff_beta = regression_model(x_vals,y_vals,2)
print("Beta values: ", coeff_beta)
3/34:
coeff_gamma = regression_model(x_vals,y_vals,3)
print("Gamma vals: ", coeff_gamma)
3/35:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    x_transpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    #coeffs = x_transpose_x @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
3/36:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
    return MSE
3/37:
def x_mul_coeff(x,coeff,n):
    x = x**n
    return x@coeff
3/38: print("MSE1: ",leaveOneOut(x_vals,y_vals,1))
3/39: print("MSE1: ",leaveOneOut(x_vals,y_vals,2))
3/40: print("MSE1: ",leaveOneOut(x_vals,y_vals,3))
3/41:
coeff_alpha = regression_model(x_vals,y_vals,1)
print("Alpha vals: ", coeff_alpha)
3/42:
coeff_beta = regression_model(x_vals,y_vals,2)
print("Beta values: ", coeff_beta)
3/43:
coeff_gamma = regression_model(x_vals,y_vals,3)
print("Gamma vals: ", coeff_gamma)
3/44:
import pandas as pd
import numpy as np
3/45: data = pd.read_csv('adj_real_estate.csv')
3/46: data.head()
3/47:
x_vals = data.values[:,0:3]
y_vals = data.values[:,3]
3/48: x_vals = np.hstack((np.ones((len(x_vals),1)),x_vals))
3/49:
def regression_model(x_vals,y_vals,n):
    x_vals = x_vals**n
    x_transpose = np.matrix.transpose(x_vals) 
    x_transpose_x = np.dot(x_transpose, x_vals) #X'X
    x_transpose_x_inverse = np.linalg.inv(x_transpose_x) #(X'X)**-1
    x_transpose_y = np.dot(x_transpose, y_vals) #X'Y
    coeffs = x_transpose_x_inverse @ x_transpose_y #(X'X)**-1 (X'Y)
    #coeffs = x_transpose_x @ x_transpose_y #(X'X)**-1 (X'Y)
    return coeffs
3/50:
def leaveOneOut(x,y,n):
    #defining and initializing an empty list to store predictions. Size same as y
    prediction_list = [0]*y
    for i in range(0,len(y)):
        #Leaving one row out for both x and y
        x_modified = np.delete(x,i,0)
        y_modified = np.delete(y,i,0)
        #Calculating coefficients of the modified dataset
        coeffs = regression_model(x_modified,y_modified,n)
        prediction_list[i] = x_mul_coeff(x[i],coeffs,n)
    MSE = np.sqrt(((np.array(prediction_list) - np.array(y))**2).mean())
    return MSE
3/51:
def x_mul_coeff(x,coeff,n):
    x = x**n
    return x@coeff
3/52:
coeff_alpha = regression_model(x_vals,y_vals,1)
print("Alpha values: ")
print("alpha0: ",coeff_alpha[0])
print("alpha1: ",coeff_alpha[1])
print("alpha2: ",coeff_alpha[2])
print("alpha3: ",coeff_alpha[3])
3/53:
coeff_beta = regression_model(x_vals,y_vals,2)
print("Beta values: ")
print("beta0: ",coeff_alpha[0])
print("beta1: ",coeff_alpha[1])
print("beta2: ",coeff_alpha[2])
print("beta3: ",coeff_alpha[3])
3/54:
coeff_beta = regression_model(x_vals,y_vals,2)
print("Beta values: ")
print("beta0: ",coeff_beta[0])
print("beta1: ",coeff_beta[1])
print("beta2: ",coeff_beta[2])
print("beta3: ",coeff_beta[3])
3/55:
coeff_gamma = regression_model(x_vals,y_vals,2)
print("Gamma values: ")
print("gamma0: ",coeff_gamma[0])
print("gamma1: ",coeff_gamma[1])
print("gamma2: ",coeff_gamma[2])
print("gamma3: ",coeff_gamma[3])
3/56:
coeff_gamma = regression_model(x_vals,y_vals,3)
print("Gamma values: ")
print("gamma0: ",coeff_gamma[0])
print("gamma1: ",coeff_gamma[1])
print("gamma2: ",coeff_gamma[2])
print("gamma3: ",coeff_gamma[3])
3/57:
print("Mean Squared Errors")
print("MSE for Model1: ",leaveOneOut(x_vals,y_vals,1))
print("MSE for Model2: ",leaveOneOut(x_vals,y_vals,2))
print("MSE for Model3: ",leaveOneOut(x_vals,y_vals,3))
3/58:
print("Root Mean Squared Errors")
print("RMSE for Model1: ",leaveOneOut(x_vals,y_vals,1))
print("RMSE for Model2: ",leaveOneOut(x_vals,y_vals,2))
print("RMSE for Model3: ",leaveOneOut(x_vals,y_vals,3))
 8/1:
import os
import numpy as np
import pandas as pd
 8/2: df = pd.read_csv('C:\Users\panka\Downloads',parse_dates=['Date'])
 8/3: df = pd.read_csv('C:\Users\panka\Downloads', parse_dates=['Date'])
 8/4: df = pd.read_csv(r'C:\Users\panka\Downloads', parse_dates=['Date'])
 9/1: df = pd.read_csv('C:\ALDA\Project\oil_merge_13.csv')
 9/2:
import os
import numpy as np
import pandas as pd
 9/3: df = pd.read_csv('C:\ALDA\Project\oil_merge_13.csv')
 9/4: df.head
 9/5: df.head()
 9/6: df = df.iloc[:,1:]
 9/7: df.columns
 9/8: X = df.iloc[:,1:]
 9/9: X.columns
9/10: Y = df.iloc[:,0]
9/11: selected_features = ['eur_close', 'heat_close']
9/12: X_selected = df[selected_features]
9/13: X_selected.columns
9/14:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X_selected)
9/15: X_normalized
9/16: from sklearn.model_selection import train_test_split
9/17: X_train, X_test, y_train, y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)
9/18: X_train_normalized, X_test_normalized, y_train_normalized, y_test_normalized = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)
9/19: X_train
9/20: X_train_normalized
9/21:
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
9/22:
knn_regressor = KNeighborsRegressor(n_neighbors=10)
knn_regressor.fit(X_train, y_train)
9/23:
rf_model = RandomForestRegressor(n_estimators=100)
rf_model.fit(X_train, y_train)
9/24: rf_pred = rf_model.predict(X_test)
9/25: knn_pred = knn_regressor.predict(X_test)
9/26: ensemble_pred = (rf_pred + knn_pred) / 2
9/27:
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, ensemble_pred)
print(f'Mean Squared Error: {mse}')
9/28:
mse_rf = mean_squared_error(y_test, rf_pred)
print(f'Mean Squared Error: {mse_rf}')
9/29:
mse_knn = mean_squared_error(y_test, knn_pred)
print(f'Mean Squared Error: {mse_knn}')
9/30:
knn_regressor = KNeighborsRegressor(n_neighbors=10)
knn_regressor.fit(X_train_normalized, y_train_normalized)
9/31:
rf_model = RandomForestRegressor(n_estimators=100)
rf_model.fit(X_train_normalized, y_train_normalized)
9/32:
rf_pred_normalized = rf_model.predict(X_test_normalized)
knn_pred_normalized = knn_regressor.predict(X_test_normalized)
9/33: ensemble_pred_normalized = (rf_pred_normalized + knn_pred_normalized) / 2
9/34:
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test_normalized, ensemble_pred_normalized)
print(f'Mean Squared Error: {mse}')
9/35:
mse_rf = mean_squared_error(y_test_normalized, rf_pred_normalized)
print(f'Mean Squared Error: {mse_rf}')
9/36:
mse_knn = mean_squared_error(y_test_normalized, knn_pred_normalized)
print(f'Mean Squared Error: {mse_knn}')
9/37:
rselected_features = ['gold_close', 'eur_close',
       'Henry Hub Natural Gas Spot Price Dollars per Million Btu', 'rub_close','SP500', 'heat_close']
9/38: Xr_selected = df[rselected_features]
9/39: Xr_selected.columns
9/40:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
Xr_normalized = scaler.fit_transform(Xr_selected)
9/41: from sklearn.model_selection import train_test_split
9/42: X_train, X_test, y_train, y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)
9/43: X_train_normalized, X_test_normalized, y_train_normalized, y_test_normalized = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)
9/44: Xr_train, Xr_test, yr_train, yr_test = train_test_split(Xr_selected, Y, test_size=0.2, random_state=42)
9/45: Xr_train_normalized, Xr_test_normalized, yr_train_normalized, yr_test_normalized = train_test_split(Xr_normalized, Y, test_size=0.2, random_state=42)
9/46: Xr_train
9/47:
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
9/48:
knn_regressor = KNeighborsRegressor(n_neighbors=10)
knn_regressor.fit(X_train, y_train)
9/49:
rf_model = RandomForestRegressor(n_estimators=100)
rf_model.fit(X_train, y_train)
9/50: rfr_pred = rf_model.predict(Xr_test)
9/51:
knn_regressor = KNeighborsRegressor(n_neighbors=10)
knn_regressor.fit(Xr_train, yr_train)
9/52:
rf_model = RandomForestRegressor(n_estimators=100)
rf_model.fit(Xr_train, yr_train)
9/53: rfr_pred = rf_model.predict(Xr_test)
9/54: knnr_pred = knn_regressor.predict(Xr_test)
9/55: ensemble_predr = (rfr_pred + knnr_pred) / 2
9/56:
from sklearn.metrics import mean_squared_error

rmse = mean_squared_error(yr_test, ensemble_predr)
print(f'Mean Squared Error: {rmse}')
9/57:
rmse_rf = mean_squared_error(yr_test, rfr_pred)
print(f'Mean Squared Error: {rmse_rf}')
9/58:
rmse_knn = mean_squared_error(yr_test, knnr_pred)
print(f'Mean Squared Error: {rmse_knn}')
9/59:
knn_regressor = KNeighborsRegressor(n_neighbors=10)
knn_regressor.fit(Xr_train_normalized, yr_train_normalized)
9/60:
rf_model = RandomForestRegressor(n_estimators=100)
rf_model.fit(Xr_train_normalized, yr_train_normalized)
9/61:
rf_pred_normalized = rf_model.predict(Xr_test_normalized)
knn_pred_normalized = knn_regressor.predict(Xr_test_normalized)
9/62: ensemble_pred_normalized = (rf_pred_normalized + knn_pred_normalized) / 2
9/63:
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(yr_test_normalized, ensemble_pred_normalized)
print(f'Mean Squared Error: {mse}')
9/64:
mse_rf = mean_squared_error(yr_test_normalized, rf_pred_normalized)
print(f'Mean Squared Error: {mse_rf}')
9/65:
mse_knn = mean_squared_error(yr_test_normalized, knn_pred_normalized)
print(f'Mean Squared Error: {mse_knn}')
 7/1: pip install scikit-learn
 7/2:
from sklearn.linear_model import LassoCV
from sklearn.model_selection import StratifiedKFold
 7/3: pip install pandas
 7/4: pip install numpy
 7/5:
import pandas as pd
import numpy as np
 7/6: data = pd.read_csv('C:\ALDA\Project\oil_merge_13.csv')
 7/7: from sklearn.preprocessing import MinMaxScaler
 7/8: data.head()
 7/9: data = data.drop('Date', axis = 'columns')
7/10:
min_max_scaler = MinMaxScaler()
min_max_normalized_data = min_max_scaler.fit_transform(data)
7/11: min_max_normalized_df = pd.DataFrame(min_max_normalized_data)
7/12:
X = min_max_normalized_df.iloc[:, 1:]
y = min_max_normalized_df.iloc[:, 0]
7/13:
Xx = pd.DataFrame(min_max_normalized_df.iloc[:, 1:])
yy = pd.DataFrame(min_max_normalized_df.iloc[:, 0])

Xx.head()
7/14:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/15: print(X.columns[lasso_model.coef]
7/16: print(X.columns[lasso_model.coef])
7/17: print(X.columns[lasso_model.coef_])
7/18: print(X.columns[lasso_model.coef_ !=0])
7/19: print(lasso_model.coef_ !=0)
7/20: print(lasso_model.coef_)
7/21:
from sklearn.linear_model import Lasso

alpha = 0.1  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/22: print(lasso_model.coef_)
7/23:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/24: print(lasso_model.coef_)
7/25:
from sklearn.linear_model import Lasso

alpha = 0.02  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/26: print(lasso_model.coef_)
7/27:
from sklearn.linear_model import Lasso

alpha = 0.001  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/28: print(lasso_model.coef_)
7/29:
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

print([feature_names[i-1] for i in selected_features])
7/30:
from sklearn.linear_model import Lasso

alpha = 0.015  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/31: print(lasso_model.coef_)
7/32:
from sklearn.linear_model import Lasso

alpha = 1  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/33:
from sklearn.linear_model import Lasso

alpha = 0.1  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/34: print(lasso_model.coef_)
7/35:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/36: print(lasso_model.coef_)
7/37:
alphas = np.logspace(-4, 2, 100)
coefs = []

for a in alphas:
    lasso_model = Lasso(alpha=a)
    lasso_model.fit(X, y)
    coefs.append(lasso_model.coef_)

plt.figure(figsize=(12, 8))
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Regularization Path')
plt.show()
7/38:
import matplotlib.pyplot as plt
alphas = np.logspace(-4, 2, 100)
coefs = []

for a in alphas:
    lasso_model = Lasso(alpha=a)
    lasso_model.fit(X, y)
    coefs.append(lasso_model.coef_)

plt.figure(figsize=(12, 8))
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Regularization Path')
plt.show()
7/39:
from sklearn.linear_model import LassoLarsIC

# Create a LassoLarsIC model
model_bic = LassoLarsIC(criterion='bic')
model_bic.fit(X, y)

# Get the best alpha according to BIC
best_alpha_bic = model_bic.alpha_
7/40:
from sklearn.linear_model import LassoLarsIC

# Create a LassoLarsIC model
model_bic = LassoLarsIC(criterion='bic')
model_bic.fit(X, y)

# Get the best alpha according to BIC
best_alpha_bic = model_bic.alpha_
print(best_alpha_bic)
7/41:
from sklearn.linear_model import Lasso

alpha = 1.507084815188837e-05  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/42: print(lasso_model.coef_)
7/43:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/44:
from sklearn.linear_model import Lasso

alpha = 0.001  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/45: print(lasso_model.coef_)
7/46:
# Get the coefficients
coefficients = lasso_model.coef_

# Plot the coefficients
plt.figure(figsize=(10, 6))
plt.bar(range(len(coefficients)), coefficients, tick_label=[f"Feature {i+1}" for i in range(len(coefficients))])
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.show()
7/47:
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot the coefficients against feature names
plt.figure(figsize=(10, 6))
plt.bar(feature_names, coefficients)
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.show()
7/48:
alphas = lasso_cv_model.alphas_
coefs = lasso_cv_model.coef_path_

plt.figure(figsize=(10, 6))
for i in range(len(coefs[0])):
    plt.plot(alphas, coefs[:, i], label=f'Feature {i + 1}')

plt.xscale('log')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Regularization Path')
plt.legend()
plt.show()
7/49:
alphas = lasso_model.alphas_
coefs = lasso_model.coef_path_

plt.figure(figsize=(10, 6))
for i in range(len(coefs[0])):
    plt.plot(alphas, coefs[:, i], label=f'Feature {i + 1}')

plt.xscale('log')
plt.xlabel('Alpha (Regularization Strength)')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Regularization Path')
plt.legend()
plt.show()
7/50:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='o', color='blue')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/51:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='o', color='pink')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/52:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='maroon')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/53:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='purple')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/54:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='violet')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/55:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = [f"Feature {i+1}" for i in range(len(coefficients))]

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='blue')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/56:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='blue')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/57:
from sklearn.linear_model import Lasso

alpha = 0.01  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/58: print(lasso_model.coef_)
7/59:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='blue')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/60:
from sklearn.linear_model import Lasso

alpha = 0.001  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/61: print(lasso_model.coef_)
7/62:
# Get the coefficients and feature names
coefficients = lasso_model.coef_
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

# Plot coefficients against feature names using a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(feature_names, coefficients, marker='x', color='blue')
plt.title('Lasso Regression Coefficients')
plt.xlabel('Features')
plt.ylabel('Coefficients')
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for better readability
plt.grid(True, linestyle='--', alpha=0.7)
plt.show()
7/63: data.columns
7/64:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')
# Get the names of the features

feature_names.rename(columns = {'Henry Hub Natural Gas Spot Price Dollars per Million Btu':'Henry Hub Natural Gas'}, inplace = True)

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
7/65: pip install scikit-learn
7/66: pip install scikit-learn
7/67:
from sklearn.linear_model import LassoCV
from sklearn.model_selection import StratifiedKFold
7/68: pip install pandas
7/69: pip install numpy
7/70:
import pandas as pd
import numpy as np
7/71: data = pd.read_csv('C:\ALDA\Project\oil_merge_13.csv')
7/72: from sklearn.preprocessing import MinMaxScaler
7/73: from sklearn.preprocessing import MinMaxScaler
7/74: data.head()
7/75: data = data.drop('Date', axis = 'columns')
7/76:
min_max_scaler = MinMaxScaler()
min_max_normalized_data = min_max_scaler.fit_transform(data)
7/77: min_max_normalized_df = pd.DataFrame(min_max_normalized_data)
7/78:
X = min_max_normalized_df.iloc[:, 1:]
y = min_max_normalized_df.iloc[:, 0]
7/79:
Xx = pd.DataFrame(min_max_normalized_df.iloc[:, 1:])
yy = pd.DataFrame(min_max_normalized_df.iloc[:, 0])

Xx.head()
7/80:
from sklearn.linear_model import Lasso

alpha = 0.001  # Adjust the regularization strength (hyperparameter)
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X, y)

# Get selected features
selected_features = X.columns[lasso_model.coef_ != 0]

# Print selected features
print('Selected Features:', selected_features)
7/81: print(lasso_model.coef_)
7/82:
import matplotlib.pyplot as plt
alphas = np.logspace(-4, 2, 100)
coefs = []

for a in alphas:
    lasso_model = Lasso(alpha=a)
    lasso_model.fit(X, y)
    coefs.append(lasso_model.coef_)

plt.figure(figsize=(12, 8))
plt.plot(alphas, coefs)
plt.xscale('log')
plt.xlabel('Alpha')
plt.ylabel('Coefficients')
plt.title('Lasso Regression Regularization Path')
plt.show()
7/83:
feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')

print([feature_names[i-1] for i in selected_features])
7/84:
from sklearn.linear_model import LassoLarsIC

# Create a LassoLarsIC model
model_bic = LassoLarsIC(criterion='bic')
model_bic.fit(X, y)

# Get the best alpha according to BIC
best_alpha_bic = model_bic.alpha_
print(best_alpha_bic)
7/85:
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 10, random_state = 0)
rfe = RFE(model)
rfe = rfe.fit(X, y)
print(rfe.support_)
print(rfe.ranking_)
7/86:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')
# Get the names of the features

feature_names.rename(columns = {'Henry Hub Natural Gas Spot Price Dollars per Million Btu':'Henry Hub Natural Gas'}, inplace = True)

# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
7/87: data.rename(columns = {'Henry Hub Natural Gas Spot Price Dollars per Million Btu':'Henry Hub Natural Gas'}, inplace = True)
7/88:
import matplotlib.pyplot as plt

feature_ranking = rfe.ranking_

feature_names = data.columns.tolist()

feature_names.remove('WTI_dollar_per_barrel')
# Get the names of the features



# Create a bar chart to visualize feature importance
plt.figure(figsize=(10, 6))
plt.barh(feature_names, feature_ranking, color='skyblue')
plt.xlabel('Ranking')
plt.title('RFE - Feature Ranking')
plt.show()
9/66: selected_features = ['copper_close', 'dji_index', 'Henry Hub Natural Gas Spot Price Dollars per Million Btu', 'eur_close', 'heat_close']
9/67: X_selected = df[selected_features]
9/68: X_selected.columns
9/69:
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X_selected)
9/70: X_normalized
9/71: from sklearn.model_selection import train_test_split
9/72: X_train, X_test, y_train, y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)
9/73: X_train_normalized, X_test_normalized, y_train_normalized, y_test_normalized = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)
9/74: X_train
9/75: X_train_normalized
9/76:
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
9/77:
knn_regressor = KNeighborsRegressor(n_neighbors=10)
knn_regressor.fit(X_train, y_train)
9/78:
rf_model = RandomForestRegressor(n_estimators=100)
rf_model.fit(X_train, y_train)
9/79: rf_pred = rf_model.predict(X_test)
9/80: rf_pred = rf_model.predict(X_test)
9/81: knn_pred = knn_regressor.predict(X_test)
9/82: ensemble_pred = (rf_pred + knn_pred) / 2
9/83:
from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, ensemble_pred)
print(f'Mean Squared Error: {mse}')
9/84:
mse_rf = mean_squared_error(y_test, rf_pred)
print(f'Mean Squared Error: {mse_rf}')
9/85:
mse_knn = mean_squared_error(y_test, knn_pred)
print(f'Mean Squared Error: {mse_knn}')
12/1:
import numpy as np

# This is the function that we will be using to check accuracy
def AlmostEqual(P, Q, digits):
    epsilon = 10 ** -digits
    return np.linalg.norm(P-Q) < epsilon
12/2:
import numpy as np

# This is the function that we will be using to check accuracy
def AlmostEqual(P, Q, digits):
    epsilon = 10 ** -digits
    return np.linalg.norm(P-Q) < epsilon
12/3:
def Df(x,y,z):
    # Initializing the values
    f_x = 0
    f_y = 0
    f_z = 0

    f_x = -16*x**15 + 12*x**2*y**2 - 2*z**3/x**3
    f_y = 8*x**3*y - 4*y**3*np.exp(z) + 4
    f_z = -y**4*np.exp(z) + 3*z**2/x**2


    return np.array([[f_x,f_y,f_z]])
12/4:
# Test input
x = 2
y = 4
z = -1

# True value of gradient for test input
Df_true = np.array([-5.23519750e+05, 1.65822863e+02, -9.34271369e+01])

# Check for correctness of result
assert AlmostEqual(Df_true, Df(x,y,z), 3)
12/5:
def Df(u,v,p,t):
    # Initializing the values
    f_u = 0
    f_v = 0
    f_p = 0
    f_t = 0

    f_u = 16*p*t**3*u + 4*t*u
    f_v = -p**2/(2*t**5*np.sqrt(v)) - 1
    f_p = 12*p**3 - 2*p*np.sqrt(v)/t**5 + 8*t**3*u**2
    f_t = 5*p**2*np.sqrt(v)/t**6 + 24*p*t**2*u**2 + 2*u**2

    return np.array([[f_u,f_v,f_p,f_t]])
12/6:
# Test input
u = -1
v = 1
p = 2
t = 1

# True value of gradient for test input
Df_true = np.array([-36,  -3, 100,  70])

# Check for correctness of result
assert AlmostEqual(Df_true, Df(u,v,p,t), 3)
12/7:
def f(z1,z2):
    return (z1-z2)*np.exp(z1)

def u(x1,x2):
    return np.array([[x1*x2],[x1**2 - x2**2]])
12/8:
# Part (a)
# Remember that Df is a row vector in this case.
def Df(z_1,z_2):

    Df_z1 = (z_1 - z_2)*np.exp(z_1) + np.exp(z_1)
    Df_z2 = -np.exp(z_1)
    return np.array([Df_z1, Df_z2])

# Part (b)
# Remember that Du is a matrix in this case.
def Du(x_1,x_2):

    Du_x1 = [x_2, x_1]
    Du_x2 = [2*x_1, -2*x_2]
    return np.array([Du_x1, Du_x2])

# Part (c)
# You should only use x_1,x_2 and the function u, Du and Df together
# with matrix multiplications.
def Dfou(x_1,x_2):
    ux = u(x_1, x_2)
    
    Df_ux = Df(ux[0, 0], ux[1, 0])
    Du_x = Du(x_1, x_2)
    
    return np.dot(Df_ux, Du_x)
12/9:
def DA2(t):

    A = np.array([[t**2, t+1], [t**3 + t + 3, 7]])
    dAdt = np.array([[2*t, 1], [3*t**2 + 1, 0]])
    dA2dt = np.dot(dAdt, A) + np.dot(A, dAdt)
    print(dA2dt)
    return dA2dt
12/10:
# Test input
t = 0

# True value of dA2dt for test input
DA2_true = np.array([[4,7],[7,4]])

# Checking for correctness of result
assert AlmostEqual(DA2_true, DA2(t), 3)
14/1:
import warnings
import numpy as np
import pandas as pd

import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot

%matplotlib inline
warnings.filterwarnings("ignore")
init_notebook_mode(connected=True)
14/2: !pip install plotly
14/3: !pip chart_studio
14/4:
import warnings
import numpy as np
import pandas as pd

import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot

%matplotlib inline
warnings.filterwarnings("ignore")
init_notebook_mode(connected=True)
14/5: !pip install chart_studio
14/6:
import warnings
import numpy as np
import pandas as pd

import chart_studio.plotly as py
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot

%matplotlib inline
warnings.filterwarnings("ignore")
init_notebook_mode(connected=True)
17/1:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
17/2: !pip install rasterio
17/3:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
17/4: !pip install seaborn
17/5:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
17/6:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rasterio.open(tif_file) as src:
    # Read the image data
    rgb_img = src.read()
17/7:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as src:
    # Read the image data
    rgb_img = src.read()
17/8: print(rgb_img.profile)
17/9:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as src:
    # Read the image data
    rgb_img = src
17/10: print(rgb_img.profile)
17/11:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as rgb_img:
17/12:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as rgb_img
17/13:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as src:
    
    rgb_img = src
17/14: print(rgb_img.profile)
17/15:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as src:
    
    rgb_img = src
    print(rgb_img.profile)
    print(rgb_img.shape)
17/16:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
with rio.open(tif_file) as src:
    # Plot the raster data
    show(src)
17/17:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
with rio.open(tif_file) as src:
    # Plot the raster data
    show(src)

img_array = np.array(src)



# Plotting the histogram for the grayscale image array

plt.figure(figsize=(10, 6))

plt.hist(img_array.ravel(), bins=256, range=(0, 256), fc='k', ec='k')

plt.title('Histogram of Grayscale Image Data')

plt.xlabel('Pixel Values')

plt.ylabel('Frequency')

plt.grid(True)

plt.show()
17/18:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
with rio.open(tif_file) as src:
    # Plot the raster data
    show(src)

img = Image.open('/mnt/data/plolt.png').convert('L')

img_array = np.array(img)



# Plotting the histogram for the grayscale image array

plt.figure(figsize=(10, 6))

plt.hist(img_array.ravel(), bins=256, range=(0, 256), fc='k', ec='k')

plt.title('Histogram of Grayscale Image Data')

plt.xlabel('Pixel Values')

plt.ylabel('Frequency')

plt.grid(True)

plt.show()
17/19:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
#with rio.open(tif_file) as src:
    # Plot the raster data
    #show(src)

#img = Image.open('/mnt/data/plolt.png').convert('L')

#img_array = np.array(img)


fig, (ax_img, ax_hist) = plt.subplots(1, 2, figsize=(14, 7))
show(rgb_img, ax=ax_img)
show_hist(rgb_img, ax=ax_hist, bins=50, lw=0.0, stacked=False, alpha=0.3, histtype='stepfilled')
plt.show()
17/20:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as src:
    
    rgb_img = src
    print(rgb_img.profile)
    print(rgb_img.shape)
    rgb_img = src.read()
17/21:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
#with rio.open(tif_file) as src:
    # Plot the raster data
    #show(src)

#img = Image.open('/mnt/data/plolt.png').convert('L')

#img_array = np.array(img)


fig, (ax_img, ax_hist) = plt.subplots(1, 2, figsize=(14, 7))
show(rgb_img, ax=ax_img)
show_hist(rgb_img, ax=ax_hist, bins=50, lw=0.0, stacked=False, alpha=0.3, histtype='stepfilled')
plt.show()
17/22:
# read csv file containing sample locations: data/ilk-3b-1024-10k-pnts.csv
# hint, you can use pandas functions

xydata = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
17/23: print(xydata.head())
17/24:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df






# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
17/25:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rasterio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Ensure coordinates are integers and within bounds
        x, y = int(row['X']), int(row['Y'])
        
        # Read the pixel value at each band
        # src.read() indexes bands starting from 1, and coordinates are (row, col)
        r = src.read(1)[y, x]
        g = src.read(2)[y, x]
        b = src.read(3)[y, x]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})






# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
17/26:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
17/27:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rasterio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Ensure coordinates are integers and within bounds
        x, y = int(row['X']), int(row['Y'])
        
        # Read the pixel value at each band
        # src.read() indexes bands starting from 1, and coordinates are (row, col)
        r = src.read(1)[y, x]
        g = src.read(2)[y, x]
        b = src.read(3)[y, x]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})






# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
17/28:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Ensure coordinates are integers and within bounds
        x, y = int(row['X']), int(row['Y'])
        
        # Read the pixel value at each band
        # src.read() indexes bands starting from 1, and coordinates are (row, col)
        r = src.read(1)[y, x]
        g = src.read(2)[y, x]
        b = src.read(3)[y, x]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})






# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
17/29:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Ensure coordinates are integers and within bounds
        x, y = int(row['X']), int(row['Y'])
        
        # Read the pixel value at each band
        # src.read() indexes bands starting from 1, and coordinates are (row, col)
        r = src.read(1)[y, x]
        g = src.read(2)[y, x]
        b = src.read(3)[y, x]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})






# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
17/30:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Convert geographic coordinates to pixel coordinates
        # `src.index` returns column (x) and row (y) indices in the raster for given spatial coordinates
        col, row = src.index(row['X'], row['Y'])

        # Read the pixel value at each band
        r = src.read(1)[row, col]
        g = src.read(2)[row, col]
        b = src.read(3)[row, col]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})








# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
17/31:
# First name the columns in the dataframe as R, G, B.
# Then display image data (r, g, b) for first 5 points

df = 
df.head()
17/32:
# First name the columns in the dataframe as R, G, B.
# Then display image data (r, g, b) for first 5 points


df.head()
17/33:
# compute mean and standard deviation of each band, 
# generate bargraphs, and show standard deviation on the bar plot

m = df.mean()
sd = df.std()
17/34:
print("Means:\n", m)
print("SD:\n", sd)
17/35:
# write code to plot the means and standard deviations here


colors = ['red', 'green', 'blue']
ax = means.plot(kind='bar', yerr=std_devs, color=colors, alpha=0.7, error_kw=dict(ecolor='gray', lw=1, capsize=5, capthick=1))
ax.set_ylabel('Pixel Value')
plt.title('Mean and Standard Deviation of RGB Bands')
plt.show()
17/36:
# write code to plot the means and standard deviations here


colors = ['red', 'green', 'blue']
ax = m.plot(kind='bar', yerr=sd, color=colors, alpha=0.7, error_kw=dict(ecolor='gray', lw=1, capsize=5, capthick=1))
ax.set_ylabel('Pixel Value')
plt.title('Mean and Standard Deviation of RGB Bands')
plt.show()
17/37:
# visualize rgb data as scatter plot; select bands 1 and 3
# note image attributes are called bands (e.g., red, green, blue bands)

plt.scatter(df['R'], df['B'], c='magenta', edgecolor='k', alpha=0.5)
plt.xlabel('R')
plt.ylabel('B')
plt.title('Scatter Plot of R and B')
plt.show()
17/38:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
KMeans(n_clusters=K, random_state=100)




# use y_km0 for storing predicted lables
y_km0 = y_km0 = km0.fit_predict(df[['R', 'G', 'B']])
17/39:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
km0 = KMeans(n_clusters=K, random_state=100)




# use y_km0 for storing predicted lables
y_km0 = y_km0 = km0.fit_predict(df[['R', 'G', 'B']])
17/40:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
km0 = KMeans(n_clusters=K, random_state=100)




# use y_km0 for storing predicted lables
y_km0 = km0.fit_predict(df[['R', 'G', 'B']])
17/41:
# write code to print number of samples in each cluster

df['cluster_label'] = y_km0

cluster_counts = df['cluster_label'].value_counts()
17/42:
# write code to print number of samples in each cluster

df['cluster_label'] = y_km0

cluster_counts = df['cluster_label'].value_counts()
# Print the counts for each cluster
print("Number of samples in each cluster:")
print(cluster_counts)
17/43:
# visualize clusters as scatter plot

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['R'], df['B'], c=df['cluster_label'], cmap='viridis', alpha=0.6, edgecolors='w')

# Add a color bar to the plot to show the cluster colors
plt.colorbar(scatter)

# Set the plot labels and title
plt.xlabel('Red Intensity')
plt.ylabel('Blue Intensity')
plt.title('Scatter Plot of Clusters')

# Show the plot
plt.show()
17/44:
# visualize clusters as scatter plot

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['R'], df['B'], c=df['cluster_label'], cmap='viridis', alpha=0.6, edgecolors='w')

# Add a color bar to the plot to show the cluster colors
plt.colorbar(scatter)

# Set the plot labels and title
plt.xlabel('Red Intensity')
plt.ylabel('Blue Intensity')
plt.title('Scatter Plot of Clusters')

# Show the plot
plt.show()
17/45:
# read full image data
# Open the raster file
with rio.open(raster_file_path) as src:
    # Read the full image data; this loads all bands
    img = src.read()
17/46:
# read full image data
# Open the raster file
with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
17/47:
# display image to check if you read the data correctly
show(img, cmap='viridis')
17/48:
# note that the img data you read is ndarray, verify it
type(img)
17/49:
# and its shape should be (3, 1024, 1024)
print(img.shape)
17/50:
# However, the clustering model prediction expects that the input to be r,g,b vectors
# So write code to convert image into rgb vectors

img_transposed = np.transpose(img, (1, 2, 0))



rgb_vec = img_transposed.reshape(-1, 3)
17/51:
# if you converted it correctly, you should get the shape of vector as (1048576, 3)
rgb_vec.shape
17/52:
# its good to verify that format of r, g, b, and actual values
# for example, first image pixel value should be (124, 129, 112)
rgb_vec[0]
17/53:
# predict label for each pixel (vector) by calling the predict method of KMeans() clustering

imgkm = kmeans.predict(rgb_vectors)
17/54:
# predict label for each pixel (vector) by calling the predict method of KMeans() clustering
km0.fit(rgb_vectors)

imgkm = km0.predict(rgb_vectors)
17/55:
# predict label for each pixel (vector) by calling the predict method of KMeans() clustering
km0.fit(rgb_vec)

imgkm = km0.predict(rgb_vec)
17/56:
# Check the shape of the output, should be same as 1024 * 1024 = 1048576
imgkm.shape
17/57:
# you need to convert the is 1-D r,g,b vector back to image of 1024,1024 (note that its a single band image)
imgkm_out = imgkm.reshape(1024, 1024)
17/58:
# check the shape
imgkm_out.shape
17/59:
# plot the clustered image
# Visualizing the clustered image
plt.figure(figsize=(8, 8))
plt.imshow(imgkm_out, cmap='viridis')  # Using 'viridis' colormap to visually distinguish the clusters
plt.colorbar()  # Show the color mapping for cluster labels
plt.title('Clustered Image Representation')
plt.show()
20/1:
# your algorith 1 clode goes here ...

def plot_clusters(data, labels, centers):
    plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='viridis', edgecolors='black')
    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.6)
    plt.show()

def anderson_darling_test(data, alpha=0.05):
    # Normalize the data
    data = (data - np.mean(data)) / np.std(data)
    result = anderson(data.flatten())
    return result.statistic, result.critical_values[2]  # Using 5% significance level

def g_means_like_algorithm(data, alpha=0.05):
    # Start with the overall mean as the initial center
    initial_center = np.mean(data, axis=0).reshape(1, -1)
    centers = KMeans(n_clusters=1, init=initial_center, n_init=1).fit(data).cluster_centers_
    
    while True:
        new_centers = []
        for center in centers:
            # Assign data points to the nearest center
            kmeans = KMeans(n_clusters=1, init=center.reshape(1, -1), n_init=1)
            kmeans.fit(data)
            assigned_data = data[kmeans.labels_ == 0]
            
            # Perform the statistical test
            statistic, critical_value = anderson_darling_test(assigned_data, alpha)
            if statistic > critical_value:
                # Split the cluster
                split_kmeans = KMeans(n_clusters=2, random_state=42).fit(assigned_data)
                new_centers.extend(split_kmeans.cluster_centers_)
            else:
                # Keep the center
                new_centers.append(center)
        
        new_centers = np.array(new_centers)
        if len(new_centers) == len(centers):
            break
        else:
            centers = new_centers
    
    return centers
20/2:
# print optimal number of clusters found with the algorithm
final_centers = g_means_like_algorithm(df, alpha=0.05)
20/3:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
20/4:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package
tif_file = "data/ilk-3b-1024.tif"

# Open the TIFF file
with rio.open(tif_file) as src:
    
    rgb_img = src
    print(rgb_img.profile)
    print(rgb_img.shape)
    rgb_img = src.read()
20/5: rgb_img.shape
20/6:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
#with rio.open(tif_file) as src:
    # Plot the raster data
    #show(src)

#img = Image.open('/mnt/data/plolt.png').convert('L')

#img_array = np.array(img)


fig, (ax_img, ax_hist) = plt.subplots(1, 2, figsize=(14, 7))
show(rgb_img, ax=ax_img)
show_hist(rgb_img, ax=ax_hist, bins=50, lw=0.0, stacked=False, alpha=0.3, histtype='stepfilled')
plt.show()
20/7:
# read csv file containing sample locations: data/ilk-3b-1024-10k-pnts.csv
# hint, you can use pandas functions

xydata = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
20/8: print(xydata.head())
20/9:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Convert geographic coordinates to pixel coordinates
        # `src.index` returns column (x) and row (y) indices in the raster for given spatial coordinates
        col, row = src.index(row['X'], row['Y'])

        # Read the pixel value at each band
        r = src.read(1)[row, col]
        g = src.read(2)[row, col]
        b = src.read(3)[row, col]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})








# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
20/10:
# First name the columns in the dataframe as R, G, B.
# Then display image data (r, g, b) for first 5 points

# already named them R, G, B

df.head()
20/11:
# compute mean and standard deviation of each band, 
# generate bargraphs, and show standard deviation on the bar plot

m = df.mean()
sd = df.std()
20/12:
print("Means:\n", m)
print("SD:\n", sd)
20/13:
# write code to plot the means and standard deviations here


colors = ['red', 'green', 'blue']
ax = m.plot(kind='bar', yerr=sd, color=colors, alpha=0.7, error_kw=dict(ecolor='gray', lw=1, capsize=5, capthick=1))
ax.set_ylabel('Pixel Value')
plt.title('Mean and Standard Deviation of RGB Bands')
plt.show()
20/14:
# visualize rgb data as scatter plot; select bands 1 and 3
# note image attributes are called bands (e.g., red, green, blue bands)

plt.scatter(df['R'], df['B'], c='magenta', edgecolor='k', alpha=0.5)
plt.xlabel('R')
plt.ylabel('B')
plt.title('Scatter Plot of R and B')
plt.show()
20/15:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
km0 = KMeans(n_clusters=K, random_state=100)




# use y_km0 for storing predicted lables
y_km0 = km0.fit_predict(df[['R', 'G', 'B']])
20/16:
# write code to print number of samples in each cluster

df['cluster_label'] = y_km0

cluster_counts = df['cluster_label'].value_counts()
# Print the counts for each cluster
print("Number of samples in each cluster:")
print(cluster_counts)
20/17:
# visualize clusters as scatter plot

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['R'], df['B'], c=df['cluster_label'], cmap='viridis', alpha=0.6, edgecolors='w')

# Add a color bar to the plot to show the cluster colors
plt.colorbar(scatter)

# Set the plot labels and title
plt.xlabel('Red Intensity')
plt.ylabel('Blue Intensity')
plt.title('Scatter Plot of Clusters')

# Show the plot
plt.show()
20/18:
# read full image data
# Open the raster file
with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
20/19:
# display image to check if you read the data correctly
show(img, cmap='viridis')
20/20:
# print optimal number of clusters found with the algorithm
final_centers = g_means_like_algorithm(df, alpha=0.05)
20/21:
# print optimal number of clusters found with the algorithm
final_centers = g_means_like_algorithm(df.to_numpy(), alpha=0.05)
20/22:
# your algorith 1 clode goes here ...
from scipy.stats import Anderson

def plot_clusters(data, labels, centers):
    plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='viridis', edgecolors='black')
    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.6)
    plt.show()

def anderson_darling_test(data, alpha=0.05):
    # Normalize the data
    data = (data - np.mean(data)) / np.std(data)
    result = anderson(data.flatten())
    return result.statistic, result.critical_values[2]  # Using 5% significance level

def g_means_like_algorithm(data, alpha=0.05):
    # Start with the overall mean as the initial center
    initial_center = np.mean(data, axis=0).reshape(1, -1)
    centers = KMeans(n_clusters=1, init=initial_center, n_init=1).fit(data).cluster_centers_
    
    while True:
        new_centers = []
        for center in centers:
            # Assign data points to the nearest center
            kmeans = KMeans(n_clusters=1, init=center.reshape(1, -1), n_init=1)
            kmeans.fit(data)
            assigned_data = data[kmeans.labels_ == 0]
            
            # Perform the statistical test
            statistic, critical_value = anderson_darling_test(assigned_data, alpha)
            if statistic > critical_value:
                # Split the cluster
                split_kmeans = KMeans(n_clusters=2, random_state=42).fit(assigned_data)
                new_centers.extend(split_kmeans.cluster_centers_)
            else:
                # Keep the center
                new_centers.append(center)
        
        new_centers = np.array(new_centers)
        if len(new_centers) == len(centers):
            break
        else:
            centers = new_centers
    
    return centers
20/23: !pip install scipy.stats
20/24:
# your algorith 1 clode goes here ...
from scipy.stats import Anderson

def plot_clusters(data, labels, centers):
    plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='viridis', edgecolors='black')
    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.6)
    plt.show()

def anderson_darling_test(data, alpha=0.05):
    # Normalize the data
    data = (data - np.mean(data)) / np.std(data)
    result = anderson(data.flatten())
    return result.statistic, result.critical_values[2]  # Using 5% significance level

def g_means_like_algorithm(data, alpha=0.05):
    # Start with the overall mean as the initial center
    initial_center = np.mean(data, axis=0).reshape(1, -1)
    centers = KMeans(n_clusters=1, init=initial_center, n_init=1).fit(data).cluster_centers_
    
    while True:
        new_centers = []
        for center in centers:
            # Assign data points to the nearest center
            kmeans = KMeans(n_clusters=1, init=center.reshape(1, -1), n_init=1)
            kmeans.fit(data)
            assigned_data = data[kmeans.labels_ == 0]
            
            # Perform the statistical test
            statistic, critical_value = anderson_darling_test(assigned_data, alpha)
            if statistic > critical_value:
                # Split the cluster
                split_kmeans = KMeans(n_clusters=2, random_state=42).fit(assigned_data)
                new_centers.extend(split_kmeans.cluster_centers_)
            else:
                # Keep the center
                new_centers.append(center)
        
        new_centers = np.array(new_centers)
        if len(new_centers) == len(centers):
            break
        else:
            centers = new_centers
    
    return centers
20/25: !pip install numpy scipy matplotlib sklearn
20/26:
# your algorith 1 clode goes here ...
from scipy.stats import Anderson

def plot_clusters(data, labels, centers):
    plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='viridis', edgecolors='black')
    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.6)
    plt.show()

def anderson_darling_test(data, alpha=0.05):
    # Normalize the data
    data = (data - np.mean(data)) / np.std(data)
    result = anderson(data.flatten())
    return result.statistic, result.critical_values[2]  # Using 5% significance level

def g_means_like_algorithm(data, alpha=0.05):
    # Start with the overall mean as the initial center
    initial_center = np.mean(data, axis=0).reshape(1, -1)
    centers = KMeans(n_clusters=1, init=initial_center, n_init=1).fit(data).cluster_centers_
    
    while True:
        new_centers = []
        for center in centers:
            # Assign data points to the nearest center
            kmeans = KMeans(n_clusters=1, init=center.reshape(1, -1), n_init=1)
            kmeans.fit(data)
            assigned_data = data[kmeans.labels_ == 0]
            
            # Perform the statistical test
            statistic, critical_value = anderson_darling_test(assigned_data, alpha)
            if statistic > critical_value:
                # Split the cluster
                split_kmeans = KMeans(n_clusters=2, random_state=42).fit(assigned_data)
                new_centers.extend(split_kmeans.cluster_centers_)
            else:
                # Keep the center
                new_centers.append(center)
        
        new_centers = np.array(new_centers)
        if len(new_centers) == len(centers):
            break
        else:
            centers = new_centers
    
    return centers
20/27:
# your algorith 1 clode goes here ...
from scipy.stats import anderson

def plot_clusters(data, labels, centers):
    plt.scatter(data[:, 0], data[:, 1], c=labels, s=50, cmap='viridis', edgecolors='black')
    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.6)
    plt.show()

def anderson_darling_test(data, alpha=0.05):
    # Normalize the data
    data = (data - np.mean(data)) / np.std(data)
    result = anderson(data.flatten())
    return result.statistic, result.critical_values[2]  # Using 5% significance level

def g_means_like_algorithm(data, alpha=0.05):
    # Start with the overall mean as the initial center
    initial_center = np.mean(data, axis=0).reshape(1, -1)
    centers = KMeans(n_clusters=1, init=initial_center, n_init=1).fit(data).cluster_centers_
    
    while True:
        new_centers = []
        for center in centers:
            # Assign data points to the nearest center
            kmeans = KMeans(n_clusters=1, init=center.reshape(1, -1), n_init=1)
            kmeans.fit(data)
            assigned_data = data[kmeans.labels_ == 0]
            
            # Perform the statistical test
            statistic, critical_value = anderson_darling_test(assigned_data, alpha)
            if statistic > critical_value:
                # Split the cluster
                split_kmeans = KMeans(n_clusters=2, random_state=42).fit(assigned_data)
                new_centers.extend(split_kmeans.cluster_centers_)
            else:
                # Keep the center
                new_centers.append(center)
        
        new_centers = np.array(new_centers)
        if len(new_centers) == len(centers):
            break
        else:
            centers = new_centers
    
    return centers
20/28:
# print optimal number of clusters found with the algorithm
final_centers = g_means_like_algorithm(df.to_numpy(), alpha=0.05)
21/1:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
22/1:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
22/2:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package


# Open the TIFF file
rgb_img = rio.open("data/ilk-3b-1024.tif")
22/3: print(rgb_img.profile)
22/4: rgb_img.shape
22/5:
# read csv file containing sample locations: data/ilk-3b-1024-10k-pnts.csv
# hint, you can use pandas functions

xydata = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
22/6: print(xydata.head())
22/7:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Convert geographic coordinates to pixel coordinates
        # `src.index` returns column (x) and row (y) indices in the raster for given spatial coordinates
        col, row = src.index(row['X'], row['Y'])

        # Read the pixel value at each band
        r = src.read(1)[row, col]
        g = src.read(2)[row, col]
        b = src.read(3)[row, col]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})








# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
22/8:
# First name the columns in the dataframe as R, G, B.
# Then display image data (r, g, b) for first 5 points

# already named them R, G, B

df.head()
22/9:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
km0 = KMeans(n_clusters=K, random_state=100)




# use y_km0 for storing predicted lables
y_km0 = km0.fit_predict(df[['R', 'G', 'B']])
22/10:
# your algorithm 1 clode goes here ...
import numpy as np

# Define a class for G-means clustering
class GMeansClustering:
    """
    G-means clustering implementation.

    Arguments:
    - min_samples: Minimum number of samples required for cluster splitting.
    - max_depth: Maximum depth of recursion for cluster splitting.
    - random_state: Seed for random number generation.
    - alpha: Significance level for normality testing.
    """

    # Constructor to initialize the GMeansClustering instance
    def __init__(self, min_samples=1, max_depth=10, random_state=None, alpha=0.01):
        """
        Initializing the GMeansClustering instance.
        """
        # Initializing parameters
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.random_state = random_state
        # Validating alpha
        if alpha not in [0.15, 0.10, 0.05, 0.025, 0.01]:
            raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")
        self.alpha = alpha
        # List for storing stopping criteria
        self.stopping_criteria = []
        # Initializing attributes
        self.data_index = None
        self.data = None
        self.labels_ = None
        self.nearest_centroid_classifier = None

    # Method to check if values follow a normal distribution
    def _check_normal_distribution(self, values):
        """
        Check if values follow a normal distribution using the Anderson-Darling test.
        """
        # Performing Anderson-Darling test
        result = anderson(values)
        statistic_value = result.statistic
        crit_val_index = np.where(result.significance_level == self.alpha * 100)[0]
        critical_value = result.critical_values[crit_val_index]
        # Checking if the statistic value is less than the critical value
        return statistic_value <= critical_value

    # Method to recursively split clusters using k-means
    def _split_clusters(self, data, depth, index):
        """
        Recursively split clusters using k-means until max depth or normal distribution is achieved.
        """
        depth += 1
        if depth == self.max_depth:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('max_depth')
            return
        kmeans = KMeans(n_clusters=2)
        kmeans.fit(data)
        centroids = kmeans.cluster_centers_
        vector = centroids[0] - centroids[1]
        x_prime = scale(data.dot(vector) / (vector.dot(vector)))
        is_normal_dist = self._check_normal_distribution(x_prime)
        if is_normal_dist:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('normal_distribution')
            return
        labels = set(kmeans.labels_)
        for label in labels:
            current_data = data[kmeans.labels_ == label]
            if current_data.shape[0] <= self.min_samples:
                self.data_index[index[:, 0]] = index
                self.stopping_criteria.append('min_samples')
                return
            current_index = index[kmeans.labels_ == label]
            current_index[:, 1] = np.random.randint(0, 9999999999, dtype=np.int64)  # Changed dtype to np.int64
            self._split_clusters(data=current_data, depth=depth, index=current_index)

    # Method to fit data to the G-means clustering model
    def fit(self, data):
        """
        Fit data to the G-means clustering model.
        """
        self.data = data
        data_index = np.array([(i, False) for i in range(data.shape[0])], dtype=np.int64)  # Changed dtype to np.int64
        self.data_index = data_index
        self._split_clusters(data=data, depth=0, index=data_index)
        self.labels_ = self._encode_labels(self.data_index[:, 1])
        self.nearest_centroid_classifier = NearestCentroid().fit(self.data, self.labels_)
        return self

    # Method to encode original labels into sequential integers
    def _encode_labels(self, original_labels):
        """
        Encode original labels into sequential integers.
        """
        label_encoder = LabelEncoder()
        label_encoder.fit(original_labels)
        return label_encoder.transform(original_labels)

    # Method to predict cluster labels for input data
    def predict(self, X):
        """
        Predict cluster labels for input data.
        """
        return self.nearest_centroid_classifier.predict(X)
22/11:
# print optimal number of clusters found with the algorithm
# print optimal number of clusters found with the algorithm
gm0 = GMeansClustering().fit(df.to_numpy())
y_gm0 = gm0.predict(df.to_numpy())
print("Optimal number of clusters:", len(np.unique(y_gm0)))
22/12:
# your algorithm 1 clode goes here ...
import numpy as np
from collections import Counter
from scipy.stats import anderson
from pprint import pprint
from sklearn.preprocessing import scale
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import LabelEncoder

# Define a class for G-means clustering
class GMeansClustering:
    """
    G-means clustering implementation.

    Arguments:
    - min_samples: Minimum number of samples required for cluster splitting.
    - max_depth: Maximum depth of recursion for cluster splitting.
    - random_state: Seed for random number generation.
    - alpha: Significance level for normality testing.
    """

    # Constructor to initialize the GMeansClustering instance
    def __init__(self, min_samples=1, max_depth=10, random_state=None, alpha=0.01):
        """
        Initializing the GMeansClustering instance.
        """
        # Initializing parameters
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.random_state = random_state
        # Validating alpha
        if alpha not in [0.15, 0.10, 0.05, 0.025, 0.01]:
            raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")
        self.alpha = alpha
        # List for storing stopping criteria
        self.stopping_criteria = []
        # Initializing attributes
        self.data_index = None
        self.data = None
        self.labels_ = None
        self.nearest_centroid_classifier = None

    # Method to check if values follow a normal distribution
    def _check_normal_distribution(self, values):
        """
        Check if values follow a normal distribution using the Anderson-Darling test.
        """
        # Performing Anderson-Darling test
        result = anderson(values)
        statistic_value = result.statistic
        crit_val_index = np.where(result.significance_level == self.alpha * 100)[0]
        critical_value = result.critical_values[crit_val_index]
        # Checking if the statistic value is less than the critical value
        return statistic_value <= critical_value

    # Method to recursively split clusters using k-means
    def _split_clusters(self, data, depth, index):
        """
        Recursively split clusters using k-means until max depth or normal distribution is achieved.
        """
        depth += 1
        if depth == self.max_depth:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('max_depth')
            return
        kmeans = KMeans(n_clusters=2)
        kmeans.fit(data)
        centroids = kmeans.cluster_centers_
        vector = centroids[0] - centroids[1]
        x_prime = scale(data.dot(vector) / (vector.dot(vector)))
        is_normal_dist = self._check_normal_distribution(x_prime)
        if is_normal_dist:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('normal_distribution')
            return
        labels = set(kmeans.labels_)
        for label in labels:
            current_data = data[kmeans.labels_ == label]
            if current_data.shape[0] <= self.min_samples:
                self.data_index[index[:, 0]] = index
                self.stopping_criteria.append('min_samples')
                return
            current_index = index[kmeans.labels_ == label]
            current_index[:, 1] = np.random.randint(0, 9999999999, dtype=np.int64)  # Changed dtype to np.int64
            self._split_clusters(data=current_data, depth=depth, index=current_index)

    # Method to fit data to the G-means clustering model
    def fit(self, data):
        """
        Fit data to the G-means clustering model.
        """
        self.data = data
        data_index = np.array([(i, False) for i in range(data.shape[0])], dtype=np.int64)  # Changed dtype to np.int64
        self.data_index = data_index
        self._split_clusters(data=data, depth=0, index=data_index)
        self.labels_ = self._encode_labels(self.data_index[:, 1])
        self.nearest_centroid_classifier = NearestCentroid().fit(self.data, self.labels_)
        return self

    # Method to encode original labels into sequential integers
    def _encode_labels(self, original_labels):
        """
        Encode original labels into sequential integers.
        """
        label_encoder = LabelEncoder()
        label_encoder.fit(original_labels)
        return label_encoder.transform(original_labels)

    # Method to predict cluster labels for input data
    def predict(self, X):
        """
        Predict cluster labels for input data.
        """
        return self.nearest_centroid_classifier.predict(X)
22/13:
# print optimal number of clusters found with the algorithm
# print optimal number of clusters found with the algorithm
gm0 = GMeansClustering().fit(df.to_numpy())
y_gm0 = gm0.predict(df.to_numpy())
print("Optimal number of clusters:", len(np.unique(y_gm0)))
22/14:
# print number of samples in each cluster
gmeans_df = pd.DataFrame(y_gm0,columns=["gmeans"])
vals = gmeans_df["gmeans"].value_counts()
print(vals.sort_index())
22/15:
# generate scatter plot and color using cluster labels
plt.figure(figsize=(6,4))
plt.scatter(df["R"],df["B"],c=y_gm0)
plt.colorbar()
plt.xlabel("R")
plt.ylabel("B")
plt.show()
22/16:
# Apply the final model to predict labels for each pixel in the image, 
# that is, generate a clustered image
print("Shape of the RGB vector:", rgb_vec.shape)

# Cluster the RGB values using the G-means clustering model
cluster_labels = gm0.predict(rgb_vec)

# Checking the shape of the clustered image, which should be equivalent to 1024 * 1024 = 1048576
print("Shape of the clustered image:", cluster_labels.shape)

# Reconstructing the single-band image from the 1D cluster labels to a 1024x1024 image
image_clustered = np.zeros([1024, 1024])
for i in range(1024):
    for j in range(1024):
        index = i * 1024 + j
        image_clustered[i][j] = cluster_labels[index]

# Verifying the shape of the reconstructed image
print("Shape of the reconstructed image:", image_clustered.shape)
22/17:
# However, the clustering model prediction expects that the input to be r,g,b vectors
# So write code to convert image into rgb vectors

img_transposed = np.transpose(img, (1, 2, 0))



rgb_vec = img_transposed.reshape(-1, 3)
22/18:
# read full image data
# Open the raster file
with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
22/19:
# write code to print number of samples in each cluster

df['cluster_label'] = y_km0

cluster_counts = df['cluster_label'].value_counts()
# Print the counts for each cluster
print("Number of samples in each cluster:")
print(cluster_counts)
22/20:
# visualize clusters as scatter plot

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['R'], df['B'], c=df['cluster_label'], cmap='viridis', alpha=0.6, edgecolors='w')

# Add a color bar to the plot to show the cluster colors
plt.colorbar(scatter)

# Set the plot labels and title
plt.xlabel('Red Intensity')
plt.ylabel('Blue Intensity')
plt.title('Scatter Plot of Clusters')

# Show the plot
plt.show()
22/21:
# read full image data
# Open the raster file
with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
22/22:
# read full image data
# Open the raster file
tiff_file = "data/ilk-3b-1024.tif"

with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
22/23:
# read full image data
# Open the raster file
tiff_file = 'data/ilk-3b-1024.tif'

with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
22/24:
# read full image data
# Open the raster file
tif_file = 'data/ilk-3b-1024.tif'

with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
22/25:
# display image to check if you read the data correctly
show(img, cmap='viridis')
22/26:
# note that the img data you read is ndarray, verify it
type(img)
22/27:
# and its shape should be (3, 1024, 1024)
print(img.shape)
22/28:
# However, the clustering model prediction expects that the input to be r,g,b vectors
# So write code to convert image into rgb vectors

img_transposed = np.transpose(img, (1, 2, 0))



rgb_vec = img_transposed.reshape(-1, 3)
22/29:
# Apply the final model to predict labels for each pixel in the image, 
# that is, generate a clustered image
print("Shape of the RGB vector:", rgb_vec.shape)

# Cluster the RGB values using the G-means clustering model
cluster_labels = gm0.predict(rgb_vec)

# Checking the shape of the clustered image, which should be equivalent to 1024 * 1024 = 1048576
print("Shape of the clustered image:", cluster_labels.shape)

# Reconstructing the single-band image from the 1D cluster labels to a 1024x1024 image
image_clustered = np.zeros([1024, 1024])
for i in range(1024):
    for j in range(1024):
        index = i * 1024 + j
        image_clustered[i][j] = cluster_labels[index]

# Verifying the shape of the reconstructed image
print("Shape of the reconstructed image:", image_clustered.shape)
22/30:
# display the clustered image here.
show(image_clustered)
22/31:
# your algorithm 1 clode goes here ...
import numpy as np
from collections import Counter
from scipy.stats import anderson
from pprint import pprint
from sklearn.preprocessing import scale
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import LabelEncoder

# Define a class for G-means clustering
class GMeansClustering:
    """
    G-means clustering implementation.

    Arguments:
    - min_samples: Minimum number of samples required for cluster splitting.
    - max_depth: Maximum depth of recursion for cluster splitting.
    - random_state: Seed for random number generation.
    - alpha: Significance level for normality testing.
    """

    # Constructor to initialize the GMeansClustering instance
    def __init__(self, min_samples=1, max_depth=10, random_state=None, alpha=0.01):
        """
        Initializing the GMeansClustering instance.
        """
        # Initializing parameters
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.random_state = random_state
        # Validating alpha
        if alpha not in [0.15, 0.10, 0.05, 0.025, 0.01]:
            raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")
        self.alpha = alpha
        # List for storing stopping criteria
        self.stopping_criteria = []
        # Initializing attributes
        self.data_index = None
        self.data = None
        self.labels_ = None
        self.nearest_centroid_classifier = None

    # Method to check if values follow a normal distribution
    def _check_normal_distribution(self, values):
        """
        Check if values follow a normal distribution using the Anderson-Darling test.
        """
        # Performing Anderson-Darling test
        result = anderson(values)
        statistic_value = result.statistic
        crit_val_index = np.where(result.significance_level == self.alpha * 100)[0]
        critical_value = result.critical_values[crit_val_index]
        # Checking if the statistic value is less than the critical value
        return statistic_value <= critical_value

    # Method to recursively split clusters using k-means
    def _split_clusters(self, data, depth, index):
        """
        Recursively split clusters using k-means until max depth or normal distribution is achieved.
        """
        depth += 1
        if depth == self.max_depth:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('max_depth')
            return
        kmeans = KMeans(n_clusters=2)
        kmeans.fit(data)
        centroids = kmeans.cluster_centers_
        vector = centroids[0] - centroids[1]
        x_prime = scale(data.dot(vector) / (vector.dot(vector)))
        is_normal_dist = self._check_normal_distribution(x_prime)
        if is_normal_dist:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('normal_distribution')
            return
        labels = set(kmeans.labels_)
        for label in labels:
            current_data = data[kmeans.labels_ == label]
            if current_data.shape[0] <= self.min_samples:
                self.data_index[index[:, 0]] = index
                self.stopping_criteria.append('min_samples')
                return
            current_index = index[kmeans.labels_ == label]
            current_index[:, 1] = np.random.randint(0, 9999999999, dtype=np.int64)  # Changed dtype to np.int64
            self._split_clusters(data=current_data, depth=depth, index=current_index)

    # Method to fit data to the G-means clustering model
    def fit(self, data):
        """
        Fit data to the G-means clustering model.
        """
        self.data = data
        data_index = np.array([(i, False) for i in range(data.shape[0])], dtype=np.int64)  # Changed dtype to np.int64
        self.data_index = data_index
        self._split_clusters(data=data, depth=0, index=data_index)
        self.labels_ = self._encode_labels(self.data_index[:, 1])
        self.nearest_centroid_classifier = NearestCentroid().fit(self.data, self.labels_)
        return self

    # Method to encode original labels into sequential integers
    def _encode_labels(self, original_labels):
        """
        Encode original labels into sequential integers.
        """
        label_encoder = LabelEncoder()
        label_encoder.fit(original_labels)
        return label_encoder.transform(original_labels)

    # Method to predict cluster labels for input data
    def predict(self, X):
        """
        Predict cluster labels for input data.
        """
        return self.nearest_centroid_classifier.predict(X)
22/32:
# print optimal number of clusters found with the algorithm
# print optimal number of clusters found with the algorithm
gm0 = GMeansClustering().fit(df.to_numpy())
y_gm0 = gm0.predict(df.to_numpy())
print("Optimal number of clusters:", len(np.unique(y_gm0)))
22/33:
# your algorithm 1 clode goes here ...
import numpy as np
from collections import Counter
from scipy.stats import anderson
from pprint import pprint
from sklearn.preprocessing import scale
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import LabelEncoder

MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

# Define functions
def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, data_indices):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return data_indices

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return data_indices

    for label in np.unique(kmeans.labels_):
        sub_cluster_data = cluster_data[kmeans.labels_ == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        sub_data_indices = recursively_split_clusters(sub_cluster_data, current_depth, data_indices)
    return sub_data_indices

def fit_g_means(data):
    validate_alpha(ALPHA)
    data_indices = np.zeros((data.shape[0], 2), dtype=np.int64)
    final_indices = recursively_split_clusters(data, 0, data_indices)
    label_encoder = LabelEncoder()
    final_labels = label_encoder.fit_transform(final_indices[:, 1])
    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)
22/34:
# print optimal number of clusters found with the algorithm


g_means_model, cluster_labels = fit_g_means(df.to_numpy())
predicted_labels = predict_clusters(g_means_model, df.to_numpy())
optimal_clusters = len(np.unique(predicted_labels))

print("Optimal number of clusters:", optimal_clusters)
22/35:
# your algorithm 1 clode goes here ...
import numpy as np
from collections import Counter
from scipy.stats import anderson
from pprint import pprint
from sklearn.preprocessing import scale
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import LabelEncoder

# Constants and Configurations
MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, data_indices):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return data_indices

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return data_indices

    for label in np.unique(kmeans.labels_):
        sub_cluster_data = cluster_data[kmeans.labels_ == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        data_indices = recursively_split_clusters(sub_cluster_data, current_depth, data_indices)

    return data_indices

def fit_g_means(data):
    validate_alpha(ALPHA)
    data_indices = np.zeros((data.shape[0], 2), dtype=np.int64)
    final_indices = recursively_split_clusters(data, 0, data_indices)
    label_encoder = LabelEncoder()
    final_labels = label_encoder.fit_transform(final_indices[:, 1])
    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)
22/36:
# your algorithm 1 clode goes here ...
import numpy as np
from collections import Counter
from scipy.stats import anderson
from pprint import pprint
from sklearn.preprocessing import scale
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Constants and Configurations
MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, data_indices):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return data_indices

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return data_indices

    for label in np.unique(kmeans.labels_):
        sub_cluster_data = cluster_data[kmeans.labels_ == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        data_indices = recursively_split_clusters(sub_cluster_data, current_depth, data_indices)

    return data_indices

def fit_g_means(data):
    validate_alpha(ALPHA)
    data_indices = np.zeros((data.shape[0], 2), dtype=np.int64)
    final_indices = recursively_split_clusters(data, 0, data_indices)
    label_encoder = LabelEncoder()
    final_labels = label_encoder.fit_transform(final_indices[:, 1])
    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)
22/37:
# print optimal number of clusters found with the algorithm


g_means_model, cluster_labels = fit_g_means(df.to_numpy())
predicted_labels = predict_clusters(g_means_model, df.to_numpy())
optimal_clusters = len(np.unique(predicted_labels))

print("Optimal number of clusters:", optimal_clusters)
22/38:
# your algorithm 1 clode goes here ...
import numpy as np
from sklearn.cluster import KMeans, NearestCentroid
from sklearn.preprocessing import scale, LabelEncoder
from scipy.stats import anderson
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Constants and Configurations
MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, cluster_indices):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return cluster_indices, [0] * len(cluster_data)  # No splits occurred

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return cluster_indices, [0] * len(cluster_data)  # Data follows normal distribution

    labels = kmeans.labels_
    label_counter = 0
    for label in np.unique(labels):
        sub_cluster_data = cluster_data[labels == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        sub_cluster_indices, sub_labels = recursively_split_clusters(sub_cluster_data, current_depth, cluster_indices[labels == label])
        cluster_indices[labels == label] = sub_cluster_indices
        labels[labels == label] = sub_labels + label_counter
        label_counter += max(sub_labels) + 1  # Adjust label counter for uniqueness across recursive calls

    return cluster_indices, labels

def fit_g_means(data):
    validate_alpha(ALPHA)
    initial_indices = np.arange(len(data))
    data_indices, labels = recursively_split_clusters(data, 0, initial_indices)
    if len(np.unique(labels)) < 2:
        print("Not enough splits to form multiple clusters.")
        return None, labels

    label_encoder = LabelEncoder()
    final_labels = label_encoder.fit_transform(labels)
    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)

# Assume 'df' is your DataFrame
data = df.to_numpy()  # Converting DataFrame to NumPy array
g_means_model, cluster_labels = fit_g_means(data)
if g_means_model:
    predicted_labels = predict_clusters(g_means_model, data)
    optimal_clusters = len(np.unique(predicted_labels))
    print("Optimal number of clusters:", optimal_clusters)
else:
    print("Failed to determine optimal clusters due to insufficient splitting.")
22/39:
# your algorithm 1 clode goes here ...
import numpy as np
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import scale, LabelEncoder
from scipy.stats import anderson
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Constants and Configurations
MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, cluster_indices):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return cluster_indices, [0] * len(cluster_data)  # No splits occurred

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return cluster_indices, [0] * len(cluster_data)  # Data follows normal distribution

    labels = kmeans.labels_
    label_counter = 0
    for label in np.unique(labels):
        sub_cluster_data = cluster_data[labels == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        sub_cluster_indices, sub_labels = recursively_split_clusters(sub_cluster_data, current_depth, cluster_indices[labels == label])
        cluster_indices[labels == label] = sub_cluster_indices
        labels[labels == label] = sub_labels + label_counter
        label_counter += max(sub_labels) + 1  # Adjust label counter for uniqueness across recursive calls

    return cluster_indices, labels

def fit_g_means(data):
    validate_alpha(ALPHA)
    initial_indices = np.arange(len(data))
    data_indices, labels = recursively_split_clusters(data, 0, initial_indices)
    if len(np.unique(labels)) < 2:
        print("Not enough splits to form multiple clusters.")
        return None, labels

    label_encoder = LabelEncoder()
    final_labels = label_encoder.fit_transform(labels)
    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)

# Assume 'df' is your DataFrame
data = df.to_numpy()  # Converting DataFrame to NumPy array
g_means_model, cluster_labels = fit_g_means(data)
if g_means_model:
    predicted_labels = predict_clusters(g_means_model, data)
    optimal_clusters = len(np.unique(predicted_labels))
    print("Optimal number of clusters:", optimal_clusters)
else:
    print("Failed to determine optimal clusters due to insufficient splitting.")
22/40:
# your algorithm 1 clode goes here ...
import numpy as np
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import scale, LabelEncoder
from scipy.stats import anderson
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Constants and Configurations
MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, cluster_labels):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return cluster_labels

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return cluster_labels

    labels = kmeans.labels_
    unique_labels = np.unique(labels)
    label_counter = np.max(cluster_labels) + 1  # Ensure unique labels across recursive calls

    for label in unique_labels:
        sub_cluster_data = cluster_data[labels == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        sub_labels = np.full(sub_cluster_data.shape[0], label_counter, dtype=int)
        updated_sub_labels = recursively_split_clusters(sub_cluster_data, current_depth, sub_labels)
        cluster_labels[labels == label] = updated_sub_labels
        label_counter = np.max(cluster_labels) + 1  # Update label_counter for next unique label

    return cluster_labels

def fit_g_means(data):
    validate_alpha(ALPHA)
    initial_labels = np.zeros(data.shape[0], dtype=int)
    final_labels = recursively_split_clusters(data, 0, initial_labels)
    if len(np.unique(final_labels)) < 2:
        print("Not enough splits to form multiple clusters.")
        return None, final_labels

    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)
22/41:
# print optimal number of clusters found with the algorithm


g_means_model, cluster_labels = fit_g_means(df.to_numpy())
predicted_labels = predict_clusters(g_means_model, df.to_numpy())
optimal_clusters = len(np.unique(predicted_labels))

print("Optimal number of clusters:", optimal_clusters)
22/42:
# print optimal number of clusters found with the algorithm


g_means_model, cluster_labels = fit_g_means(df.to_numpy())
predicted_labels = predict_clusters(g_means_model, df.to_numpy())
optimal_clusters = len(np.unique(predicted_labels))

print("Optimal number of clusters:", optimal_clusters)
22/43:
cluster_counts = np.bincount(cluster_labels)
    print("Number of samples in each cluster:")
    for i, count in enumerate(cluster_counts):
        print(f"Cluster {i}: {count} samples")
#print(vals.sort_index())
22/44:
cluster_counts = np.bincount(cluster_labels)
print("Number of samples in each cluster:")
for i, count in enumerate(cluster_counts):
        print(f"Cluster {i}: {count} samples")
#print(vals.sort_index())
22/45:
# print optimal number of clusters found with the algorithm


g_means_model, cluster_labels = fit_g_means(df.to_numpy())
predicted_labels = predict_clusters(g_means_model, df.to_numpy())
optimal_clusters = len(np.unique(predicted_labels))

print("Optimal number of clusters:", optimal_clusters)
22/46:
cluster_counts = np.bincount(optimal_clusters)
print("Number of samples in each cluster:")
for i, count in enumerate(cluster_counts):
        print(f"Cluster {i}: {count} samples")
#print(vals.sort_index())
22/47:
cluster_counts = np.bincount(optimal_clusters)
print("Number of samples in each cluster:")
for i, count in enumerate(cluster_counts):
        print(f"Cluster {i}: {count} samples")
#print(vals.sort_index())
22/48: type(cluster_labels)
22/49: cluster_labels
22/50: unique_clusters = np.unique(predicted_labels)
22/51:
for i, count in enumerate(cluster_counts):
        print(f"Cluster {i}: {count} samples")
22/52:
for i, count in enumerate(unique_clusters):
        print(f"Cluster {i}: {count} samples")
23/1: print(y_gm0)
23/2:
# Import necessary libraries
import numpy as np

# Define a class for G-means clustering
class GMeansClustering:
    """
    G-means clustering implementation.

    Arguments:
    - min_samples: Minimum number of samples required for cluster splitting.
    - max_depth: Maximum depth of recursion for cluster splitting.
    - random_state: Seed for random number generation.
    - alpha: Significance level for normality testing.
    """

    # Constructor to initialize the GMeansClustering instance
    def __init__(self, min_samples=1, max_depth=10, random_state=None, alpha=0.01):
        """
        Initializing the GMeansClustering instance.
        """
        # Initializing parameters
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.random_state = random_state
        # Validating alpha
        if alpha not in [0.15, 0.10, 0.05, 0.025, 0.01]:
            raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")
        self.alpha = alpha
        # List for storing stopping criteria
        self.stopping_criteria = []
        # Initializing attributes
        self.data_index = None
        self.data = None
        self.labels_ = None
        self.nearest_centroid_classifier = None

    # Method to check if values follow a normal distribution
    def _check_normal_distribution(self, values):
        """
        Check if values follow a normal distribution using the Anderson-Darling test.
        """
        # Performing Anderson-Darling test
        result = anderson(values)
        statistic_value = result.statistic
        crit_val_index = np.where(result.significance_level == self.alpha * 100)[0]
        critical_value = result.critical_values[crit_val_index]
        # Checking if the statistic value is less than the critical value
        return statistic_value <= critical_value

    # Method to recursively split clusters using k-means
    def _split_clusters(self, data, depth, index):
        """
        Recursively split clusters using k-means until max depth or normal distribution is achieved.
        """
        depth += 1
        if depth == self.max_depth:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('max_depth')
            return
        kmeans = KMeans(n_clusters=2)
        kmeans.fit(data)
        centroids = kmeans.cluster_centers_
        vector = centroids[0] - centroids[1]
        x_prime = scale(data.dot(vector) / (vector.dot(vector)))
        is_normal_dist = self._check_normal_distribution(x_prime)
        if is_normal_dist:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('normal_distribution')
            return
        labels = set(kmeans.labels_)
        for label in labels:
            current_data = data[kmeans.labels_ == label]
            if current_data.shape[0] <= self.min_samples:
                self.data_index[index[:, 0]] = index
                self.stopping_criteria.append('min_samples')
                return
            current_index = index[kmeans.labels_ == label]
            current_index[:, 1] = np.random.randint(0, 9999999999, dtype=np.int64)  # Changed dtype to np.int64
            self._split_clusters(data=current_data, depth=depth, index=current_index)

    # Method to fit data to the G-means clustering model
    def fit(self, data):
        """
        Fit data to the G-means clustering model.
        """
        self.data = data
        data_index = np.array([(i, False) for i in range(data.shape[0])], dtype=np.int64)  # Changed dtype to np.int64
        self.data_index = data_index
        self._split_clusters(data=data, depth=0, index=data_index)
        self.labels_ = self._encode_labels(self.data_index[:, 1])
        self.nearest_centroid_classifier = NearestCentroid().fit(self.data, self.labels_)
        return self

    # Method to encode original labels into sequential integers
    def _encode_labels(self, original_labels):
        """
        Encode original labels into sequential integers.
        """
        label_encoder = LabelEncoder()
        label_encoder.fit(original_labels)
        return label_encoder.transform(original_labels)

    # Method to predict cluster labels for input data
    def predict(self, X):
        """
        Predict cluster labels for input data.
        """
        return self.nearest_centroid_classifier.predict(X)
23/3:
# print optimal number of clusters found with the algorithm
gm0 = GMeansClustering().fit(df.to_numpy())
y_gm0 = gm0.predict(df.to_numpy())
print("Optimal number of clusters:", len(np.unique(y_gm0)))
23/4:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
23/5:
# if you need additional libraries, include them here
# make to include all your additional liberies here and not any other cells
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
from collections import Counter
from scipy.stats import anderson
from pprint import pprint
from sklearn.preprocessing import scale
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import LabelEncoder
23/6:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package

rgb_img = rio.open("data/ilk-3b-1024.tif")
23/7:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package

rgb_img = rio.open("data/ilk-3b-1024.tif")
23/8: print(rgb_img.profile)
23/9:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc

fig, (axrgb, axhist) = plt.subplots(1, 2, figsize=(12,5))
show(rgb_img, cmap='viridis', transform=rgb_img.transform, ax=axrgb)
show_hist(rgb_img, bins=50, histtype='stepfilled',
          lw=0.0, stacked=False, alpha=0.3, ax=axhist)
axhist.legend(fontsize='x-small')
plt.show()
23/10:
# read csv file containing sample locations: data/ilk-3b-1024-10k-pnts.csv
# hint, you can use pandas functions

xydata = pd.read_csv("data/ilk-3b-1024-10k-pnts.csv")
23/11: print(xydata.head())
23/12:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df


transformer = rio.transform.AffineTransformer(rgb_img.transform)
red_band = rgb_img.read(1)
green_band = rgb_img.read(2)
blue_band = rgb_img.read(3)
id = 1
row, col = transformer.rowcol(xydata.iloc[id]["X"], xydata.iloc[id]["Y"])
print(xydata.iloc[id]["X"], xydata.iloc[id]["Y"], [red_band[row][col], green_band[row][col], blue_band[row][col]])
print(xydata.iloc[id]["X"], xydata.iloc[id]["Y"], [red_band[col][row], green_band[col][row], blue_band[col][row]])


assert (xydata.id == xydata.index).all(), "Index does not match id column"
d = []
transformer = rio.transform.AffineTransformer(rgb_img.transform)
red_band = rgb_img.read(1)
green_band = rgb_img.read(2)
blue_band = rgb_img.read(3)
for id in xydata.index:
    row, col = transformer.rowcol(xydata.iloc[id]["X"], xydata.iloc[id]["Y"])
    d.append([red_band[row][col], green_band[row][col], blue_band[row][col]])
23/13:
# First name the columns in the dataframe as R, G, B.
# Then display image data (r, g, b) for first 5 points

df = pd.DataFrame(d, columns =["R","G","B"])
df.head()
23/14:
# compute mean and standard deviation of each band,
# generate bargraphs, and show standard deviation on the bar plot

m = df.mean()
sd = df.std()
23/15:
print("Means:\n", m)
print("SD:\n", sd)
23/16:
# write code to plot the means and standard deviations here
plt.figure(figsize=(4.5,3))
plt.bar(m.index, m.values, yerr=sd.values, color=["red", "green", "blue"], width=0.5)
plt.ylim(0,200)
plt.title("RGB Band Means and SD")
plt.xlabel("Spectral Bands")
plt.ylabel("Spectral Average")
plt.show()
23/17:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
km0 = KMeans(n_clusters=K).fit(df.to_numpy())


# use y_km0 for storing predicted lables
y_km0 = km0.predict(df.to_numpy())
23/18:
# write code to print number of samples in each cluster
cluster_counts = Counter(y_km0)

# Print the cluster counts
for cluster, count in sorted(cluster_counts.items()):
    print(f"[{cluster} {count}]")
23/19:
# visualize clusters as scatter plot
plt.figure(figsize=(6,4))
plt.scatter(df["R"],df["B"],c=y_km0)
plt.colorbar()
plt.xlabel("R")
plt.ylabel("B")
plt.show()
23/20:
# read full image data
img = rio.open('data/ilk-3b-1024.tif').read()
23/21:
# display image to check if you read the data correctly
show(img, cmap='viridis')
23/22:
# However, the clustering model prediction expects that the input to be r,g,b vectors
# So write code to convert image into rgb vectors

rgb_vec = np.array([[img[0][i][j],img[1][i][j],img[2][i][j]] for i in range(1024) for j in range(1024)])
23/23:
# if you converted it correctly, you should get the shape of vector as (1048576, 3)
rgb_vec.shape
23/24:
# its good to verify that format of r, g, b, and actual values
# for example, first image pixel value should be (124, 129, 112)
rgb_vec[0]
23/25:
# predict label for each pixel (vector) by calling the predict method of KMeans() clustering

imgkm = km0.predict(rgb_vec)
23/26:
# check the shape
imgkm_out.shape
23/27:
# you need to convert the is 1-D r,g,b vector back to image of 1024,1024 (note that its a single band image)
imgkm_out =  imgkm.reshape(1024, 1024)
23/28:
# check the shape
imgkm_out.shape
23/29:
# plot the clustered image
show(imgkm_out)
23/30:
# Import necessary libraries
import numpy as np

# Define a class for G-means clustering
class GMeansClustering:
    """
    G-means clustering implementation.

    Arguments:
    - min_samples: Minimum number of samples required for cluster splitting.
    - max_depth: Maximum depth of recursion for cluster splitting.
    - random_state: Seed for random number generation.
    - alpha: Significance level for normality testing.
    """

    # Constructor to initialize the GMeansClustering instance
    def __init__(self, min_samples=1, max_depth=10, random_state=None, alpha=0.01):
        """
        Initializing the GMeansClustering instance.
        """
        # Initializing parameters
        self.max_depth = max_depth
        self.min_samples = min_samples
        self.random_state = random_state
        # Validating alpha
        if alpha not in [0.15, 0.10, 0.05, 0.025, 0.01]:
            raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")
        self.alpha = alpha
        # List for storing stopping criteria
        self.stopping_criteria = []
        # Initializing attributes
        self.data_index = None
        self.data = None
        self.labels_ = None
        self.nearest_centroid_classifier = None

    # Method to check if values follow a normal distribution
    def _check_normal_distribution(self, values):
        """
        Check if values follow a normal distribution using the Anderson-Darling test.
        """
        # Performing Anderson-Darling test
        result = anderson(values)
        statistic_value = result.statistic
        crit_val_index = np.where(result.significance_level == self.alpha * 100)[0]
        critical_value = result.critical_values[crit_val_index]
        # Checking if the statistic value is less than the critical value
        return statistic_value <= critical_value

    # Method to recursively split clusters using k-means
    def _split_clusters(self, data, depth, index):
        """
        Recursively split clusters using k-means until max depth or normal distribution is achieved.
        """
        depth += 1
        if depth == self.max_depth:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('max_depth')
            return
        kmeans = KMeans(n_clusters=2)
        kmeans.fit(data)
        centroids = kmeans.cluster_centers_
        vector = centroids[0] - centroids[1]
        x_prime = scale(data.dot(vector) / (vector.dot(vector)))
        is_normal_dist = self._check_normal_distribution(x_prime)
        if is_normal_dist:
            self.data_index[index[:, 0]] = index
            self.stopping_criteria.append('normal_distribution')
            return
        labels = set(kmeans.labels_)
        for label in labels:
            current_data = data[kmeans.labels_ == label]
            if current_data.shape[0] <= self.min_samples:
                self.data_index[index[:, 0]] = index
                self.stopping_criteria.append('min_samples')
                return
            current_index = index[kmeans.labels_ == label]
            current_index[:, 1] = np.random.randint(0, 9999999999, dtype=np.int64)  # Changed dtype to np.int64
            self._split_clusters(data=current_data, depth=depth, index=current_index)

    # Method to fit data to the G-means clustering model
    def fit(self, data):
        """
        Fit data to the G-means clustering model.
        """
        self.data = data
        data_index = np.array([(i, False) for i in range(data.shape[0])], dtype=np.int64)  # Changed dtype to np.int64
        self.data_index = data_index
        self._split_clusters(data=data, depth=0, index=data_index)
        self.labels_ = self._encode_labels(self.data_index[:, 1])
        self.nearest_centroid_classifier = NearestCentroid().fit(self.data, self.labels_)
        return self

    # Method to encode original labels into sequential integers
    def _encode_labels(self, original_labels):
        """
        Encode original labels into sequential integers.
        """
        label_encoder = LabelEncoder()
        label_encoder.fit(original_labels)
        return label_encoder.transform(original_labels)

    # Method to predict cluster labels for input data
    def predict(self, X):
        """
        Predict cluster labels for input data.
        """
        return self.nearest_centroid_classifier.predict(X)
23/31:
# print optimal number of clusters found with the algorithm
gm0 = GMeansClustering().fit(df.to_numpy())
y_gm0 = gm0.predict(df.to_numpy())
print("Optimal number of clusters:", len(np.unique(y_gm0)))
23/32: print(y_gm0)
22/53:
# generate scatter plot and color using cluster labels
plt.figure(figsize=(6,4))
plt.scatter(df["R"],df["B"],c=unique_clusters)
plt.colorbar()
plt.xlabel("R")
plt.ylabel("B")
plt.show()
22/54: type(unique_clusters)
22/55: unique_clusters
23/33: type(y_gm0)
23/34: y_gm0
22/56: df
22/57:
# generate scatter plot and color using cluster labels
plt.figure(figsize=(6, 4))
scatter = plt.scatter(df["R"], df["B"], c=unique_clusters, cmap='viridis')  # Use a colormap
plt.colorbar(scatter)
plt.xlabel("R")
plt.ylabel("B")
plt.title("Scatter Plot Colored by Cluster Labels")
plt.show()
22/58:
# generate scatter plot and color using cluster labels
n_clusters = unique_clusters
colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))  # Generate 189 equally spaced colors

plt.figure(figsize=(10, 8))
scatter = plt.scatter(df["R"], df["B"], c=unique_clusters, cmap='viridis', s=10)  # Use 's' to adjust point size
plt.colorbar(scatter, ticks=range(n_clusters))
plt.xlabel("R")
plt.ylabel("B")
plt.title("Scatter Plot Colored by Cluster Labels")
plt.show()

Key Points

    Color Map: I used the 'viridis' colormap, which is a good default choice for many clusters because it provides a smooth gradient of colors. However, for visual distinction among 189 different clusters, this might still be insufficient due to human visual limitations.

    Custom Colors: If 'viridis' or similar colormaps are not sufficient, you might consider generating a custom set of colors. However, distinguishing 189 visually different colors is generally beyond typical human capability, especially in a scatter plot.

    Plot Adjustments: Adjust the size of the scatter points using the s parameter if
22/59:
# generate scatter plot and color using cluster labels
n_clusters = unique_clusters
colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))  # Generate 189 equally spaced colors

plt.figure(figsize=(10, 8))
scatter = plt.scatter(df["R"], df["B"], c=unique_clusters, cmap='viridis', s=10)  # Use 's' to adjust point size
plt.colorbar(scatter, ticks=range(n_clusters))
plt.xlabel("R")
plt.ylabel("B")
plt.title("Scatter Plot Colored by Cluster Labels")
plt.show()
22/60:
# generate scatter plot and color using cluster labels
n_clusters = len(unique_clusters)
colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))  # Generate 189 equally spaced colors

plt.figure(figsize=(10, 8))
scatter = plt.scatter(df["R"], df["B"], c=unique_clusters, cmap='viridis', s=10)  # Use 's' to adjust point size
plt.colorbar(scatter, ticks=range(n_clusters))
plt.xlabel("R")
plt.ylabel("B")
plt.title("Scatter Plot Colored by Cluster Labels")
plt.show()
22/61: len(unique_clusters)
22/62: unique_clusters
23/35: len(y_gm0)
22/63: len(cluster_labels)
22/64:
# generate scatter plot and color using cluster labels
n_clusters = len(unique_clusters)
colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))  # Generate 189 equally spaced colors

plt.figure(figsize=(10, 8))
scatter = plt.scatter(df["R"], df["B"], c=cluster_labels, cmap='viridis', s=10)  # Use 's' to adjust point size
plt.colorbar(scatter, ticks=range(n_clusters))
plt.xlabel("R")
plt.ylabel("B")
plt.title("Scatter Plot Colored by Cluster Labels")
plt.show()
22/65:

print("Shape of the RGB vector:", rgb_vec.shape)

# Cluster the RGB values using the G-means clustering model
labels = cluster_labels.predict(rgb_vec)

# Checking the shape of the clustered image, which should be equivalent to 1024 * 1024 = 1048576
print("Shape of the clustered image:", cluster_labels.shape)

# Reconstructing the single-band image from the 1D cluster labels to a 1024x1024 image
image_clustered = np.zeros([1024, 1024])
for i in range(1024):
    for j in range(1024):
        index = i * 1024 + j
        image_clustered[i][j] = cluster_labels[index]

# Verifying the shape of the reconstructed image
print("Shape of the reconstructed image:", image_clustered.shape)
22/66: type(rgb_vec)
22/67:

print("Shape of the RGB vector:", rgb_vec.shape)

# Cluster the RGB values using the G-means clustering model
g_means_model_pred, cluster_labels_pred = fit_g_means(rgb_vec)
labels = predict_clusters(g_means_model_pred, rgb_vec)


# Checking the shape of the clustered image, which should be equivalent to 1024 * 1024 = 1048576
print("Shape of the clustered image:", labels.shape)

# Reconstructing the single-band image from the 1D cluster labels to a 1024x1024 image
image_clustered = np.zeros([1024, 1024])
for i in range(1024):
    for j in range(1024):
        index = i * 1024 + j
        image_clustered[i][j] = labels[index]

# Verifying the shape of the reconstructed image
print("Shape of the reconstructed image:", image_clustered.shape)
22/68:
# display the clustered image here.
show(image_clustered)
22/69:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
22/70:
# import necessary libraries; don't change this cell, if you need any additional libraries, use next cell
import numpy as np
import rasterio as rio
from rasterio.plot import show
from rasterio.plot import show_hist
from rasterio.windows import Window
from rasterio.windows import from_bounds
import pandas as pd
import random 
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cluster import KMeans
import sys
#
np.random.seed(100)
22/71:
# if you need additional libraries, include them here
# make to include all your additional liberies here and not any other cells

# your algorithm 1 clode goes here ...
import numpy as np
from sklearn.cluster import KMeans
from sklearn.neighbors import NearestCentroid
from sklearn.preprocessing import scale, LabelEncoder
from scipy.stats import anderson
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
22/72:
# Read image: data/ilk-3b-1024.tif
# Hint: explore rasterio package


# Open the TIFF file
rgb_img = rio.open("data/ilk-3b-1024.tif")
22/73: print(rgb_img.profile)
22/74: rgb_img.shape
22/75:
# visualize image data and its histogram (write your code here to generate figure similar to the one below
# Hint: you can explore matplotlib subplots, raterio.plot, etc
#with rio.open(tif_file) as src:
    # Plot the raster data
    #show(src)

#img = Image.open('/mnt/data/plolt.png').convert('L')

#img_array = np.array(img)


fig, (ax_img, ax_hist) = plt.subplots(1, 2, figsize=(14, 7))
show(rgb_img, ax=ax_img)
show_hist(rgb_img, ax=ax_hist, bins=50, lw=0.0, stacked=False, alpha=0.3, histtype='stepfilled')
plt.show()
22/76:
# read csv file containing sample locations: data/ilk-3b-1024-10k-pnts.csv
# hint, you can use pandas functions

xydata = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
22/77: print(xydata.head())
22/78:
# now you can read data from raster image for every x,y coordinate from the csv file
# Hint: explore rasterio image reading functions
# The code you write here should finally produce a panadas dataframe, say df
with rio.open('data/ilk-3b-1024.tif') as src:
    # Read the coordinate points from CSV
    points_df = pd.read_csv('data/ilk-3b-1024-10k-pnts.csv')
    
    # List to hold RGB values for each point
    d = []

    # Iterate through each point in the dataframe
    for idx, row in points_df.iterrows():
        # Convert geographic coordinates to pixel coordinates
        # `src.index` returns column (x) and row (y) indices in the raster for given spatial coordinates
        col, row = src.index(row['X'], row['Y'])

        # Read the pixel value at each band
        r = src.read(1)[row, col]
        g = src.read(2)[row, col]
        b = src.read(3)[row, col]
        
        # Append the RGB values to the list
        d.append({'R': r, 'G': g, 'B': b})








# convert the data you read (say into variable d) to a dataframe                
df = pd.DataFrame(d)
22/79:
# First name the columns in the dataframe as R, G, B.
# Then display image data (r, g, b) for first 5 points

# already named them R, G, B

df.head()
22/80:
# compute mean and standard deviation of each band, 
# generate bargraphs, and show standard deviation on the bar plot

m = df.mean()
sd = df.std()
22/81:
print("Means:\n", m)
print("SD:\n", sd)
22/82:
# write code to plot the means and standard deviations here


colors = ['red', 'green', 'blue']
ax = m.plot(kind='bar', yerr=sd, color=colors, alpha=0.7, error_kw=dict(ecolor='gray', lw=1, capsize=5, capthick=1))
ax.set_ylabel('Pixel Value')
plt.title('Mean and Standard Deviation of RGB Bands')
plt.show()
22/83:
# visualize rgb data as scatter plot; select bands 1 and 3
# note image attributes are called bands (e.g., red, green, blue bands)

plt.scatter(df['R'], df['B'], c='magenta', edgecolor='k', alpha=0.5)
plt.xlabel('R')
plt.ylabel('B')
plt.title('Scatter Plot of R and B')
plt.show()
22/84:
# k-means clustering using KMeans() from scikit-learn with K = 10

K = 10

# use km0 to store mode
km0 = KMeans(n_clusters=K, random_state=100)




# use y_km0 for storing predicted lables
y_km0 = km0.fit_predict(df[['R', 'G', 'B']])
22/85:
# write code to print number of samples in each cluster

df['cluster_label'] = y_km0

cluster_counts = df['cluster_label'].value_counts()
# Print the counts for each cluster
print("Number of samples in each cluster:")
print(cluster_counts)
22/86:
# visualize clusters as scatter plot

# Create a scatter plot
plt.figure(figsize=(10, 6))
scatter = plt.scatter(df['R'], df['B'], c=df['cluster_label'], cmap='viridis', alpha=0.6, edgecolors='w')

# Add a color bar to the plot to show the cluster colors
plt.colorbar(scatter)

# Set the plot labels and title
plt.xlabel('Red Intensity')
plt.ylabel('Blue Intensity')
plt.title('Scatter Plot of Clusters')

# Show the plot
plt.show()
22/87:
# read full image data
# Open the raster file
tif_file = 'data/ilk-3b-1024.tif'

with rio.open(tif_file) as src:
    # Read the full image data; this loads all bands
    img = src.read()
22/88:
# display image to check if you read the data correctly
show(img, cmap='viridis')
22/89:
# note that the img data you read is ndarray, verify it
type(img)
22/90:
# and its shape should be (3, 1024, 1024)
print(img.shape)
22/91:
# However, the clustering model prediction expects that the input to be r,g,b vectors
# So write code to convert image into rgb vectors

img_transposed = np.transpose(img, (1, 2, 0))



rgb_vec = img_transposed.reshape(-1, 3)
22/92:
# if you converted it correctly, you should get the shape of vector as (1048576, 3)
rgb_vec.shape
22/93:
# its good to verify that format of r, g, b, and actual values
# for example, first image pixel value should be (124, 129, 112)
rgb_vec[0]
22/94:
# predict label for each pixel (vector) by calling the predict method of KMeans() clustering
km0.fit(rgb_vec)

imgkm = km0.predict(rgb_vec)
22/95:
# Check the shape of the output, should be same as 1024 * 1024 = 1048576
imgkm.shape
22/96:
# you need to convert the is 1-D r,g,b vector back to image of 1024,1024 (note that its a single band image)
imgkm_out = imgkm.reshape(1024, 1024)
22/97:
# check the shape
imgkm_out.shape
22/98:
# plot the clustered image
# Visualizing the clustered image
plt.figure(figsize=(8, 8))
plt.imshow(imgkm_out, cmap='viridis')  # Using 'viridis' colormap to visually distinguish the clusters
plt.colorbar()  # Show the color mapping for cluster labels
plt.title('Clustered Image Representation')
plt.show()
22/99:


# Constants and Configurations
MIN_SAMPLES = 1
MAX_DEPTH = 10
RANDOM_STATE = None
ALPHA = 0.01

def validate_alpha(alpha_level):
    if alpha_level not in [0.15, 0.10, 0.05, 0.025, 0.01]:
        raise ValueError("Alpha must be one of: 0.15, 0.10, 0.05, 0.025, 0.01")

def perform_anderson_test(values):
    normalized_values = (values - np.mean(values)) / np.std(values)
    result = anderson(normalized_values.flatten())
    critical_value = result.critical_values[2]  # 5% significance
    return result.statistic <= critical_value

def recursively_split_clusters(cluster_data, current_depth, cluster_labels):
    current_depth += 1
    if current_depth >= MAX_DEPTH:
        return cluster_labels

    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE)
    kmeans.fit(cluster_data)
    cluster_centers = kmeans.cluster_centers_
    separation_vector = cluster_centers[0] - cluster_centers[1]
    x_prime = scale(cluster_data.dot(separation_vector) / (separation_vector.dot(separation_vector)))

    if perform_anderson_test(x_prime):
        return cluster_labels

    labels = kmeans.labels_
    unique_labels = np.unique(labels)
    label_counter = np.max(cluster_labels) + 1  # Ensure unique labels across recursive calls

    for label in unique_labels:
        sub_cluster_data = cluster_data[labels == label]
        if len(sub_cluster_data) <= MIN_SAMPLES:
            continue
        sub_labels = np.full(sub_cluster_data.shape[0], label_counter, dtype=int)
        updated_sub_labels = recursively_split_clusters(sub_cluster_data, current_depth, sub_labels)
        cluster_labels[labels == label] = updated_sub_labels
        label_counter = np.max(cluster_labels) + 1  # Update label_counter for next unique label

    return cluster_labels

def fit_g_means(data):
    validate_alpha(ALPHA)
    initial_labels = np.zeros(data.shape[0], dtype=int)
    final_labels = recursively_split_clusters(data, 0, initial_labels)
    if len(np.unique(final_labels)) < 2:
        print("Not enough splits to form multiple clusters.")
        return None, final_labels

    centroid_model = NearestCentroid().fit(data, final_labels)
    return centroid_model, final_labels

def predict_clusters(model, data):
    return model.predict(data)
22/100:
# print optimal number of clusters found with the algorithm


g_means_model, cluster_labels = fit_g_means(df.to_numpy())
predicted_labels = predict_clusters(g_means_model, df.to_numpy())
optimal_clusters = len(np.unique(predicted_labels))

print("Optimal number of clusters:", optimal_clusters)
22/101: unique_clusters = np.unique(predicted_labels)
22/102:
for i, count in enumerate(unique_clusters):
        print(f"Cluster {i}: {count} samples")
22/103: unique_clusters
22/104:
# generate scatter plot and color using cluster labels
n_clusters = len(unique_clusters)
colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))  # Generate 189 equally spaced colors

plt.figure(figsize=(10, 8))
scatter = plt.scatter(df["R"], df["B"], c=cluster_labels, cmap='viridis', s=10)  # Use 's' to adjust point size
plt.colorbar(scatter, ticks=range(n_clusters))
plt.xlabel("R")
plt.ylabel("B")
plt.title("Scatter Plot Colored by Cluster Labels")
plt.show()
22/105:

print("Shape of the RGB vector:", rgb_vec.shape)

# Cluster the RGB values using the G-means clustering model
g_means_model_pred, cluster_labels_pred = fit_g_means(rgb_vec)
labels = predict_clusters(g_means_model_pred, rgb_vec)


# Checking the shape of the clustered image, which should be equivalent to 1024 * 1024 = 1048576
print("Shape of the clustered image:", labels.shape)

# Reconstructing the single-band image from the 1D cluster labels to a 1024x1024 image
image_clustered = np.zeros([1024, 1024])
for i in range(1024):
    for j in range(1024):
        index = i * 1024 + j
        image_clustered[i][j] = labels[index]

# Verifying the shape of the reconstructed image
print("Shape of the reconstructed image:", image_clustered.shape)
22/106:
# display the clustered image here.
show(image_clustered)
24/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
24/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
24/3:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
25/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
26/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
27/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
27/2: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
27/3: !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
27/4:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
27/5:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
27/6: !pip install pandas
27/7:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.extend(dataset[i, 4*n+1:4*n+3])
            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])
        x_raw.append(x_raw_1)
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []
        
    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
27/8: !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
27/9:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
27/10:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
27/11:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
27/12: !pip install matplotlib
27/13:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
27/14:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.extend(dataset[i, 4*n+1:4*n+3])
            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])
        x_raw.append(x_raw_1)
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []
        
    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
27/15:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
27/16: !pip install openpyxl
27/17:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
27/18:
train_percentage = 100
val_percentage = 100

train_dataset = slice_dataset(dataset1, train_percentage)
val_dataset = slice_dataset(dataset2, val_percentage)

n_bus = 14

#actual data
x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)
x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)

#normalized data
x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)
x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)
27/19:
class My_NN(torch.nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None):
        super(My_NN, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = Linear(self.input_size, self.hidden_size1)
        self.lin2 = Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = Linear(self.hidden_size2, self.output_size)

    def forward(self, x):
        
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)
        x = torch.tanh(x)

        x = self.lin3(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
27/20:
%%time

input_size = n_bus*2
hidden_size1 = 30
hidden_size2 = 30
output_size = n_bus*2
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_prediction = model(x_norm_train)
    train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))
    train_loss.backward()
    optimizer.step()
    train_loss_list.append(train_loss.detach())

    model.eval()
    y_val_prediction = model(x_norm_val)
    val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_loss_list.append(val_loss.detach())

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
27/21:
plt.title('NN on power flow dataset')
plt.plot(train_loss_list, label="train loss")
plt.plot(val_loss_list, label="val loss")
plt.yscale('log')
plt.xlabel("# Epoch")
plt.ylabel("Loss")
plt.legend(loc='best')
plt.show()

print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))
print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))
27/22:
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
27/23:
model.eval()

y_train_prediction = model(x_norm_train)
train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_train.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(train_loss))

print("===========================================================================")

y_val_prediction = model(x_norm_val)
val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_val.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(val_loss))
27/24:
best_model = torch.load("[PyG] [14 bus] Best_NN_model.pt")
best_model.eval()

y_train_prediction = best_model(x_norm_train)
train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_train.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(train_loss))

print("===========================================================================")

y_val_prediction = best_model(x_norm_val)
val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_val.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(val_loss))
27/25:
%%time

best_model = torch.load("[PyG] [14 bus] Best_NN_model.pt")
best_model.eval()

test_loss_list = []

for i in range(102):
    
    dataset = pd.read_excel('dataset\Grid_14 bus_%d.xlsx' % (i+1)).values
    test_percentage = 100
    test_dataset = slice_dataset(dataset, test_percentage)
    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)
    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)
    
    print('dataset {:d}'.format(i+1))
    
    y_test_prediction = best_model(x_norm_test)
    test_loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_test, y_val_mean, y_val_std))
    
    if i == 0:
        print('Train loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))
    elif i == 1:
        print('Val loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))
    else:
        print('Test loss (MSE): {:.7f}'.format(test_loss))
        test_loss_list.append(test_loss.detach().numpy())
    
    print("===========================")

column = []
for i in range(100):
    column.append('test loss %d' % (i+1))
    
test_loss_file = pd.DataFrame([test_loss_list], columns=column)
test_loss_file.to_excel("[PyG] [14 bus] [MSE] NN test loss.xlsx")
print("\ntest loss file saved!\n")
27/26:
%%time

best_model = torch.load("[PyG] [14 bus] Best_NN_model.pt")
best_model.eval()

test_loss_list = []

for i in range(102):
    
    dataset = pd.read_excel('dataset\Grid_14 bus_%d.xlsx' % (i+1)).values
    test_percentage = 100
    test_dataset = slice_dataset(dataset, test_percentage)
    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)
    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)
    
    print('dataset {:d}'.format(i+1))
    
    yhat = denormalize_output(best_model(x_norm_test), y_val_mean, y_val_std)
    y = y_raw_test
    test_loss_NRMSE = NRMSE(yhat, y)
    
    if i == 0:
        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
    elif i == 1:
        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
    else:
        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
        test_loss_list.append(test_loss_NRMSE.detach().numpy())
    
    print("===========================")

column = []
for i in range(100):
    column.append('test loss %d' % (i+1))
    
test_loss_file = pd.DataFrame([test_loss_list], columns=column)
test_loss_file.to_excel("[PyG] [14 bus] [NRMSE] NN test loss.xlsx")
print("\ntest loss file saved!\n")
27/27:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
27/28:
train_percentage = 100
val_percentage = 100

train_dataset = slice_dataset(dataset1, train_percentage)
val_dataset = slice_dataset(dataset2, val_percentage)

n_bus = 14

#actual data
x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)
x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)

#normalized data
x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)
x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)
27/29:
class My_NN(torch.nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None):
        super(My_NN, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = Linear(self.input_size, self.hidden_size1)
        self.lin2 = Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = Linear(self.hidden_size2, self.output_size)

    def forward(self, x):
        
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)
        x = torch.tanh(x)

        x = self.lin3(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
27/30:
%%time

input_size = n_bus*2
hidden_size1 = 30
hidden_size2 = 30
output_size = n_bus*2
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_prediction = model(x_norm_train)
    train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))
    train_loss.backward()
    optimizer.step()
    train_loss_list.append(train_loss.detach())

    model.eval()
    y_val_prediction = model(x_norm_val)
    val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_loss_list.append(val_loss.detach())

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
27/31:
plt.title('NN on power flow dataset')
plt.plot(train_loss_list, label="train loss")
plt.plot(val_loss_list, label="val loss")
plt.yscale('log')
plt.xlabel("# Epoch")
plt.ylabel("Loss")
plt.legend(loc='best')
plt.show()

print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))
print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))
27/32:
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
27/33:
model.eval()

y_train_prediction = model(x_norm_train)
train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_train.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(train_loss))

print("===========================================================================")

y_val_prediction = model(x_norm_val)
val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_val.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(val_loss))
27/34:
best_model = torch.load("[PyG] [14 bus] Best_NN_model.pt")
best_model.eval()

y_train_prediction = best_model(x_norm_train)
train_loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_train, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_train.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_train_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(train_loss))

print("===========================================================================")

y_val_prediction = best_model(x_norm_val)
val_loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
print("Train output ground-truth: \n" + str(y_raw_val.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_val_prediction, y_val_mean, y_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(val_loss))
27/35:
%%time

best_model = torch.load("[PyG] [14 bus] Best_NN_model.pt")
best_model.eval()

test_loss_list = []

for i in range(102):
    
    dataset = pd.read_excel('dataset\Grid_14 bus_%d.xlsx' % (i+1)).values
    test_percentage = 100
    test_dataset = slice_dataset(dataset, test_percentage)
    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)
    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)
    
    print('dataset {:d}'.format(i+1))
    
    y_test_prediction = best_model(x_norm_test)
    test_loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(y_norm_test, y_val_mean, y_val_std))
    
    if i == 0:
        print('Train loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))
    elif i == 1:
        print('Val loss (MSE): {:.7f}'.format(test_loss.detach().numpy()))
    else:
        print('Test loss (MSE): {:.7f}'.format(test_loss))
        test_loss_list.append(test_loss.detach().numpy())
    
    print("===========================")

column = []
for i in range(100):
    column.append('test loss %d' % (i+1))
    
test_loss_file = pd.DataFrame([test_loss_list], columns=column)
test_loss_file.to_excel("[PyG] [14 bus] [MSE] NN test loss.xlsx")
print("\ntest loss file saved!\n")
27/36:
%%time

best_model = torch.load("[PyG] [14 bus] Best_NN_model.pt")
best_model.eval()

test_loss_list = []

for i in range(102):
    
    dataset = pd.read_excel('dataset\Grid_14 bus_%d.xlsx' % (i+1)).values
    test_percentage = 100
    test_dataset = slice_dataset(dataset, test_percentage)
    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)
    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)
    
    print('dataset {:d}'.format(i+1))
    
    yhat = denormalize_output(best_model(x_norm_test), y_val_mean, y_val_std)
    y = y_raw_test
    test_loss_NRMSE = NRMSE(yhat, y)
    
    if i == 0:
        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
    elif i == 1:
        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
    else:
        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
        test_loss_list.append(test_loss_NRMSE.detach().numpy())
    
    print("===========================")

column = []
for i in range(100):
    column.append('test loss %d' % (i+1))
    
test_loss_file = pd.DataFrame([test_loss_list], columns=column)
test_loss_file.to_excel("[PyG] [14 bus] [NRMSE] NN test loss.xlsx")
print("\ntest loss file saved!\n")
30/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
30/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
30/3: !pip install torch-geometric
30/4:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
30/5:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n+1], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
30/6:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
30/7:
train_percentage = 100
val_percentage = 100

train_dataset = slice_dataset(dataset1, train_percentage)
val_dataset = slice_dataset(dataset2, val_percentage)

n_bus = 14

#actual data
x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)
x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)

#normalized data
x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)
x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)
30/8:
x_train, y_train = x_norm_train, y_norm_train
x_val, y_val = x_norm_val, y_norm_val
edge_index = torch.tensor([[0, 1, 1, 2, 1, 3, 2, 4, 3, 5, 4, 6, 4, 7, 5, 8, 5, 9, 1, 10, 10, 11, 11, 12, 11, 13],
                           [1, 0, 2, 1, 3, 1, 4, 2, 5, 3, 6, 4, 7, 4, 8, 5, 9, 5, 10, 1, 11, 10, 12, 11, 13, 11]], dtype=torch.long)

data_train_list, data_val_list = [], []
for i,_ in enumerate(x_train):
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))
for i,_ in enumerate(x_val):
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
30/9:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
30/10:
feat_in = 2
feat_size1 = 4
hidden_size1 = 30
output_size = n_bus*2
lr = 0.0001

model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
30/11:
%%time

feat_in = 2
feat_size1 = 4
hidden_size1 = 30
output_size = n_bus*2
lr = 0.0001

model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2001):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
31/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
31/2:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n+1], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
31/3:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
31/4:
train_percentage = 100
val_percentage = 100

train_dataset = slice_dataset(dataset1, train_percentage)
val_dataset = slice_dataset(dataset2, val_percentage)

n_bus = 14

#actual data
x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)
x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)

#normalized data
x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)
x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)
31/5:
x_train, y_train = x_norm_train, y_norm_train
x_val, y_val = x_norm_val, y_norm_val
edge_index = torch.tensor([[0, 1, 1, 2, 1, 3, 2, 4, 3, 5, 4, 6, 4, 7, 5, 8, 5, 9, 1, 10, 10, 11, 11, 12, 11, 13],
                           [1, 0, 2, 1, 3, 1, 4, 2, 5, 3, 6, 4, 7, 4, 8, 5, 9, 5, 10, 1, 11, 10, 12, 11, 13, 11]], dtype=torch.long)

data_train_list, data_val_list = [], []
for i,_ in enumerate(x_train):
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))
for i,_ in enumerate(x_val):
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
31/6:
class My_GNN_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, feat_size2=None, hidden_size1=None, output_size=None):
        super(My_GNN_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 5
        self.feat_size2 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.conv2 = GCNConv(feat_size1, feat_size2)
        self.lin1 = Linear(node_size*feat_size2, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)
        
        x = self.conv2(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
31/7:

feat_in = 2
feat_size1 = 8
feat_size2 = 4
hidden_size1 = 30
output_size = n_bus*2
lr = 0.0001

model = My_GNN_GNN_NN(n_bus, feat_in, feat_size1, feat_size2, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
31/8:
%%time

feat_in = 2
feat_size1 = 8
feat_size2 = 4
hidden_size1 = 30
output_size = n_bus*2
lr = 0.0001

model = My_GNN_GNN_NN(n_bus, feat_in, feat_size1, feat_size2, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2001):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
31/9:
plt.title('GNN GNN NN on power flow dataset')
plt.plot(train_loss_list, label="train loss")
plt.plot(val_loss_list, label="val loss")
plt.yscale('log')
plt.xlabel("# Epoch")
plt.ylabel("Loss")
plt.legend(loc='best')
plt.show()

print('last epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(epoch, train_loss, val_loss))
print('best epoch: {:d}, train loss: {:.7f}, val loss: {:.7f}'.format(best_epoch, best_train_loss, best_val_loss))
31/10:
model.eval()

y_train_prediction_1 = model(train_loader.dataset[0])
train_loss_1 = MSE(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_train[0], y_val_mean, y_val_std))
print("[1 datapoint] Train output ground-truth: \n" + str(y_raw_train[0].detach().numpy()))
print("[1 datapoint] Train output prediction: \n" + str(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std).detach().numpy()))
print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))

train_loss = 0
for batch in train_loader:
    pred = model(batch)
    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
    train_loss += loss.item() * batch.num_graphs
train_loss /= len(train_loader.dataset)
print('Train loss (MSE): {:.7f}'.format(train_loss))

print("=========================================================================")

y_val_prediction_1 = model(val_loader.dataset[0])
val_loss_1 = MSE(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_val[0], y_val_mean, y_val_std))
print("[1 datapoint] Val output ground-truth: \n" + str(y_raw_val[0].detach().numpy()))
print("[1 datapoint] Val output prediction: \n" + str(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std).detach().numpy()))
print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))

val_loss=0
for batch in val_loader:
    pred = model(batch)
    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
    val_loss += loss.item() * batch.num_graphs
val_loss /= len(val_loader.dataset)
print('Val loss (MSE): {:.7f}'.format(val_loss))
31/11:
best_model = torch.load("[PyG] [14 bus] Best_GNN_GNN_NN_model.pt")
best_model.eval()

y_train_prediction_1 = best_model(train_loader.dataset[0])
train_loss_1 = MSE(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_train[0], y_val_mean, y_val_std))
print("[1 datapoint] Train output ground-truth: \n" + str(y_raw_train[0].detach().numpy()))
print("[1 datapoint] Train output prediction: \n" + str(denormalize_output(y_train_prediction_1, y_val_mean, y_val_std).detach().numpy()))
print('[1 datapoint] Train loss (MSE): {:.7f}'.format(train_loss_1))

train_loss = 0
for batch in train_loader:
    pred = best_model(batch)
    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
    train_loss += loss.item() * batch.num_graphs
train_loss /= len(train_loader.dataset)
print('Train loss (MSE): {:.7f}'.format(train_loss))

print("=========================================================================")

y_val_prediction_1 = best_model(val_loader.dataset[0])
val_loss_1 = MSE(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std), denormalize_output(y_norm_val[0], y_val_mean, y_val_std))
print("[1 datapoint] Val output ground-truth: \n" + str(y_raw_val[0].detach().numpy()))
print("[1 datapoint] Val output prediction: \n" + str(denormalize_output(y_val_prediction_1, y_val_mean, y_val_std).detach().numpy()))
print('[1 datapoint] Val loss (MSE): {:.7f}'.format(val_loss_1))

val_loss=0
for batch in val_loader:
    pred = best_model(batch)
    loss = MSE(denormalize_output(pred, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
    val_loss += loss.item() * batch.num_graphs
val_loss /= len(val_loader.dataset)
print('Val loss (MSE): {:.7f}'.format(val_loss))
31/12:
%%time

best_model = torch.load("[PyG] [14 bus] Best_GNN_GNN_NN_model.pt")
best_model.eval()

test_loss_list = []

for i in range(102):
    
    dataset = pd.read_excel('dataset\Grid_14 bus_%d.xlsx' % (i+1)).values
    test_percentage = 100
    test_dataset = slice_dataset(dataset, test_percentage)
    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)
    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)
    
    x_test, y_test = x_norm_test, y_norm_test
    
    data_test_list = []
    for j,_ in enumerate(x_test):
        data_test_list.append(Data(x=x_test[j], y=y_test[j], edge_index=edge_index))

    test_loader = DataLoader(data_test_list, batch_size=1)
    
    print('dataset {:d}'.format(i+1))
    
    test_loss = 0
    for batch in test_loader:
        y_test_prediction = best_model(batch)
        loss = MSE(denormalize_output(y_test_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        test_loss += loss.item() * batch.num_graphs
    test_loss /= len(test_loader.dataset)
    
    if i == 0:
        print('Train loss (MSE): {:.7f}'.format(test_loss))
    elif i == 1:
        print('Val loss (MSE): {:.7f}'.format(test_loss))
    else:
        print('Test loss (MSE): {:.7f}'.format(test_loss))
        test_loss_list.append(test_loss)
    
    print("===========================")

column = []
for i in range(100):
    column.append('test loss %d' % (i+1))
    
test_loss_file = pd.DataFrame([test_loss_list], columns=column)
test_loss_file.to_excel("[PyG] [14 bus] [MSE] GNN GNN NN test loss.xlsx")
print("\ntest loss file saved!\n")
31/13:
%%time

best_model = torch.load("[PyG] [14 bus] Best_GNN_GNN_NN_model.pt")
best_model.eval()

test_loss_list = []

for i in range(102):
    
    dataset = pd.read_excel('dataset\Grid_14 bus_%d.xlsx' % (i+1)).values
    test_percentage = 100
    test_dataset = slice_dataset(dataset, test_percentage)
    x_raw_test, y_raw_test = make_dataset(test_dataset, n_bus)
    x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_test, y_raw_test)
    
    x_test, y_test = x_norm_test, y_norm_test
    
    data_test_list = []
    for j,_ in enumerate(x_test):
        data_test_list.append(Data(x=x_test[j], y=y_test[j], edge_index=edge_index))

    test_loader = DataLoader(data_test_list, batch_size=1)
    
    print('dataset {:d}'.format(i+1))
    
    test_loss = 0
    yhat = torch.empty(0, n_bus*2)
    for batch in test_loader:
        y_test_prediction = best_model(batch)
        yhat = torch.cat((yhat, y_test_prediction.reshape(1, n_bus*2)))
    
    yhat = denormalize_output(yhat, y_val_mean, y_val_std)
    y = y_raw_test
    test_loss_NRMSE = NRMSE(yhat, y)
    
    if i == 0:
        print('Train loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
    elif i == 1:
        print('Val loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
    else:
        print('Test loss (NRMSE): {:.7f}'.format(test_loss_NRMSE.detach().numpy()))
        test_loss_list.append(test_loss_NRMSE.detach().numpy())
    
    print("===========================")

column = []
for i in range(100):
    column.append('test loss %d' % (i+1))
    
test_loss_file = pd.DataFrame([test_loss_list], columns=column)
test_loss_file.to_excel("[PyG] [14 bus] [NRMSE] GNN GNN NN test loss.xlsx")
print("\ntest loss file saved!\n")
32/1:
import torch
import torch.nn as nn
import torch.nn.functional as F

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
32/2:
NN_dataset = pd.read_excel('[PyG] [14 bus] [MSE] NN test loss.xlsx').values
NN_testloss = NN_dataset[0][1:]

GNN_NN_dataset = pd.read_excel('[PyG] [14 bus] [MSE] GNN NN test loss.xlsx').values
GNN_NN_testloss = GNN_NN_dataset[0][1:]

GNN_GNN_NN_dataset = pd.read_excel('[PyG] [14 bus] [MSE] GNN GNN NN test loss.xlsx').values
GNN_GNN_NN_testloss = GNN_GNN_NN_dataset[0][1:]

NN_dataset_NRMSE = pd.read_excel('[PyG] [14 bus] [NRMSE] NN test loss.xlsx').values
NN_testloss_NRMSE = NN_dataset_NRMSE[0][1:]

GNN_NN_dataset_NRMSE = pd.read_excel('[PyG] [14 bus] [NRMSE] GNN NN test loss.xlsx').values
GNN_NN_testloss_NRMSE = GNN_NN_dataset_NRMSE[0][1:]

GNN_GNN_NN_dataset_NRMSE = pd.read_excel('[PyG] [14 bus] [NRMSE] GNN GNN NN test loss.xlsx').values
GNN_GNN_NN_testloss_NRMSE = GNN_GNN_NN_dataset_NRMSE[0][1:]
33/1:
dataset1 = pd.read_excel('dataset\measured_reactive_power.xlsx').values
dataset2 = pd.read_excel('dataset\measured_active_power.xlsx').values
dataset3 = pd.read_excel('dataset\actual_voltage_angles.xlsx').values
dataset4 = pd.read_excel('dataset\actual_voltage_magnitudes.xlsx').values
33/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
33/3:
dataset1 = pd.read_csv('dataset\measured_reactive_power.xlsx').values
dataset2 = pd.read_csv('dataset\measured_active_power.xlsx').values
dataset3 = pd.read_csv('dataset\actual_voltage_angles.xlsx').values
dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.xlsx').values
33/4:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
33/5:
dataset1 = pd.read_csv('dataset\measured_reactive_power.xlsx').values
dataset2 = pd.read_csv('dataset\measured_active_power.xlsx').values
dataset3 = pd.read_csv('dataset\actual_voltage_angles.xlsx').values
dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.xlsx').values
33/6:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset\actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.csv').values
33/7:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset\actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.csv').values
33/8:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset\actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.csv').values
33/9:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset\\actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.csv').values
33/10:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values

dataset4 = pd.read_csv('dataset\actual_voltage_magnitudes.csv').values
33/11:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/12:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
33/13:
df1.columns = ['P_' + col for col in df1.columns]
df2.columns = ['Q_' + col for col in df2.columns]
df3.columns = ['V_' + col for col in df3.columns]
df4.columns = ['d_' + col for col in df4.columns]
33/14:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
33/15: df1.columns
33/16:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/17:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
33/18: df1.columns
33/19: df2.columns
33/20: df3.columns
33/21: dataset1
33/22: dataset1.shape
33/23: dataset2.shape
33/24: dataset3.shape
33/25: dataset4.shape
33/26: df1
33/27:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv').values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/28:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
33/29: df1
33/30: df2.head
33/31: df2
33/32:
dataset1 = pd.read_csv('dataset\measured_reactive_power.csv', header=0).values
dataset2 = pd.read_csv('dataset\measured_active_power.csv').values
dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/33:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
33/34: df1
33/35: dataset1
33/36:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
33/37: dataset1
33/38: df5 = pd.DataFrame(dataset1)
33/39: df5.head
33/40:
dataset3 = pd.read_csv('dataset\measured_reactive_power.csv', header=0).values
dataset4 = pd.read_csv('dataset\measured_active_power.csv').values
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/41:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
33/42: df1
33/43:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
33/44: df5 = pd.DataFrame(dataset1)
33/45: df5.head
33/46: df5
33/47: df.type()
33/48: type(df5)
33/49: dataset(1)
33/50: dataset1(1)
33/51: dataset1[1]
33/52: dataset1[0]
33/53:
dataset3 = pd.read_csv('dataset\measured_reactive_power.csv', encoding= 'utf-8').values
dataset4 = pd.read_csv('dataset\measured_active_power.csv').values
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/54:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
33/55: df1
33/56:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
33/57: df1
33/58:
dataset3 = pd.read_csv('dataset/measured_reactive_power.csv', encoding= 'utf-8').values
dataset4 = pd.read_csv('dataset/measured_active_power.csv').values
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/59:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
33/60: df1
33/61:
dataset3 = pd.read_csv('dataset/measured_reactive_power.csv', header = None).values
dataset4 = pd.read_csv('dataset/measured_active_power.csv').values
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv').values
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv').values
33/62:
dataset3 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset4 = pd.read_csv('dataset/measured_active_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
33/63:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
33/64: df1
33/65:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
33/66: df1.columns
33/67: df2.columns
33/68:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
33/69:
dataset3 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset4 = pd.read_csv('dataset/measured_active_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
33/70:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
33/71:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
33/72: df1
33/73:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
33/74: df2.columns
33/75:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
33/76:
combined_df = pd.concat([df1, df2, df3, df4], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
34/1: dataset1.shape
34/2:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
34/3:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
34/4:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
34/5: dataset1.shape
34/6:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
34/7:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
34/8:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
34/9: df2.shape
34/10: df2.columns
34/11:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
34/12:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
34/13:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
34/14:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
34/15: df2.columns
34/16: df2.shape
34/17: df1.shape
34/18: df3.shape
34/19: df4.shape
34/20:
combined_df = pd.concat([df1, df2, df3, df4], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
34/21: combined_df. shape
34/22: phase1_df = combined_df.filter(regex='.1$')
34/23: phase1_df.shape
34/24: phase1_df.head
34/25:
phase1_df = combined_df.filter(regex='.1$')
phase2_df = combined_df.filter(regex='.2$')
phase3_df = combined_df.filter(regex='.3$')
34/26: type(phase1_df)
34/27:
incomplete_sets_phase1 = find_existing_incomplete_columns(phase1_df)
incomplete_sets_phase2 = find_existing_incomplete_columns(phase2_df)
incomplete_sets_phase3 = find_existing_incomplete_columns(phase3_df)
34/28:
def find_existing_incomplete_columns(df):
    # Extract the base names without the prefixes
    base_names = set([col.split('_', 1)[1] for col in df.columns if '_' in col])
    
    incomplete_sets = []

    # Iterate through the base names and check for the presence of P_, Q_, V_, and d_ columns
    for base_name in base_names:
        existing_columns = []
        
        # Check if P_, Q_, V_, and d_ versions exist
        if f'P_{base_name}' in df.columns:
            existing_columns.append(f'P_{base_name}')
        if f'Q_{base_name}' in df.columns:
            existing_columns.append(f'Q_{base_name}')
        if f'V_{base_name}' in df.columns:
            existing_columns.append(f'V_{base_name}')
        if f'd_{base_name}' in df.columns:
            existing_columns.append(f'd_{base_name}')
        
        # If not all four columns are present, add the existing ones to the result
        if len(existing_columns) < 4:
            incomplete_sets.append(existing_columns)
    
    return incomplete_sets
34/29:
incomplete_sets_phase1 = find_existing_incomplete_columns(phase1_df)
incomplete_sets_phase2 = find_existing_incomplete_columns(phase2_df)
incomplete_sets_phase3 = find_existing_incomplete_columns(phase3_df)
34/30: incomplete_sets_phase1
34/31: incomplete_sets_phase2
34/32: incomplete_sets_phase3
34/33:
def drop_incomplete_columns(df, incomplete_columns_list):
    # Flatten the list of incomplete columns (because it's a list of lists)
    columns_to_drop = [col for sublist in incomplete_columns_list for col in sublist]
    
    # Drop the columns from the DataFrame
    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')
    
    return df_cleaned
34/34: len(incomplete_sets_phase1)
34/35: len(incomplete_sets_phase2)
34/36:
cleaned_phase1 = drop_incomplete_columns(phase1_df, incomplete_sets_phase1)
cleaned_phase2 = drop_incomplete_columns(phase2_df, incomplete_sets_phase2)
cleaned_phase3 = drop_incomplete_columns(phase1_df, incomplete_sets_phase3)
34/37: cleaned_phase1.shape
34/38: cleaned_phase2.shape
34/39: cleaned_phase3.shape
34/40:
percentage = 100
phase1_dataset = slice_dataset(cleaned_phase1, percentage)
34/41:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.extend(dataset[i, 4*n:4*n+2])
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])
        x_raw.append(x_raw_1)
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []
        
    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
34/42:
percentage = 100
phase1_dataset = slice_dataset(cleaned_phase1, percentage)
34/43:
percentage = 100
n_bus = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage)
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase1, y_raw_phase1)

#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage)
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase2, y_raw_phase2)

#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage)
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase3, y_raw_phase3)
34/44: type(phase1_dataset)
34/45:
percentage = 100
n_bus = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase1, y_raw_phase1)

#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase2, y_raw_phase2)

#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase3, y_raw_phase3)
34/46: type(phase1_dataset)
34/47:
percentage = 100
n_bus = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase1, y_raw_phase1)

#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase2, y_raw_phase2)

#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus)
x_norm_test, y_norm_test, _, _, _, _ = normalize_dataset(x_raw_phase3, y_raw_phase3)
34/48:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/49:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/50:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/51:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/52: %pip install scikit-learn
34/53:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/54:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase2, y_raw_phase2, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/55:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train, X_temp, y_train, y_temp = train_test_split(x_raw_phase3, y_raw_phase3, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/56:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/57:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p2, X_temp_p2, y_train_p2, y_temp_p2 = train_test_split(x_raw_phase2, y_raw_phase2, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p2, X_test_p2, y_val_p2, y_test_p2 = train_test_split(X_temp_p2, y_temp_p2, test_size=0.5, random_state=42)
34/58:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p2, X_temp_p2, y_train_p2, y_temp_p2 = train_test_split(x_raw_phase2, y_raw_phase2, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p2, X_test_p2, y_val_p2, y_test_p2 = train_test_split(X_temp_p2, y_temp_p2, test_size=0.5, random_state=42)
34/59:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p3, X_temp_p3, y_train_p3, y_temp_p3 = train_test_split(x_raw_phase3, y_raw_phase3, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p3, X_test_p3, y_val_p3, y_test_p3 = train_test_split(X_temp_p3, y_temp_p3, test_size=0.5, random_state=42)
34/60:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)

#Phase2

x_norm_phase2_train, y_norm_phase2_train, _, _, _, _ = normalize_dataset(X_train_p2, y_train_p2)
x_norm_phase2_val, y_norm_phase2_val, x_phase2_val_mean, y_phase2_val_mean, x_phase2_val_std, y_phase2_val_std = normalize_dataset(X_val_p2, y_val_p2)

#Phase3

x_norm_phase3_train, y_norm_phase3_train, _, _, _, _ = normalize_dataset(X_train_p3, y_train_p3)
x_norm_phase3_val, y_norm_phase3_val, x_phase3_val_mean, y_phase3_val_mean, x_phase3_val_std, y_phase3_val_std = normalize_dataset(X_val_p3, y_val_p3)
34/61:
%%time

input_size = n_bus*2
hidden_size1 = 146
hidden_size2 = 146
output_size = n_bus*2
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train__p1_loss_list, val__p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train__p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/62:
class My_NN(torch.nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None):
        super(My_NN, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = Linear(self.input_size, self.hidden_size1)
        self.lin2 = Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = Linear(self.hidden_size2, self.output_size)

    def forward(self, x):
        
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)
        x = torch.tanh(x)

        x = self.lin3(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
34/63:
class My_NN(torch.nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None):
        super(My_NN, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = Linear(self.input_size, self.hidden_size1)
        self.lin2 = Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = Linear(self.hidden_size2, self.output_size)

    def forward(self, x):
        
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)
        x = torch.tanh(x)

        x = self.lin3(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
34/64:
%%time

input_size = n_bus*2
hidden_size1 = 146
hidden_size2 = 146
output_size = n_bus*2
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train__p1_loss_list, val__p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train__p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/65: x_norm_phase1_train.shape
34/66:
%%time

input_size = n_bus*2
hidden_size1 = 136
hidden_size2 = 136
output_size = n_bus*2
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train__p1_loss_list, val__p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train__p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/67:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = n_bus*2
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train__p1_loss_list, val__p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train__p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/68: y_norm_phase1_train.shape
34/69: x_phase1_val_mean
34/70: x_phase1_val_mean.shape
34/71: y_norm_phase1_train.shape
34/72: y_phase1_val_mean.shape
34/73: y_phase1_val_std.shape
34/74:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train__p1_loss_list, val__p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train__p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/75:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train__p1_loss_list, val__p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/76:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_val, y_val_mean, y_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/77:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val__p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/78:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/79:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/80:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/81:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.0001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/82:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.003

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/83:
import torch
import torch.nn as nn

class My_NN_new(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None, dropout_rate=0.5):
        super(My_NN_new, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.output_size)
        
        # Initialize dropout layers
        self.dropout1 = nn.Dropout(p=dropout_rate)  # Dropout after the first layer
        self.dropout2 = nn.Dropout(p=dropout_rate)  # Dropout after the second layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.tanh(x)
        x = self.dropout1(x)  # Apply dropout after activation

        x = self.lin2(x)
        x = torch.tanh(x)
        x = self.dropout2(x)  # Apply dropout after activation

        x = self.lin3(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model.state_dict(), name)  # Save model weights
34/84:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/85:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.0001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/86: x_norm_phase1_val
34/87: X_train_p1
34/88: x_raw_phase1
34/89: phase1_dataset
34/90: cleaned_phase1
34/91: phase1_dataset = slice_dataset(cleaned_phase1, percentage)
34/92: phase1_dataset
34/93: phase1_dataset.shape
34/94: phase2_dataset.shape
34/95: phase3_dataset.shape
34/96: phase2_dataset.shape
34/97:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
34/98:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/99:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p2, X_temp_p2, y_train_p2, y_temp_p2 = train_test_split(x_raw_phase2, y_raw_phase2, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p2, X_test_p2, y_val_p2, y_test_p2 = train_test_split(X_temp_p2, y_temp_p2, test_size=0.5, random_state=42)
34/100:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p3, X_temp_p3, y_train_p3, y_temp_p3 = train_test_split(x_raw_phase3, y_raw_phase3, test_size=0.3, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p3, X_test_p3, y_val_p3, y_test_p3 = train_test_split(X_temp_p3, y_temp_p3, test_size=0.5, random_state=42)
34/101:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)

#Phase2

x_norm_phase2_train, y_norm_phase2_train, _, _, _, _ = normalize_dataset(X_train_p2, y_train_p2)
x_norm_phase2_val, y_norm_phase2_val, x_phase2_val_mean, y_phase2_val_mean, x_phase2_val_std, y_phase2_val_std = normalize_dataset(X_val_p2, y_val_p2)

#Phase3

x_norm_phase3_train, y_norm_phase3_train, _, _, _, _ = normalize_dataset(X_train_p3, y_train_p3)
x_norm_phase3_val, y_norm_phase3_val, x_phase3_val_mean, y_phase3_val_mean, x_phase3_val_std, y_phase3_val_std = normalize_dataset(X_val_p3, y_val_p3)
34/102:
import torch
import torch.nn as nn

class My_NN_new(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, output_size=None, dropout_rate=0.5):
        super(My_NN_new, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.output_size)
        
        # Initialize dropout layers
        self.dropout1 = nn.Dropout(p=dropout_rate)  # Dropout after the first layer
        self.dropout2 = nn.Dropout(p=dropout_rate)  # Dropout after the second layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.tanh(x)
        x = self.dropout1(x)  # Apply dropout after activation

        x = self.lin2(x)
        x = torch.tanh(x)
        x = self.dropout2(x)  # Apply dropout after activation

        x = self.lin3(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model.state_dict(), name)  # Save model weights
34/103:
%%time

input_size = 136
hidden_size1 = 146
hidden_size2 = 146
output_size = 136
lr = 0.0001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/104:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 272
output_size = 136
lr = 0.0001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/105:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
output_size = 136
lr = 0.0001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/106:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
output_size = 136
lr = 0.001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/107:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/108:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
34/109:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
output_size = 136
lr = 0.001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/110:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
output_size = 136
lr = 0.001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/111:
import torch
import torch.nn as nn

class My_NN_new(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, hidden_size3=None, output_size=None, dropout_rate=0.5):
        super(My_NN_new, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.hidden_size3 = hidden_size3 if hidden_size3 is not None else 38  # New hidden layer size
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.hidden_size3)  # New hidden layer
        self.lin4 = nn.Linear(self.hidden_size3, self.output_size)  # Output layer
        
        # Initialize dropout layers
        self.dropout1 = nn.Dropout(p=dropout_rate)  # Dropout after the first layer
        self.dropout2 = nn.Dropout(p=dropout_rate)  # Dropout after the second layer
        self.dropout3 = nn.Dropout(p=dropout_rate)  # Dropout after the third layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.tanh(x)
        x = self.dropout1(x)  # Apply dropout after activation

        x = self.lin2(x)
        x = torch.tanh(x)
        x = self.dropout2(x)  # Apply dropout after activation

        x = self.lin3(x)  # Pass through the third layer
        x = torch.tanh(x)
        x = self.dropout3(x)  # Apply dropout after activation

        x = self.lin4(x)  # Output layer

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
34/112:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
output_size = 136
lr = 0.001

model = My_NN_new(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/113:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.001

model = My_NN_new(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/114:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_new(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/115:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/116:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
#hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/117:
import torch
import torch.nn as nn

class My_NN_1(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, hidden_size3=None, output_size=None, dropout_rate=0.5):
        super(My_NN_new, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.hidden_size3 = hidden_size3 if hidden_size3 is not None else 38  # New hidden layer size
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.hidden_size3)  # New hidden layer
        self.lin4 = nn.Linear(self.hidden_size3, self.output_size)  # Output layer
        
        # Initialize dropout layers
        self.dropout1 = nn.Dropout(p=dropout_rate)  # Dropout after the first layer
        self.dropout2 = nn.Dropout(p=dropout_rate)  # Dropout after the second layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        x = self.dropout1(x)  # Apply dropout after activation

        x = self.lin2(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        x = self.dropout2(x)  # Apply dropout after activation

        x = self.lin3(x)  # Pass through the third layer
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin4(x)  # Output layer

        return x
34/118:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_1(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/119:
import torch
import torch.nn as nn

class My_NN_1(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, hidden_size3=None, output_size=None, dropout_rate=0.5):
        super(My_NN_1, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.hidden_size3 = hidden_size3 if hidden_size3 is not None else 38  # New hidden layer size
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.hidden_size3)  # New hidden layer
        self.lin4 = nn.Linear(self.hidden_size3, self.output_size)  # Output layer
        
        # Initialize dropout layers
        self.dropout1 = nn.Dropout(p=dropout_rate)  # Dropout after the first layer
        self.dropout2 = nn.Dropout(p=dropout_rate)  # Dropout after the second layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        x = self.dropout1(x)  # Apply dropout after activation

        x = self.lin2(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        x = self.dropout2(x)  # Apply dropout after activation

        x = self.lin3(x)  # Pass through the third layer
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin4(x)  # Output layer

        return x
34/120:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_1(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/121:
import torch
import torch.nn as nn

class My_NN_1(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, hidden_size3=None, output_size=None, dropout_rate=0.5):
        super(My_NN_1, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.hidden_size3 = hidden_size3 if hidden_size3 is not None else 38  # New hidden layer size
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.hidden_size3)  # New hidden layer
        self.lin4 = nn.Linear(self.hidden_size3, self.output_size)  # Output layer
        
        # Initialize dropout layers
        self.dropout1 = nn.Dropout(p=dropout_rate)  # Dropout after the first layer
        self.dropout2 = nn.Dropout(p=dropout_rate)  # Dropout after the second layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        x = self.dropout1(x)  # Apply dropout after activation

        x = self.lin2(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        x = self.dropout2(x)  # Apply dropout after activation

        x = self.lin3(x)  # Pass through the third layer
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin4(x)  # Output layer

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
34/122:
%%time

input_size = 136
hidden_size1 = 272
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_1(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/123:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 272
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_1(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/124:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 136
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_1(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/125:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 136
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_2(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/126:
import torch
import torch.nn as nn

class My_NN_2(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, hidden_size3=None, output_size=None, dropout_rate=0.5):
        super(My_NN_2, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.hidden_size3 = hidden_size3 if hidden_size3 is not None else 38  # New hidden layer size
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.hidden_size3)  # New hidden layer
        self.lin4 = nn.Linear(self.hidden_size3, self.output_size)  # Output layer
        
        # Initialize a single dropout layer
        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout after the second layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin2(x)
        x = torch.relu(x)  # Changed activation function to ReLU
        
        x = self.dropout(x)  # Apply dropout after the second layer

        x = self.lin3(x)  # Pass through the third layer
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin4(x)  # Output layer

        return x
    
    def save_weights(self, model, name):
        torch.save(model.state_dict(), name)
34/127:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 136
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_2(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/128:
import torch
import torch.nn as nn

class My_NN_3(nn.Module):
    def __init__(self, input_size=None, hidden_size1=None, hidden_size2=None, hidden_size3=None, output_size=None):
        super(My_NN_3, self).__init__()
        self.input_size = input_size if input_size is not None else 18 
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 38
        self.hidden_size2 = hidden_size2 if hidden_size2 is not None else 38
        self.hidden_size3 = hidden_size3 if hidden_size3 is not None else 38  # New hidden layer size
        self.output_size = output_size if output_size is not None else 18
        
        self.lin1 = nn.Linear(self.input_size, self.hidden_size1)
        self.lin2 = nn.Linear(self.hidden_size1, self.hidden_size2)
        self.lin3 = nn.Linear(self.hidden_size2, self.hidden_size3)  # New hidden layer
        self.lin4 = nn.Linear(self.hidden_size3, self.output_size)  # Output layer

    def forward(self, x):
        x = self.lin1(x)
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin2(x)
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin3(x)  # Pass through the third layer
        x = torch.relu(x)  # Changed activation function to ReLU

        x = self.lin4(x)  # Output layer

        return x
    
    def save_weights(self, model, name):
        torch.save(model.state_dict(), name)
34/129:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 136
hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN_3(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/130:
%%time

input_size = 136
hidden_size1 = 136
hidden_size2 = 136
#hidden_size3 = 272
output_size = 136
lr = 0.0001

model = My_NN(input_size, hidden_size1, hidden_size2, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count=0
patience=10000
lossMin = 1e10

for epoch in range(10001):

    model.train()
    optimizer.zero_grad()
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
    val_p1_loss_list.append(val_p1_loss.detach())

    #early stopping
    if (val_p1_loss < lossMin):
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    #if (train_loss <= 0):
    #    print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
    #    break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/131:
import torch
import torch.nn as nn

# Assuming My_NN class is already defined as per your previous code

# Define hyperparameters
input_size = 136
hidden_size1 = 136
hidden_size2 = 136
output_size = 136
hidden_size3 = 272
lr = 0.0001
l2_lambda = 0.01  # Regularization coefficient

# Initialize model and optimizer
model = My_NN(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count = 0
patience = 10000
lossMin = 1e10

for epoch in range(10001):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), 
                        denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    
    # Calculate L2 regularization
    l2_norm = sum(param.pow(2).sum() for param in model.parameters())
    train_p1_loss += l2_lambda * l2_norm  # Add L2 regularization term

    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), 
                      denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))

    val_p1_loss_list.append(val_p1_loss.detach())

    # Early stopping
    if val_p1_loss < lossMin:
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count += 1
        if count > patience:
            print("Early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("Best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/132:
import torch
import torch.nn as nn

# Assuming My_NN class is already defined as per your previous code

# Define hyperparameters
input_size = 136
hidden_size1 = 136
hidden_size2 = 136
output_size = 136
hidden_size3 = 272
lr = 0.0001
l2_lambda = 0.01  # Regularization coefficient

# Initialize model and optimizer
model = My_NN_3(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count = 0
patience = 10000
lossMin = 1e10

for epoch in range(10001):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), 
                        denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    
    # Calculate L2 regularization
    l2_norm = sum(param.pow(2).sum() for param in model.parameters())
    train_p1_loss += l2_lambda * l2_norm  # Add L2 regularization term

    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), 
                      denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))

    val_p1_loss_list.append(val_p1_loss.detach())

    # Early stopping
    if val_p1_loss < lossMin:
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count += 1
        if count > patience:
            print("Early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("Best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/133:
import torch
import torch.nn as nn

# Assuming My_NN class is already defined as per your previous code

# Define hyperparameters
input_size = 136
hidden_size1 = 136
hidden_size2 = 136
output_size = 136
hidden_size3 = 272
lr = 0.0001
l2_lambda = 0.01  # Regularization coefficient

# Initialize model and optimizer
model = My_NN_2(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_p1_loss_list, val_p1_loss_list = [], []

count = 0
patience = 10000
lossMin = 1e10

for epoch in range(10001):
    model.train()
    optimizer.zero_grad()
    
    # Forward pass
    y_train_p1_prediction = model(x_norm_phase1_train)
    train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), 
                        denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
    
    # Calculate L2 regularization
    l2_norm = sum(param.pow(2).sum() for param in model.parameters())
    train_p1_loss += l2_lambda * l2_norm  # Add L2 regularization term

    train_p1_loss.backward()
    optimizer.step()
    train_p1_loss_list.append(train_p1_loss.detach())

    model.eval()
    y_val_p1_prediction = model(x_norm_phase1_val)
    val_p1_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), 
                      denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))

    val_p1_loss_list.append(val_p1_loss.detach())

    # Early stopping
    if val_p1_loss < lossMin:
        lossMin = val_p1_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_p1_loss
        best_val_loss = val_p1_loss
        model.save_weights(model, "Best_NN_model_p1.pt")
    else:
        count += 1
        if count > patience:
            print("Early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_p1_loss, val_p1_loss))
            print("Best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_p1_loss, val_p1_loss))
34/134: x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)
34/135: x_raw_phase1
34/136:
x_raw_1, y_raw_1 = [], []
x_raw, y_raw = [], []

for i in range(len(phase1_dataset)):
        for n in range(n_bus):
            x_raw_1.extend(phase1_dataset[i, 4*n:4*n+2])
            y_raw_1.extend(phase1_dataset[i, 4*n+2:4*n+4])
34/137: x_raw_1
34/138: y_raw_1
34/139: type(y_raw_1)
34/140:


for i in range(len(phase1_dataset)):
        for n in range(n_bus):
            x_raw_1 = phase1_dataset[i, 4*n:4*n+2]
            y_raw_1 = phase1_dataset[i, 4*n+2:4*n+4]
34/141: type(y_raw_1)
34/142:


for i in range(len(phase1_dataset)):
        for n in range(n_bus):
            x_raw_1 = phase1_dataset.loc[i, 4*n:4*n+2]
            y_raw_1 = phase1_dataset[i, 4*n+2:4*n+4]
34/143: type(phase1_dataset)
34/144: phase1_dataset
34/145: cleaned_phase1
34/146: new_df = pd.DataFrame()
34/147:
new_df = pd.DataFrame()

for i in range(len(cleaned_phase1)):
    for n in range(68):
        new_df = cleaned_phase1[i, 4*n:4*n+2]
34/148:
new_df = pd.DataFrame()

for i in range(len(cleaned_phase1)):
    for n in range(72):
        new_df = cleaned_phase1[i, 4*n:4*n+2]
34/149: cleaned_phase1.shape
34/150:
new_df = pd.DataFrame()

for i in range(len(cleaned_phase1)):
    for n in range(72):
         x_raw = cleaned_phase1.iloc[i, 4*n:4*n+2]
34/151: x_raw
34/152: type(x_raw)
34/153:
new_df = pd.DataFrame()

for i in range(len(cleaned_phase1)):
    for n in range(72):
        x_raw = cleaned_phase1.iloc[i, 4*n:4*n+2]

        # Convert the extracted data (with column names) into a DataFrame and append it to new_df
        new_df = pd.concat([new_df, pd.DataFrame([x_raw])], ignore_index=True)
34/154:
new_df = pd.DataFrame()

for i in range(len(cleaned_phase1)):
    for n in range(72):
        x_raw = cleaned_phase1.iloc[i, 4*n:4*n+2]

        # Convert the extracted data (with column names) into a DataFrame and append it to new_df
        new_df = pd.concat([new_df, pd.DataFrame([x_raw])], ignore_index=True)
34/155:

# Create a new DataFrame to store the results
new_df = pd.DataFrame()

for i in range(len(cleaned_phase1)):
    for n in range(68):
        # Extract the rows with original column names
        x_raw = cleaned_phase1.iloc[i, 4*n:4*n+2]
        
        # Convert the extracted data (with column names) into a DataFrame and append it to new_df
        new_df = pd.concat([new_df, pd.DataFrame([x_raw])], ignore_index=True)

# Display the resulting DataFrame
print(new_df)
34/156: new_df
35/1:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
35/2:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
35/3:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
35/4:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
35/5:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
35/6:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
35/7:
combined_df = pd.concat([df1, df2, df3, df4], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
35/8:
phase1_df = combined_df.filter(regex='.1$')
phase2_df = combined_df.filter(regex='.2$')
phase3_df = combined_df.filter(regex='.3$')
35/9:
def find_existing_incomplete_columns(df):
    # Extract the base names without the prefixes
    base_names = set([col.split('_', 1)[1] for col in df.columns if '_' in col])
    
    incomplete_sets = []

    # Iterate through the base names and check for the presence of P_, Q_, V_, and d_ columns
    for base_name in base_names:
        existing_columns = []
        
        # Check if P_, Q_, V_, and d_ versions exist
        if f'P_{base_name}' in df.columns:
            existing_columns.append(f'P_{base_name}')
        if f'Q_{base_name}' in df.columns:
            existing_columns.append(f'Q_{base_name}')
        if f'V_{base_name}' in df.columns:
            existing_columns.append(f'V_{base_name}')
        if f'd_{base_name}' in df.columns:
            existing_columns.append(f'd_{base_name}')
        
        # If not all four columns are present, add the existing ones to the result
        if len(existing_columns) < 4:
            incomplete_sets.append(existing_columns)
    
    return incomplete_sets
35/10:
incomplete_sets_phase1 = find_existing_incomplete_columns(phase1_df)
incomplete_sets_phase2 = find_existing_incomplete_columns(phase2_df)
incomplete_sets_phase3 = find_existing_incomplete_columns(phase3_df)
35/11:
def drop_incomplete_columns(df, incomplete_columns_list):
    # Flatten the list of incomplete columns (because it's a list of lists)
    columns_to_drop = [col for sublist in incomplete_columns_list for col in sublist]
    
    # Drop the columns from the DataFrame
    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')
    
    return df_cleaned
35/12:
cleaned_phase1 = drop_incomplete_columns(phase1_df, incomplete_sets_phase1)
cleaned_phase2 = drop_incomplete_columns(phase2_df, incomplete_sets_phase2)
cleaned_phase3 = drop_incomplete_columns(phase1_df, incomplete_sets_phase3)
35/13:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
35/14:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n+1], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
35/15:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
35/16:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
35/17:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)
35/18:
def generate_edge_index(n_nodes):
    edge_index = []
    for i in range(n_nodes - 1):
        # Connect each node to the next one (you can modify the logic to suit your graph structure)
        edge_index.append([i, i + 1])  # Connect node i to i+1
        edge_index.append([i + 1, i])  # Connect node i+1 to i (bidirectional)
    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # Transpose and return
35/19:
import torch
from torch_geometric.data import Data, DataLoader

# Assuming x_train, y_train, x_val, y_val are already defined
x_train, y_train = x_norm_phase1_train, y_norm_phase1_train
x_val, y_val = x_norm_phase1_val, y_norm_phase1_val

data_train_list, data_val_list = [], []

# Generate Data for training
for i in range(len(x_train)):
    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically
    edge_index = generate_edge_index(n_nodes)  # Generate dynamic edge_index
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))

# Generate Data for validation
for i in range(len(x_val)):
    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically
    edge_index = generate_edge_index(n_nodes)  # Generate dynamic edge_index
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

# DataLoaders
train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
35/20:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
35/21:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
35/22:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
35/23:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
35/24:
%%time

feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2001):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
35/25:
%%time

feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2001):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction,y_phase1_val_mean, y_phase1_val_std), denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
35/26:
%%time

feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2001):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction,y_phase1_val_mean, y_phase1_val_std), denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
35/27: y_val_prediction.shape
35/28: batch.shape
35/29:
 for batch in val_loader:
        y_val_prediction = model(batch)
35/30: y_val_prediction.shape
35/31: y_phase1_val_mean.shape
35/32: y_phase1_val_std.shape
35/33: y_val_p1.shape
35/34: x_val_p1.shape
35/35: X_val_p1.shape
35/36: x_raw_phase1.shape
35/37: y_raw_phase1.shape
35/38: y_raw_phase1.shape
35/39:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
35/40:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
35/41:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
35/42:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
35/43:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
35/44:
combined_df = pd.concat([df1, df2, df3, df4], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
35/45:
phase1_df = combined_df.filter(regex='.1$')
phase2_df = combined_df.filter(regex='.2$')
phase3_df = combined_df.filter(regex='.3$')
35/46:
def find_existing_incomplete_columns(df):
    # Extract the base names without the prefixes
    base_names = set([col.split('_', 1)[1] for col in df.columns if '_' in col])
    
    incomplete_sets = []

    # Iterate through the base names and check for the presence of P_, Q_, V_, and d_ columns
    for base_name in base_names:
        existing_columns = []
        
        # Check if P_, Q_, V_, and d_ versions exist
        if f'P_{base_name}' in df.columns:
            existing_columns.append(f'P_{base_name}')
        if f'Q_{base_name}' in df.columns:
            existing_columns.append(f'Q_{base_name}')
        if f'V_{base_name}' in df.columns:
            existing_columns.append(f'V_{base_name}')
        if f'd_{base_name}' in df.columns:
            existing_columns.append(f'd_{base_name}')
        
        # If not all four columns are present, add the existing ones to the result
        if len(existing_columns) < 4:
            incomplete_sets.append(existing_columns)
    
    return incomplete_sets
35/47:
incomplete_sets_phase1 = find_existing_incomplete_columns(phase1_df)
incomplete_sets_phase2 = find_existing_incomplete_columns(phase2_df)
incomplete_sets_phase3 = find_existing_incomplete_columns(phase3_df)
35/48:
def drop_incomplete_columns(df, incomplete_columns_list):
    # Flatten the list of incomplete columns (because it's a list of lists)
    columns_to_drop = [col for sublist in incomplete_columns_list for col in sublist]
    
    # Drop the columns from the DataFrame
    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')
    
    return df_cleaned
35/49:
cleaned_phase1 = drop_incomplete_columns(phase1_df, incomplete_sets_phase1)
cleaned_phase2 = drop_incomplete_columns(phase2_df, incomplete_sets_phase2)
cleaned_phase3 = drop_incomplete_columns(phase1_df, incomplete_sets_phase3)
35/50:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
35/51: y_raw_phase1.shape
35/52:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
35/53:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)
35/54:
def generate_edge_index(n_nodes):
    edge_index = []
    for i in range(n_nodes - 1):
        # Connect each node to the next one (you can modify the logic to suit your graph structure)
        edge_index.append([i, i + 1])  # Connect node i to i+1
        edge_index.append([i + 1, i])  # Connect node i+1 to i (bidirectional)
    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # Transpose and return
35/55:
import torch
from torch_geometric.data import Data, DataLoader

# Assuming x_train, y_train, x_val, y_val are already defined
x_train, y_train = x_norm_phase1_train, y_norm_phase1_train
x_val, y_val = x_norm_phase1_val, y_norm_phase1_val

data_train_list, data_val_list = [], []

# Generate Data for training
for i in range(len(x_train)):
    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically
    edge_index = generate_edge_index(n_nodes)  # Generate dynamic edge_index
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))

# Generate Data for validation
for i in range(len(x_val)):
    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically
    edge_index = generate_edge_index(n_nodes)  # Generate dynamic edge_index
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

# DataLoaders
train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
35/56:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
35/57:
%%time

feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2001):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction,y_phase1_val_mean, y_phase1_val_std), denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
34/157:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Train output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_prediction = model(x_norm_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Train output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(val_p1_loss))
34/158:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Train output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Train output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Train output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Train loss (MSE): {:.7f}'.format(val_p1_loss))
34/159:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/160:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/161:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_val)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/162:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_std, y_phase1_val_mean), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/163:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(val_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/164:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/165:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/166:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/167:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_val)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
34/168:
model.eval()

y_train_p1_prediction = model(x_norm_phase1_train)
train_p1_loss = MSE(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std), denormalize_output(y_norm_phase1_train, y_phase1_val_mean, y_phase1_val_std))
print("Model1: Test output ground-truth: \n" + str(y_train_p1.detach().numpy()[0]))
print("Model1: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model1: Test loss (MSE): {:.7f}'.format(train_p1_loss))

print("===========================================================================")

y_val_p1_prediction = model(x_norm_phase1_val)
val_loss = MSE(denormalize_output(y_val_p1_prediction, y_phase1_val_std, y_phase1_val_mean), denormalize_output(y_norm_phase1_val, y_phase1_val_mean, y_phase1_val_std))
print("Model2: Test output ground-truth: \n" + str(y_val_p1.detach().numpy()[0]))
print("Model2: Test output prediction: \n" + str(denormalize_output(y_train_p1_prediction, y_phase1_val_mean, y_phase1_val_std).detach().numpy()[0]))
print('Model2: Test loss (MSE): {:.7f}'.format(val_p1_loss))
35/58:
import pandas as pd
import torch

# Step 1: Read the CSV file into a pandas DataFrame
df = pd.read_csv('C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv', header=None)

# Step 2: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 3: Iterate through the DataFrame to extract edges
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(i)
            target_nodes.append(j)

# Step 4: Create the edge_index tensor
edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)

print(edge_index)
35/59: df
35/60: edge_index
35/61: source_nodes
35/62: target_nodes
35/63: df.iloc[2,3]
35/64:
import pandas as pd
import torch

# Step 1: Read the CSV file into a pandas DataFrame
df = pd.read_csv('C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv', header=None)

# Step 2: Extract row and column names (node names)
node_names = list(df.columns)

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(node_names[i])  # append the source node name
            target_nodes.append(node_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(node_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

print(edge_index)
35/65: df.iloc[2,3]
35/66: df
35/67:
import pandas as pd
import torch

# Step 1: Read the CSV file into a pandas DataFrame
df = pd.read_csv('C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv', header=None)

# Step 2: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 3: Iterate through the DataFrame to extract edges
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(i)
            target_nodes.append(j)

# Step 4: Create the edge_index tensor
edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)

print(edge_index)
35/68: df
35/69: df.columns
35/70:
import pandas as pd
import torch

# Step 1: Read the CSV file into a pandas DataFrame
df = pd.read_csv('C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv')

# Step 2: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 3: Iterate through the DataFrame to extract edges
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(i)
            target_nodes.append(j)

# Step 4: Create the edge_index tensor
edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)

print(edge_index)
35/71: df.columns
35/72:
import pandas as pd
import torch

# Step 1: Read the CSV file, ignoring the first cell (A1), and set the first column as the index (row names)
df = pd.read_csv('edges_matrix.csv', index_col=0)

# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)
row_names = df.index.tolist()    # Row names (from the first column)
col_names = df.columns.tolist()  # Column names (from the first row)

# Ensure that the row and column names match (adjacency matrix should be square)
assert row_names == col_names, "Row names and column names must match in an adjacency matrix."

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(row_names[i])  # append the source node name
            target_nodes.append(col_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(row_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

print(edge_index)
35/73:
import pandas as pd
import torch

# File path (use raw string format to handle backslashes in Windows paths)
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

# Step 1: Read the CSV file, ignoring the first cell (A1), and set the first column as the index (row names)
df = pd.read_csv(file_path, index_col=0)

# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)
row_names = df.index.tolist()    # Row names (from the first column)
col_names = df.columns.tolist()  # Column names (from the first row)

# Ensure that the row and column names match (adjacency matrix should be square)
assert row_names == col_names, "Row names and column names must match in an adjacency matrix."

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(row_names[i])  # append the source node name
            target_nodes.append(col_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(row_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

# Print the edge index to verify the result
print(edge_index)
35/74: df
35/75: nvidia-smi
35/76: !pip install tensorflow-gpu
35/77: python -version
35/78: !python -version
35/79: import sys
35/80: print(sys.version)
35/81: !pip install tensorflow-gpu
35/82: !pip install tensorflow-gpu
35/83: ! pip install tensorflow-gpu
35/84: ! python -m pip install --upgrade pip setuptools
35/85: ! pip install tensorflow-gpu==2.12.0
35/86: ! pip freeze
35/87: ! pip install tensorflow
35/88: ! pip install tensorflow-gpu
35/89: ! pip install tensorflow-gpu== 2.6.0
35/90: !pip install tensorflow-gpu==2.6.0
35/91: !pip install tensorflow-gpu==2.12.0
38/1: !pip install tensorflow-gpu==2.12.0
38/2: !pip install --upgrade --force-reinstall setuptools
38/3: !pip install tensorflow-gpu==2.12.0
38/4:
import torch

# Check if a GPU is available and set the device accordingly
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1 * 2
lr = 0.0001

# Move the model to the GPU
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size).to(device)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # Move data to the GPU
        batch = batch.to(device)
        
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # Move data to the GPU
            batch = batch.to(device)
            
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")

# CPU and GPU usage will automatically be managed based on device setup.
38/5:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
38/6:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
38/7:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
38/8:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
38/9:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
38/10:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
38/11:
combined_df = pd.concat([df1, df2, df3, df4], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
38/12:
phase1_df = combined_df.filter(regex='.1$')
phase2_df = combined_df.filter(regex='.2$')
phase3_df = combined_df.filter(regex='.3$')
38/13:
def find_existing_incomplete_columns(df):
    # Extract the base names without the prefixes
    base_names = set([col.split('_', 1)[1] for col in df.columns if '_' in col])
    
    incomplete_sets = []

    # Iterate through the base names and check for the presence of P_, Q_, V_, and d_ columns
    for base_name in base_names:
        existing_columns = []
        
        # Check if P_, Q_, V_, and d_ versions exist
        if f'P_{base_name}' in df.columns:
            existing_columns.append(f'P_{base_name}')
        if f'Q_{base_name}' in df.columns:
            existing_columns.append(f'Q_{base_name}')
        if f'V_{base_name}' in df.columns:
            existing_columns.append(f'V_{base_name}')
        if f'd_{base_name}' in df.columns:
            existing_columns.append(f'd_{base_name}')
        
        # If not all four columns are present, add the existing ones to the result
        if len(existing_columns) < 4:
            incomplete_sets.append(existing_columns)
    
    return incomplete_sets
38/14:
incomplete_sets_phase1 = find_existing_incomplete_columns(phase1_df)
incomplete_sets_phase2 = find_existing_incomplete_columns(phase2_df)
incomplete_sets_phase3 = find_existing_incomplete_columns(phase3_df)
38/15:
def drop_incomplete_columns(df, incomplete_columns_list):
    # Flatten the list of incomplete columns (because it's a list of lists)
    columns_to_drop = [col for sublist in incomplete_columns_list for col in sublist]
    
    # Drop the columns from the DataFrame
    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')
    
    return df_cleaned
38/16:
cleaned_phase1 = drop_incomplete_columns(phase1_df, incomplete_sets_phase1)
cleaned_phase2 = drop_incomplete_columns(phase2_df, incomplete_sets_phase2)
cleaned_phase3 = drop_incomplete_columns(phase1_df, incomplete_sets_phase3)
38/17:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/18:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
38/19:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/20:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
38/21:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)
38/22:
def generate_edge_index(n_nodes):
    edge_index = []
    for i in range(n_nodes - 1):
        # Connect each node to the next one (you can modify the logic to suit your graph structure)
        edge_index.append([i, i + 1])  # Connect node i to i+1
        edge_index.append([i + 1, i])  # Connect node i+1 to i (bidirectional)
    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()  # Transpose and return
38/23:
import torch
from torch_geometric.data import Data, DataLoader

# Assuming x_train, y_train, x_val, y_val are already defined
x_train, y_train = x_norm_phase1_train, y_norm_phase1_train
x_val, y_val = x_norm_phase1_val, y_norm_phase1_val

data_train_list, data_val_list = [], []

# Generate Data for training
for i in range(len(x_train)):
    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically
    edge_index = generate_edge_index(n_nodes)  # Generate dynamic edge_index
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))

# Generate Data for validation
for i in range(len(x_val)):
    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically
    edge_index = generate_edge_index(n_nodes)  # Generate dynamic edge_index
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

# DataLoaders
train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
38/24:

# File path (use raw string format to handle backslashes in Windows paths)
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

# Step 1: Read the CSV file, ignoring the first cell (A1), and set the first column as the index (row names)
df = pd.read_csv(file_path, index_col=0)

# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)
row_names = df.index.tolist()    # Row names (from the first column)
col_names = df.columns.tolist()  # Column names (from the first row)

# Ensure that the row and column names match (adjacency matrix should be square)
assert row_names == col_names, "Row names and column names must match in an adjacency matrix."

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(row_names[i])  # append the source node name
            target_nodes.append(col_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(row_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

# Print the edge index to verify the result
print(edge_index)
38/25:
import torch
from torch_geometric.data import Data, DataLoader

# Assuming x_train, y_train, x_val, y_val are already defined
x_train, y_train = x_norm_phase1_train, y_norm_phase1_train
x_val, y_val = x_norm_phase1_val, y_norm_phase1_val

data_train_list, data_val_list = [], []

# Generate Data for training
for i in range(len(x_train)):
    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically
   
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))

# Generate Data for validation
for i in range(len(x_val)):
    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically
    
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

# DataLoaders
train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
38/26:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/27:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
38/28:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/29:
import torch

# Check if a GPU is available and set the device accordingly
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1 * 2
lr = 0.0001

# Move the model to the GPU
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size).to(device)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # Move data to the GPU
        batch = batch.to(device)
        
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # Move data to the GPU
            batch = batch.to(device)
            
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")

# CPU and GPU usage will automatically be managed based on device setup.
38/30:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val
38/31:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 138
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/32: y_train_prediction.size
38/33: y_phase1_val_mean.shape
38/34: y_train_prediction.shape
38/35: batch.shape
38/36: batch
38/37:
n_bus_p1 = 84
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/38:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/39:
percentage = 100
n_bus_p1 = 84
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/40:
percentage = 100
n_bus_p1 = 84
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/41:
percentage = 100
n_bus_p1 = 84
n_bus_p2 = 84
n_bus_p3 = 84

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/42:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 84
n_bus_p3 = 84

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/43: cleaned_phase1.shape
38/44:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/45: len(edge_index)
38/46: len(edge_index[0])
38/47: len(edge_index[1])
38/48:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/49:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/50: batch.y
38/51: len(batch.y)
38/52: y_train_prediction.size
38/53: model(batch)
38/54: batch
38/55: edge_index
38/56:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/57:
n_bus_p1 = 69
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/58:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/59: data_train_list.shape
38/60: len(data_train_list)
38/61: train_loader
38/62: train_loader.shape
38/63: len(train_loader)
38/64:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
38/65:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
38/66:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
38/67:
train_percentage = 100
val_percentage = 100

train_dataset = slice_dataset(dataset1, train_percentage)
val_dataset = slice_dataset(dataset2, val_percentage)

n_bus = 14

#actual data
x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)
x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)

#normalized data
x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)
x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)
38/68:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n+1], dataset[i, 4*n+3]]))
            y_raw_1.extend(dataset[i, 4*n+3:4*n+5])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
38/69:
dataset1 = pd.read_excel('dataset\Grid_14 bus_1.xlsx').values
dataset2 = pd.read_excel('dataset\Grid_14 bus_2.xlsx').values
38/70:
train_percentage = 100
val_percentage = 100

train_dataset = slice_dataset(dataset1, train_percentage)
val_dataset = slice_dataset(dataset2, val_percentage)

n_bus = 14

#actual data
x_raw_train, y_raw_train = make_dataset(train_dataset, n_bus)
x_raw_val, y_raw_val = make_dataset(val_dataset, n_bus)

#normalized data
x_norm_train, y_norm_train, _, _, _, _ = normalize_dataset(x_raw_train, y_raw_train)
x_norm_val, y_norm_val, x_val_mean, y_val_mean, x_val_std, y_val_std = normalize_dataset(x_raw_val, y_raw_val)
38/71: x_norm_train.shape
38/72: y_norm_train.shape
38/73:
x_train, y_train = x_norm_train, y_norm_train
x_val, y_val = x_norm_val, y_norm_val
edge_index = torch.tensor([[0, 1, 1, 2, 1, 3, 2, 4, 3, 5, 4, 6, 4, 7, 5, 8, 5, 9, 1, 10, 10, 11, 11, 12, 11, 13],
                           [1, 0, 2, 1, 3, 1, 4, 2, 5, 3, 6, 4, 7, 4, 8, 5, 9, 5, 10, 1, 11, 10, 12, 11, 13, 11]], dtype=torch.long)

data_train_list, data_val_list = [], []
for i,_ in enumerate(x_train):
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))
for i,_ in enumerate(x_val):
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
38/74: train_loader.shape
38/75: train_loader
38/76: data_train_list
38/77:
data_train_list.shape
edge_index.shape
38/78:
len(data_train_list)
edge_index.shape
38/79: len(data_train_list)
38/80:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
38/81:
feat_in = 2
feat_size1 = 4
hidden_size1 = 30
output_size = n_bus*2
lr = 0.0001

model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/82:
%%time

feat_in = 2
feat_size1 = 4
hidden_size1 = 30
output_size = n_bus*2
lr = 0.0001

model = My_GNN_NN(n_bus, feat_in, feat_size1, hidden_size1, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
train_loss_list, val_loss_list = [], []

count=0
patience=2000
lossMin = 1e10

for epoch in range(2):

    model.train()
    train_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        y_train_prediction = model(batch)
        loss = MSE(denormalize_output(y_train_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch.num_graphs
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)

    model.eval()
    val_loss=0
    for batch in val_loader:
        y_val_prediction = model(batch)
        loss = MSE(denormalize_output(y_val_prediction, y_val_mean, y_val_std), denormalize_output(batch.y, y_val_mean, y_val_std))
        val_loss += loss.item() * batch.num_graphs
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)

    #early stopping
    if (val_loss < lossMin):
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        model.save_weights(model, "[PyG] [14 bus] Best_GNN_NN_model.pt")
    else:
        count+=1
        if(count>patience):
            print("early stop at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
            print("best val at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(best_epoch, best_train_loss, best_val_loss))
            break
    
    if (train_loss <= 0):
        print("min train loss at epoch {:d}    train loss: {:.7f}    val loss: {:.7f}".format(epoch, train_loss, val_loss))
        break

    if (epoch % 10) == 0:
        print('epoch: {:d}    train loss: {:.7f}    val loss: {:.7f}'.format(epoch, train_loss, val_loss))
38/83: batch
38/84: incomplete_sets_phase1
38/85: incomplete_sets_phase1.shape
38/86: len(incomplete_sets_phase1)
38/87: phase1_df.shape
38/88: phase2_df.shape
38/89: phase3_df.shape
38/90: phase1_df.columns
38/91: df1.columns
38/92: df1.shape
38/93: data
38/94:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        print(data)
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
38/95:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/96:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
38/97:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
38/98:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/99:
dataset3 = pd.read_csv('dataset/measured_active_power.csv')
dataset4 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset5 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset6 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
38/100:
df1 = pd.DataFrame(dataset3)
df2 = pd.DataFrame(dataset4)
df3 = pd.DataFrame(dataset5)
df4 = pd.DataFrame(dataset6)
38/101:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
38/102: df1.shape
38/103:
df1.columns = ['P_' + str(col) for col in df1.columns]
df2.columns = ['Q_' + str(col) for col in df2.columns]
df3.columns = ['V_' + str(col) for col in df3.columns]
df4.columns = ['d_' + str(col) for col in df4.columns]
38/104:
combined_df = pd.concat([df1, df2, df3, df4], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
38/105:
phase1_df = combined_df.filter(regex='.1$')
phase2_df = combined_df.filter(regex='.2$')
phase3_df = combined_df.filter(regex='.3$')
38/106: phase1_df.columns
38/107:
def find_existing_incomplete_columns(df):
    # Extract the base names without the prefixes
    base_names = set([col.split('_', 1)[1] for col in df.columns if '_' in col])
    
    incomplete_sets = []

    # Iterate through the base names and check for the presence of P_, Q_, V_, and d_ columns
    for base_name in base_names:
        existing_columns = []
        
        # Check if P_, Q_, V_, and d_ versions exist
        if f'P_{base_name}' in df.columns:
            existing_columns.append(f'P_{base_name}')
        if f'Q_{base_name}' in df.columns:
            existing_columns.append(f'Q_{base_name}')
        if f'V_{base_name}' in df.columns:
            existing_columns.append(f'V_{base_name}')
        if f'd_{base_name}' in df.columns:
            existing_columns.append(f'd_{base_name}')
        
        # If not all four columns are present, add the existing ones to the result
        if len(existing_columns) < 4:
            incomplete_sets.append(existing_columns)
    
    return incomplete_sets
38/108:
incomplete_sets_phase1 = find_existing_incomplete_columns(phase1_df)
incomplete_sets_phase2 = find_existing_incomplete_columns(phase2_df)
incomplete_sets_phase3 = find_existing_incomplete_columns(phase3_df)
38/109: len(incomplete_sets_phase1)
38/110:
def drop_incomplete_columns(df, incomplete_columns_list):
    # Flatten the list of incomplete columns (because it's a list of lists)
    columns_to_drop = [col for sublist in incomplete_columns_list for col in sublist]
    
    # Drop the columns from the DataFrame
    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')
    
    return df_cleaned
38/111:
cleaned_phase1 = drop_incomplete_columns(phase1_df, incomplete_sets_phase1)
cleaned_phase2 = drop_incomplete_columns(phase2_df, incomplete_sets_phase2)
cleaned_phase3 = drop_incomplete_columns(phase1_df, incomplete_sets_phase3)
38/112: cleaned_phase1..
38/113:
percentage = 100
n_bus_p1 = 68
n_bus_p2 = 69
n_bus_p3 = 72

#Phase1
phase1_dataset = slice_dataset(cleaned_phase1, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)


#Phase2
phase2_dataset = slice_dataset(cleaned_phase2, percentage).to_numpy()
x_raw_phase2, y_raw_phase2 = make_dataset(phase2_dataset, n_bus_p2)


#Phase3
phase3_dataset = slice_dataset(cleaned_phase3, percentage).to_numpy()
x_raw_phase3, y_raw_phase3 = make_dataset(phase3_dataset, n_bus_p3)
38/114:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)

# You now have 70% training, 15% validation, and 15% test sets
38/115:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)
38/116:

# File path (use raw string format to handle backslashes in Windows paths)
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

# Step 1: Read the CSV file, ignoring the first cell (A1), and set the first column as the index (row names)
df = pd.read_csv(file_path, index_col=0)

# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)
row_names = df.index.tolist()    # Row names (from the first column)
col_names = df.columns.tolist()  # Column names (from the first row)

# Ensure that the row and column names match (adjacency matrix should be square)
assert row_names == col_names, "Row names and column names must match in an adjacency matrix."

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(row_names[i])  # append the source node name
            target_nodes.append(col_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(row_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

# Print the edge index to verify the result
print(edge_index)
38/117: len(edge_index[1])
38/118:
import torch
from torch_geometric.data import Data, DataLoader

# Assuming x_train, y_train, x_val, y_val are already defined
x_train, y_train = x_norm_phase1_train, y_norm_phase1_train
x_val, y_val = x_norm_phase1_val, y_norm_phase1_val

data_train_list, data_val_list = [], []

# Generate Data for training
for i in range(len(x_train)):
    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically
   
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))

# Generate Data for validation
for i in range(len(x_val)):
    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically
    
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

# DataLoaders
train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
38/119:
n_bus_p1 = 68
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
38/120:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/121: data
38/122:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        print(data.x)
        print(data.edge_index)
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
38/123:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/124:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        print(data.x.shape)
        print(data.edge_index.shape)
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
38/125:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
38/126: edge_index.max()
38/127: phase1_df.shape
38/128:
import pandas as pd
import torch

# File path (use raw string format to handle backslashes in Windows paths)
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

# Step 1: Read the CSV file, ignoring the first cell (A1), and set the first column as the index (row names)
df = pd.read_csv(file_path, index_col=0)

# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)
row_names = df.index.tolist()    # Row names (from the first column)
col_names = df.columns.tolist()  # Column names (from the first row)

# Ensure that the row and column names match (adjacency matrix should be square)
assert row_names == col_names, "Row names and column names must match in an adjacency matrix."

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(row_names[i])  # append the source node name
            target_nodes.append(col_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(row_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)

# Print the edge index to verify the result
38/129: datasetx = pd.read_csv('dataset/measured_active_power.csv')
38/130: dfx = pd.DataFrame(datasetx)
38/131: dfx.shape
38/132:
import pandas as pd



# Function to clean column names
def clean_column_names(df):
    # Strip off everything after and including the last dot (.)
    df.columns = df.columns.str.replace(r'\.\w+', '', regex=True)
    return df

# Clean the column names
dfz = clean_column_names(dfx)

# Get the unique column names
unique_columns = df.columns.unique()

# Print the unique column names
print(unique_columns)
38/133:
import pandas as pd



# Function to clean column names
def clean_column_names(df):
    # Strip off everything after and including the last dot (.)
    df.columns = df.columns.str.replace(r'\.\w+', '', regex=True)
    return df

# Clean the column names
dfz = clean_column_names(dfx)

# Get the unique column names
unique_columns = df.columns.unique()
38/134: unique_columns.size
38/135:
import pandas as pd



# Function to clean column names
def clean_column_names(df):
    # Strip off everything after and including the last dot (.)
    df.columns = df.columns.str.replace(r'\.\w+', '', regex=True)
    return df

# Clean the column names
dfz = clean_column_names(df2)

# Get the unique column names
unique_columns = df.columns.unique()
38/136: unique_columns.size
38/137:
import pandas as pd



# Function to clean column names
def clean_column_names(df):
    # Strip off everything after and including the last dot (.)
    df.columns = df.columns.str.replace(r'\.\w+', '', regex=True)
    return df

# Clean the column names
dfz = clean_column_names(df3)

# Get the unique column names
unique_columns = df.columns.unique()
38/138: unique_columns.size
38/139:
import pandas as pd



# Function to clean column names
def clean_column_names(df):
    # Strip off everything after and including the last dot (.)
    df.columns = df.columns.str.replace(r'\.\w+', '', regex=True)
    return df

# Clean the column names
dfz = clean_column_names(df4)

# Get the unique column names
unique_columns = df.columns.unique()
38/140: unique_columns.size
40/1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
40/2:
dataset1 = pd.read_csv('dataset/measured_active_power.csv')
dataset2 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
40/3:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
40/4:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
40/5:
phase1_df_p = df1.filter(regex='.1$')
phase1_df_q = df2.filter(regex='.1$')
phase1_df_v = df3.filter(regex='.1$')
phase1_df_d = df4.filter(regex='.1$')
40/6:
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

df = pd.read_csv(file_path, index_col=0)
40/7: df.coulmns
40/8: df.columns
40/9: df.columns.shape
40/10: phase1_df_p.columns
40/11: phase1_df_p.columns.shape
40/12: df.columns
40/13: phase1_df_d.columns
40/14: #function to sort the column issues
40/15:
#function to sort the column issues


def update_dataframe_columns(df, column_list):
    # Step 1: Extract the part of the column names before `.1`
    cleaned_columns = df.columns.str.replace(r'\.\d+', '', regex=True).str.lower()
    
    # Step 2: Compare with the provided list of columns (ignore case)
    column_list_lower = [col.lower() for col in column_list]
    
    # Step 3: Find missing columns from the provided list
    missing_columns = [col for col in column_list_lower if col not in cleaned_columns]
    
    # Step 4: Add missing columns with the `.1` suffix and initialize them with 0
    for missing_col in missing_columns:
        df[f'{missing_col}.1'] = 0
    
    # Step 5: Re-arrange the DataFrame columns in the same order as the provided list
    # The columns in the DataFrame will have the suffix `.1`, so we match by this
    ordered_columns = []
    for col in column_list:
        col_lower = col.lower()
        if col_lower in cleaned_columns.values:
            # If column already exists in the DataFrame, retain its original name
            original_col_name = df.columns[cleaned_columns == col_lower][0]
            ordered_columns.append(original_col_name)
        else:
            # If it's a new column, add it with the `.1` suffix
            ordered_columns.append(f'{col_lower}.1')
    
    # Step 6: Reorder the DataFrame columns
    df = df[ordered_columns]
    
    return df
40/16:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
40/17:
#function to sort the column issues


def update_dataframe_columns(df, column_list):
    # Step 1: Extract the part of the column names before `.1`
    cleaned_columns = df.columns.str.replace(r'\.\d+', '', regex=True).str.lower()
    
    # Step 2: Compare with the provided list of columns (ignore case)
    column_list_lower = [col.lower() for col in column_list]
    
    # Step 3: Find missing columns from the provided list
    missing_columns = [col for col in column_list_lower if col not in cleaned_columns]
    
    # Step 4: Add missing columns with the `.1` suffix and initialize them with 0
    for missing_col in missing_columns:
        df[f'{missing_col}.1'] = 0
    
   
    return df
40/18:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
40/19: phase1_df_d_cleaned.columns
40/20: phase1_df_d_cleaned.columns.shape
40/21:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, df.columns)

phase1_df_p_cleaned = update_dataframe_columns(phase1_df_p, desired_cols)
p1_p = reorder_columns(phase1_df_p_cleaned, desired_cols)

phase1_df_q_cleaned = update_dataframe_columns(phase1_df_q, desired_cols)
p1_q = reorder_columns(phase1_df_q_cleaned, desired_cols)

phase1_df_v_cleaned = update_dataframe_columns(phase1_df_v, desired_cols)
p1_v = reorder_columns(phase1_df_v_cleaned, df.columns)
40/22:
def reorder_columns(df, column_list):
    # Step 1: Convert the DataFrame columns and provided list to lowercase for case-insensitive matching
    df_columns_lower = df.columns.str.lower()
    column_list_lower = [col.lower() for col in column_list]
    
    # Step 2: Create a mapping from lowercase column names to original column names
    column_mapping = {col.lower(): col for col in df.columns}
    
    # Step 3: Reorder the DataFrame columns based on the provided list
    ordered_columns = [column_mapping[col] for col in column_list_lower]
    
    # Step 4: Return the DataFrame with the columns reordered
    return df[ordered_columns]
40/23:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, df.columns)

phase1_df_p_cleaned = update_dataframe_columns(phase1_df_p, desired_cols)
p1_p = reorder_columns(phase1_df_p_cleaned, desired_cols)

phase1_df_q_cleaned = update_dataframe_columns(phase1_df_q, desired_cols)
p1_q = reorder_columns(phase1_df_q_cleaned, desired_cols)

phase1_df_v_cleaned = update_dataframe_columns(phase1_df_v, desired_cols)
p1_v = reorder_columns(phase1_df_v_cleaned, df.columns)
40/24:
d_cols = phase1_df_d.columns

d_cols.lower()
40/25:
d_cols = phase1_df_d.columns

new_cols = []

for col in d_cols:
    new_cols.append(col.lower())
40/26: new_cols
40/27:
def reorder_columns(df, column_list):
    # Step 1: Convert the DataFrame columns and provided list to lowercase for case-insensitive matching
    df_columns_lower =  df.columns.str.replace(r'\.\d+', '', regex=True).str.lower()
    column_list_lower = [col.lower() for col in column_list]
    
    # Step 2: Create a mapping from lowercase column names to original column names
    column_mapping = {col.lower(): col for col in df_columns_lower.columns}
    
    # Step 3: Reorder the DataFrame columns based on the provided list
    ordered_columns = [column_mapping[col] for col in column_list_lower]
    
    # Step 4: Return the DataFrame with the columns reordered
    return df[ordered_columns]
40/28:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, df.columns)
40/29:

def reorder_columns(df, column_list):
    # Step 1: Strip the `.1` suffix from the DataFrame columns
    df_columns_no_suffix = df.columns.str.replace(r'\.1$', '', regex=True).str.lower()
    column_list_lower = [col.lower() for col in column_list]  # Lowercase for case-insensitive comparison

    # Step 2: Create a mapping between stripped (lowercase) column names and original names (with .1)
    column_mapping = {col.lower(): col for col in df.columns}
    
    # Step 3: Reorder DataFrame columns based on the provided list
    ordered_columns = [column_mapping[col] for col in column_list_lower if col in df_columns_no_suffix.values]

    # Step 4: Add `.1` suffix back to the reordered columns
    ordered_columns_with_suffix = [f"{col}.1" if not col.endswith('.1') else col for col in ordered_columns]

    # Step 5: Return the DataFrame with the columns reordered and the `.1` suffix
    return df[ordered_columns_with_suffix]
40/30:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, df.columns)
40/31:


def reorder_and_suffix_columns(df, column_list):
    # Step 1: Strip the `.1` suffix from the DataFrame columns
    df_columns_no_suffix = df.columns.str.replace(r'\.1$', '', regex=True).str.lower()
    
    # Step 2: Create a mapping between stripped (lowercase) column names and original names (with .1)
    column_mapping = {col.lower(): col for col in df.columns}

    # Step 3: Reorder DataFrame columns based on the provided lowercase list
    ordered_columns = [column_mapping[col] for col in column_list if col in df_columns_no_suffix.values]

    # Step 4: Add `.1` suffix back to the reordered columns
    ordered_columns_with_suffix = [f"{col}.1" for col in ordered_columns]

    # Step 5: Return the DataFrame with the columns reordered and the `.1` suffix
    return df[ordered_columns_with_suffix]
40/32:


def reorder_columns(df, column_list):
    # Step 1: Strip the `.1` suffix from the DataFrame columns
    df_columns_no_suffix = df.columns.str.replace(r'\.1$', '', regex=True).str.lower()
    
    # Step 2: Create a mapping between stripped (lowercase) column names and original names (with .1)
    column_mapping = {col.lower(): col for col in df.columns}

    # Step 3: Reorder DataFrame columns based on the provided lowercase list
    ordered_columns = [column_mapping[col] for col in column_list if col in df_columns_no_suffix.values]

    # Step 4: Add `.1` suffix back to the reordered columns
    ordered_columns_with_suffix = [f"{col}.1" for col in ordered_columns]

    # Step 5: Return the DataFrame with the columns reordered and the `.1` suffix
    return df[ordered_columns_with_suffix]
40/33:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, df.columns)
40/34: phase1_df_d_cleaned['P1UDM3853']
40/35:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, desired_cols)
40/36: phase1_df_d_cleaned.columns
40/37: phase1_df_d_cleaned.to_csv
40/38: phase1_df_d_cleaned.to_csv('p1_d')
40/39: phase1_df_d_cleaned.to_csv('p1_d.csv')
40/40:
#function to sort the column issues

def update_dataframe_columns(df, column_list):
    # Step 1: Extract the part of the column names before `.1`
    cleaned_columns = df.columns.str.replace(r'\.\d+', '', regex=True).str.lower()
    
    # Step 2: Compare with the provided list of columns (ignore case)
    column_list_lower = [col.lower() for col in column_list]
    
    # Step 3: Find missing columns from the provided list
    missing_columns = [col for col in column_list_lower if col not in cleaned_columns]
    
    # Step 4: Add missing columns with the `.1` suffix and initialize them with 0
    for missing_col in missing_columns:
        df[f'{missing_col}.1'] = 0
    
    # Step 5: Return the DataFrame with all column names in lowercase
    return df.rename(columns=str.lower)
40/41:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, desired_cols)
40/42: phase1_df_d_cleaned.columns
40/43:
def reorder_columns(df, column_list):
    # Step 1: Strip the `.1` suffix from the DataFrame columns
    ordered_cols = [f"{col}.1" for col in column_list]

    return df[ordered_cols]
40/44:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, desired_cols)
40/45: p1_d.columns
40/46:
phase1_df_p_cleaned = update_dataframe_columns(phase1_df_p, desired_cols)
p1_p = reorder_columns(phase1_df_p_cleaned, desired_cols)

phase1_df_q_cleaned = update_dataframe_columns(phase1_df_q, desired_cols)
p1_q = reorder_columns(phase1_df_q_cleaned, desired_cols)

phase1_df_v_cleaned = update_dataframe_columns(phase1_df_v, desired_cols)
p1_v = reorder_columns(phase1_df_v_cleaned, df.columns)
40/47: p1_p.columns
40/48: p1_q.shape
40/49: p1_d.shape
40/50: type(p1_d)
40/51:
p1_d.columns = ['P_' + str(col) for col in p1_d.columns]
p1_p.columns = ['Q_' + str(col) for col in p1_p.columns]
p1_q.columns = ['V_' + str(col) for col in p1_q.columns]
p1_v.columns = ['d_' + str(col) for col in p1_v.columns]
40/52:
p1_p.columns = ['P_' + str(col) for col in p1_p.columns]
p1_q.columns = ['Q_' + str(col) for col in p1_q.columns]
p1_v.columns = ['V_' + str(col) for col in p1_v.columns]
p1_d.columns = ['d_' + str(col) for col in p1_d.columns]
40/53:
combined_df = pd.concat([p1_p, p1_q, p1_d, p1_d], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
40/54: combined_df.columns
40/55:
dataset1 = pd.read_csv('dataset/measured_active_power.csv')
dataset2 = pd.read_csv('dataset/measured_reactive_power.csv')
dataset3 = pd.read_csv('dataset/actual_voltage_angles.csv')
dataset4 = pd.read_csv('dataset/actual_voltage_magnitudes.csv')
40/56:
df1 = pd.DataFrame(dataset1)
df2 = pd.DataFrame(dataset2)
df3 = pd.DataFrame(dataset3)
df4 = pd.DataFrame(dataset4)
40/57:
df1 = df1.drop(columns=['Timestep'])
df2 = df2.drop(columns=['Timestep'])
df3 = df3.drop(columns=['Timestep'])
df4 = df4.drop(columns=['Timestep'])
40/58:
phase1_df_p = df1.filter(regex='.1$')
phase1_df_q = df2.filter(regex='.1$')
phase1_df_v = df3.filter(regex='.1$')
phase1_df_d = df4.filter(regex='.1$')
40/59:
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

df = pd.read_csv(file_path, index_col=0)
40/60:
#function to sort the column issues

def update_dataframe_columns(df, column_list):
    # Step 1: Extract the part of the column names before `.1`
    cleaned_columns = df.columns.str.replace(r'\.\d+', '', regex=True).str.lower()
    
    # Step 2: Compare with the provided list of columns (ignore case)
    column_list_lower = [col.lower() for col in column_list]
    
    # Step 3: Find missing columns from the provided list
    missing_columns = [col for col in column_list_lower if col not in cleaned_columns]
    
    # Step 4: Add missing columns with the `.1` suffix and initialize them with 0
    for missing_col in missing_columns:
        df[f'{missing_col}.1'] = 0
    
    # Step 5: Return the DataFrame with all column names in lowercase
    return df.rename(columns=str.lower)
40/61:
def reorder_columns(df, column_list):
    # Step 1: Strip the `.1` suffix from the DataFrame columns
    ordered_cols = [f"{col}.1" for col in column_list]

    return df[ordered_cols]
40/62:
desired_cols = df.columns

phase1_df_d_cleaned = update_dataframe_columns(phase1_df_d, desired_cols)
p1_d = reorder_columns(phase1_df_d_cleaned, desired_cols)
40/63:
phase1_df_p_cleaned = update_dataframe_columns(phase1_df_p, desired_cols)
p1_p = reorder_columns(phase1_df_p_cleaned, desired_cols)

phase1_df_q_cleaned = update_dataframe_columns(phase1_df_q, desired_cols)
p1_q = reorder_columns(phase1_df_q_cleaned, desired_cols)

phase1_df_v_cleaned = update_dataframe_columns(phase1_df_v, desired_cols)
p1_v = reorder_columns(phase1_df_v_cleaned, df.columns)
40/64:
p1_p.columns = ['P_' + str(col) for col in p1_p.columns]
p1_q.columns = ['Q_' + str(col) for col in p1_q.columns]
p1_v.columns = ['V_' + str(col) for col in p1_v.columns]
p1_d.columns = ['d_' + str(col) for col in p1_d.columns]
40/65:
combined_df = pd.concat([p1_p, p1_q, p1_d, p1_d], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
40/66: combined_df.columns
40/67: combined_df.shape
40/68:
combined_df = pd.concat([p1_p, p1_q, p1_v, p1_d], axis=1)

# Sorting columns by the suffix part to ensure same suffix columns are next to each other
combined_df = combined_df.reindex(sorted(combined_df.columns, key=lambda x: x.split('_')[1]), axis=1)

# Display the combined dataframe
print(combined_df.head())
40/69:
def slice_dataset(dataset, percentage):
    data_size = len(dataset)
    return dataset[:int(data_size*percentage/100)]

def make_dataset(dataset, n_bus):
    x_raw_1, y_raw_1 = [], []
    x_raw, y_raw = [], []

    for i in range(len(dataset)):
        for n in range(n_bus):
            x_raw_1.append(list([dataset[i, 4*n], dataset[i, 4*n+2]]))
            y_raw_1.extend(dataset[i, 4*n+2:4*n+4])

        x_raw.append(list(x_raw_1))
        y_raw.append(y_raw_1)
        x_raw_1, y_raw_1 = [], []

    x_raw = torch.tensor(x_raw, dtype=torch.float)
    y_raw = torch.tensor(y_raw, dtype=torch.float)
    return x_raw, y_raw

def normalize_dataset(x, y):
    x_mean = torch.mean(x,0)
    y_mean = torch.mean(y,0)
    x_std = torch.std(x,0)
    y_std = torch.std(y,0)
    x_norm = (x-x_mean)/x_std
    y_norm = (y-y_mean)/y_std
    x_norm = torch.where(torch.isnan(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isnan(y_norm), torch.zeros_like(y_norm), y_norm)
    x_norm = torch.where(torch.isinf(x_norm), torch.zeros_like(x_norm), x_norm)
    y_norm = torch.where(torch.isinf(y_norm), torch.zeros_like(y_norm), y_norm)
    return x_norm, y_norm, x_mean, y_mean, x_std, y_std

def denormalize_output(y_norm, y_mean, y_std):
    y = y_norm*y_std+y_mean
    return y

def NRMSE(yhat,y):
    return torch.sqrt(torch.mean(((yhat-y)/torch.std(yhat,0))**2))

def MSE(yhat,y):
    return torch.mean((yhat-y)**2)
40/70:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        print(data.x.shape)
        print(data.edge_index.shape)
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
40/71:
percentage = 100
n_bus_p1 = 84
n_bus_p2 = 84
n_bus_p3 = 84

#Phase1
phase1_dataset = slice_dataset(combined_df, percentage).to_numpy()
x_raw_phase1, y_raw_phase1 = make_dataset(phase1_dataset, n_bus_p1)
40/72:
from sklearn.model_selection import train_test_split

# Assuming X is your features and y is your labels
X_train_p1, X_temp_p1, y_train_p1, y_temp_p1 = train_test_split(x_raw_phase1, y_raw_phase1, test_size=0.5, random_state=42)

# Now split X_temp and y_temp into validation and test sets
X_val_p1, X_test_p1, y_val_p1, y_test_p1 = train_test_split(X_temp_p1, y_temp_p1, test_size=0.5, random_state=42)
40/73:
#Phase1

x_norm_phase1_train, y_norm_phase1_train, _, _, _, _ = normalize_dataset(X_train_p1, y_train_p1)
x_norm_phase1_val, y_norm_phase1_val, x_phase1_val_mean, y_phase1_val_mean, x_phase1_val_std, y_phase1_val_std = normalize_dataset(X_val_p1, y_val_p1)
40/74:
# File path (use raw string format to handle backslashes in Windows paths)
file_path = r'C:\FREEDM\gnn-powerflow\jupyter notebook\dataset\edge_matrix.csv'

# Step 1: Read the CSV file, ignoring the first cell (A1), and set the first column as the index (row names)
df = pd.read_csv(file_path, index_col=0)

# Step 2: Extract row and column names (node names) from the CSV (they should be the same in a matrix)
row_names = df.index.tolist()    # Row names (from the first column)
col_names = df.columns.tolist()  # Column names (from the first row)

# Ensure that the row and column names match (adjacency matrix should be square)
assert row_names == col_names, "Row names and column names must match in an adjacency matrix."

# Step 3: Initialize two lists to store source and target nodes
source_nodes = []
target_nodes = []

# Step 4: Iterate through the DataFrame to extract edges (ignore the diagonal)
for i in range(df.shape[0]):  # iterate over rows (source nodes)
    for j in range(df.shape[1]):  # iterate over columns (target nodes)
        if df.iloc[i, j] == 1:  # if there's an edge between node i and node j
            source_nodes.append(row_names[i])  # append the source node name
            target_nodes.append(col_names[j])  # append the target node name

# Step 5: Convert node names to numerical indices
node_to_index = {name: idx for idx, name in enumerate(row_names)}

source_indices = [node_to_index[node] for node in source_nodes]
target_indices = [node_to_index[node] for node in target_nodes]

# Step 6: Create the edge_index tensor
edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)
40/75:
# Assuming x_train, y_train, x_val, y_val are already defined
x_train, y_train = x_norm_phase1_train, y_norm_phase1_train
x_val, y_val = x_norm_phase1_val, y_norm_phase1_val

data_train_list, data_val_list = [], []

# Generate Data for training
for i in range(len(x_train)):
    n_nodes = x_train[i].shape[0]  # Determine the number of nodes dynamically
   
    data_train_list.append(Data(x=x_train[i], y=y_train[i], edge_index=edge_index))

# Generate Data for validation
for i in range(len(x_val)):
    n_nodes = x_val[i].shape[0]  # Determine the number of nodes dynamically
    
    data_val_list.append(Data(x=x_val[i], y=y_val[i], edge_index=edge_index))

# DataLoaders
train_loader = DataLoader(data_train_list, batch_size=1)
val_loader = DataLoader(data_val_list, batch_size=1)
40/76:
n_bus_p1 = 84
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1*2
lr = 0.0001

model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)
for name, param in model.named_parameters():
  print(name)
  print(param.size())

param = sum(p.numel() for p in model.parameters() if p.requires_grad)
param
40/77:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
40/78:
class My_GNN_NN(torch.nn.Module):
    def __init__(self, node_size=None, feat_in=None, feat_size1=None, hidden_size1=None, output_size=None):
        super(My_GNN_NN, self).__init__()
        self.feat_in = feat_in if feat_in is not None else 2
        self.feat_size1 = feat_in if feat_in is not None else 4
        self.hidden_size1 = hidden_size1 if hidden_size1 is not None else 20
        self.output_size = output_size if output_size is not None else 12
        
        self.conv1 = GCNConv(feat_in, feat_size1)
        self.lin1 = Linear(node_size*feat_size1, hidden_size1)
        self.lin2 = Linear(hidden_size1, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index

        x = self.conv1(x, edge_index)
        x = torch.tanh(x)

        x = x.flatten(start_dim = 0)
        x = self.lin1(x)
        x = torch.tanh(x)

        x = self.lin2(x)

        return x
    
    def save_weights(self, model, name):
        torch.save(model, name)
40/79:
import torch

# Define your parameters
feat_in = 2
feat_size1 = 4
hidden_size1 = 168
output_size = n_bus_p1 * 2  # Ensure n_bus_p1 is defined in your code
lr = 0.0001

# Initialize the model
model = My_GNN_NN(n_bus_p1, feat_in, feat_size1, hidden_size1, output_size)

# Optimizer remains the same
optimizer = torch.optim.Adam(model.parameters(), lr=lr)

# Lists for storing training and validation losses
train_loss_list, val_loss_list = [], []

# Early stopping parameters
count = 0
patience = 2000
lossMin = 1e10

# Main training loop
for epoch in range(2001):
    model.train()  # Set model to training mode
    train_loss = 0
    
    # Training loop
    for batch in train_loader:
        # No device transfer, batch remains on CPU
        optimizer.zero_grad()  # Reset gradients
        
        # Forward pass
        y_train_prediction = model(batch)
        
        # Compute loss
        loss = MSE(
            denormalize_output(y_train_prediction, y_phase1_val_mean, y_phase1_val_std),
            denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
        )
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Accumulate the loss, weighted by the number of graphs
        train_loss += loss.item() * batch.num_graphs
        
    # Average training loss over the dataset
    train_loss /= len(train_loader.dataset)
    train_loss_list.append(train_loss)
    
    # Validation phase
    model.eval()  # Set model to evaluation mode
    val_loss = 0
    with torch.no_grad():  # Disable gradient computation for validation
        for batch in val_loader:
            # No device transfer, batch remains on CPU
            # Forward pass
            y_val_prediction = model(batch)
            
            # Compute validation loss
            loss = MSE(
                denormalize_output(y_val_prediction, y_phase1_val_mean, y_phase1_val_std),
                denormalize_output(batch.y, y_phase1_val_mean, y_phase1_val_std)
            )
            
            # Accumulate the loss, weighted by the number of graphs
            val_loss += loss.item() * batch.num_graphs
        
    # Average validation loss over the dataset
    val_loss /= len(val_loader.dataset)
    val_loss_list.append(val_loss)
    
    # Early stopping mechanism
    if val_loss < lossMin:
        lossMin = val_loss
        count = 0
        best_epoch = epoch
        best_train_loss = train_loss
        best_val_loss = val_loss
        
        # Save the best model (Ensure your model.save_weights function is properly defined)
        model.save_weights(model, "New_GNN_NN.pt")
    else:
        count += 1
        if count > patience:
            print(f"early stop at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
            print(f"best val at epoch {best_epoch}    train loss: {best_train_loss:.7f}    val loss: {best_val_loss:.7f}")
            break
    
    # Stop if training loss is too low
    if train_loss <= 0:
        print(f"min train loss at epoch {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
        break

    # Print loss every 10 epochs
    if epoch % 10 == 0:
        print(f"epoch: {epoch}    train loss: {train_loss:.7f}    val loss: {val_loss:.7f}")
40/80: ! pip install tensorflow-gpu
40/81: ! pip freeze
40/82:
import tensorflow as tf

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))
40/83:
if torch.cuda.is_available():
    print(f"CUDA is available! Number of GPUs: {torch.cuda.device_count()}")
else:
    print("No GPU found!")
40/84:
import tensorflow as tf
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        print("Hi")
        # Set memory growth to avoid full allocation at once
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        # Set the first GPU to be used
        tf.config.set_visible_devices(gpus[0], 'GPU')
    except RuntimeError as e:
        print(e)  # This will throw if GPUs are already initialized
40/85:
import tensorflow as tf
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        print("Hi")
        # Set memory growth to avoid full allocation at once
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        # Set the first GPU to be used
        tf.config.set_visible_devices(gpus[0], 'GPU')
    except RuntimeError as e:
        print(e)  # This will throw if GPUs are already initialized
   1:
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import Linear

import torch_geometric.nn as pyg_nn
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

np.set_printoptions(precision=5, suppress=True)
torch.set_printoptions(precision=5, sci_mode=False)
   2:

%history -g -f filename
